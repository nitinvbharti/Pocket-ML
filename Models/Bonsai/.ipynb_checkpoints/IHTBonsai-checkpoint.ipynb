{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS Data\n",
    "\n",
    "Dataset has been already preprocessed and onehotted...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = './cifar10/'\n",
    "Xtrain = np.load(dirc + 'Xtrain.npy').reshape(-1,32*32*3)\n",
    "Ytrain = np.load(dirc + 'Ytrain.npy')\n",
    "Xtest = np.load(dirc + 'Xtest.npy').reshape(-1,32*32*3)\n",
    "Ytest = np.load(dirc + 'Ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 50000 ,Data Dims: 3072 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 24 # index of data object....\n",
    "obj = Xtrain[i].reshape(32,32,3)\n",
    "plt.imshow(np.rollaxis(obj,1,0),cmap='gray')\n",
    "plt.show()\n",
    "print(Ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "\n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.dDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),self.pDims) # dimensions are D^x1\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx1\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x1\n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x1\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx1\n",
    "            \n",
    "            \n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = 1 # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if (itersInPhase % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "                    Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (100 * self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 0.1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > (1/2)*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 74, tDepth = 4, sigma = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.001, lT = 0.001, lV = 0.001, lZ = 0.0001, lr = 0.01, X = X, Y = Y,\n",
    "                              sZ = 0.999, sW = 0.999, sV = 0.999, sT = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "Train Loss: 587.3918915479172 Train accuracy: 0.1901625563879184\n",
      "Test accuracy 0.0824\n",
      "MarginLoss + RegLoss: 3086.075 + 21.127844 = 3107.203\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 311.5695178797427 Train accuracy: 0.2417320635153039\n",
      "Test accuracy 0.0959\n",
      "MarginLoss + RegLoss: 2338.2817 + 20.93634 = 2359.218\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 273.04123773275467 Train accuracy: 0.2510209813914491\n",
      "Test accuracy 0.0965\n",
      "MarginLoss + RegLoss: 2203.142 + 20.807808 = 2223.95\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 209.38753738745444 Train accuracy: 0.270239430425413\n",
      "Test accuracy 0.1086\n",
      "MarginLoss + RegLoss: 1956.2836 + 20.68264 = 1976.9662\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 178.50348228830927 Train accuracy: 0.27310217930329755\n",
      "Test accuracy 0.1437\n",
      "MarginLoss + RegLoss: 1343.7365 + 20.546701 = 1364.2832\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: 1575.6713435425352 Train accuracy: 0.20715887336720265\n",
      "Test accuracy 0.1202\n",
      "MarginLoss + RegLoss: 8517.259 + 20.807987 = 8538.066\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: 486.5082865488369 Train accuracy: 0.2557655351044351\n",
      "Test accuracy 0.1021\n",
      "MarginLoss + RegLoss: 5438.3574 + 20.662033 = 5459.0195\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: 248.9106402888961 Train accuracy: 0.29262091593624767\n",
      "Test accuracy 0.1417\n",
      "MarginLoss + RegLoss: 2350.7854 + 20.569487 = 2371.355\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: 262.28690163650856 Train accuracy: 0.28475336443148386\n",
      "Test accuracy 0.1283\n",
      "MarginLoss + RegLoss: 1361.7122 + 20.475403 = 1382.1875\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: 229.54671871715598 Train accuracy: 0.28955797537025313\n",
      "Test accuracy 0.1604\n",
      "MarginLoss + RegLoss: 996.106 + 20.382261 = 1016.4883\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: 168.7410580416966 Train accuracy: 0.30995755913278983\n",
      "Test accuracy 0.1534\n",
      "MarginLoss + RegLoss: 679.358 + 20.295609 = 699.65356\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: 149.7039852398928 Train accuracy: 0.3207479183115232\n",
      "Test accuracy 0.1539\n",
      "MarginLoss + RegLoss: 628.1909 + 20.200987 = 648.3919\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: 151.23837314485968 Train accuracy: 0.3149623639647736\n",
      "Test accuracy 0.1614\n",
      "MarginLoss + RegLoss: 585.4255 + 20.108044 = 605.5335\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: 298.70027783312605 Train accuracy: 0.27380285091330653\n",
      "Test accuracy 0.1821\n",
      "MarginLoss + RegLoss: 620.35803 + 19.973593 = 640.3316\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: 119.19886564246208 Train accuracy: 0.3441904229968118\n",
      "Test accuracy 0.2\n",
      "MarginLoss + RegLoss: 389.0836 + 19.870245 = 408.95383\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: 2075.428847924476 Train accuracy: 0.2408912551376317\n",
      "Test accuracy 0.157\n",
      "MarginLoss + RegLoss: 3435.152 + 20.14418 = 3455.2964\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: 1136.18287761329 Train accuracy: 0.2253763623234937\n",
      "Test accuracy 0.2115\n",
      "MarginLoss + RegLoss: 614.41986 + 20.017576 = 634.43744\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: 777.9099429006534 Train accuracy: 0.24333360072876842\n",
      "Test accuracy 0.2928\n",
      "MarginLoss + RegLoss: 460.7018 + 19.870373 = 480.57217\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: 317.6552490097525 Train accuracy: 0.29398222334449065\n",
      "Test accuracy 0.2923\n",
      "MarginLoss + RegLoss: 263.0522 + 19.763268 = 282.81546\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: 319.55295401945244 Train accuracy: 0.28575432387435384\n",
      "Test accuracy 0.2714\n",
      "MarginLoss + RegLoss: 452.4121 + 19.66156 = 472.07367\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: 306.3972447827258 Train accuracy: 0.2893577845508208\n",
      "Test accuracy 0.3296\n",
      "MarginLoss + RegLoss: 227.04297 + 19.55999 = 246.60297\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: 543.7467527517823 Train accuracy: 0.26707639301304326\n",
      "Test accuracy 0.1816\n",
      "MarginLoss + RegLoss: 1623.7244 + 19.734243 = 1643.4586\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: 746.6141074142114 Train accuracy: 0.27164077591735686\n",
      "Test accuracy 0.2978\n",
      "MarginLoss + RegLoss: 264.9898 + 19.615768 = 284.6056\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: 391.21467562962005 Train accuracy: 0.2949831845247157\n",
      "Test accuracy 0.3076\n",
      "MarginLoss + RegLoss: 199.5181 + 19.500933 = 219.01903\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: 275.743274705827 Train accuracy: 0.30905669479894\n",
      "Test accuracy 0.3227\n",
      "MarginLoss + RegLoss: 167.74243 + 19.397312 = 187.13974\n",
      "\n",
      "Epoch Number: 25\n",
      "Train Loss: 264.15581960635336 Train accuracy: 0.302850737833656\n",
      "Test accuracy 0.2188\n",
      "MarginLoss + RegLoss: 360.62268 + 19.292753 = 379.91544\n",
      "\n",
      "Epoch Number: 26\n",
      "Train Loss: 243.97615735520162 Train accuracy: 0.3114990385524895\n",
      "Test accuracy 0.2525\n",
      "MarginLoss + RegLoss: 316.1126 + 19.185385 = 335.298\n",
      "\n",
      "Epoch Number: 27\n",
      "Train Loss: 176.21786139791857 Train accuracy: 0.3331998716288083\n",
      "Test accuracy 0.3271\n",
      "MarginLoss + RegLoss: 190.63034 + 19.078629 = 209.70897\n",
      "\n",
      "Epoch Number: 28\n",
      "Train Loss: 205.9039219056544 Train accuracy: 0.3152025949393687\n",
      "Test accuracy 0.2731\n",
      "MarginLoss + RegLoss: 197.95782 + 18.970491 = 216.92831\n",
      "\n",
      "Epoch Number: 29\n",
      "Train Loss: 181.61801677755176 Train accuracy: 0.3230501283043703\n",
      "Test accuracy 0.3143\n",
      "MarginLoss + RegLoss: 149.31808 + 18.859465 = 168.17755\n",
      "\n",
      "Epoch Number: 30\n",
      "Train Loss: 148.5588391907012 Train accuracy: 0.3380445215867774\n",
      "Test accuracy 0.269\n",
      "MarginLoss + RegLoss: 183.20805 + 18.743074 = 201.95113\n",
      "\n",
      "Epoch Number: 31\n",
      "Train Loss: 171.4732779943355 Train accuracy: 0.3185057647426032\n",
      "Test accuracy 0.3578\n",
      "MarginLoss + RegLoss: 131.82143 + 18.61846 = 150.43988\n",
      "\n",
      "Epoch Number: 32\n",
      "Train Loss: 200.66557900467262 Train accuracy: 0.3196868999389255\n",
      "Test accuracy 0.3384\n",
      "MarginLoss + RegLoss: 161.34883 + 18.504406 = 179.85324\n",
      "\n",
      "Epoch Number: 33\n",
      "Train Loss: 158.740092256144 Train accuracy: 0.3333199866817671\n",
      "Test accuracy 0.2927\n",
      "MarginLoss + RegLoss: 196.02188 + 18.362408 = 214.3843\n",
      "\n",
      "Epoch Number: 34\n",
      "Train Loss: 1628.0952759473314 Train accuracy: 0.22065182553078028\n",
      "Test accuracy 0.2822\n",
      "MarginLoss + RegLoss: 467.16394 + 18.584566 = 485.7485\n",
      "\n",
      "Epoch Number: 35\n",
      "Train Loss: 498.2176040854689 Train accuracy: 0.27562459985905163\n",
      "Test accuracy 0.2148\n",
      "MarginLoss + RegLoss: 557.0471 + 18.38645 = 575.4336\n",
      "\n",
      "Epoch Number: 36\n",
      "Train Loss: 331.79715650070943 Train accuracy: 0.2993073359198634\n",
      "Test accuracy 0.3421\n",
      "MarginLoss + RegLoss: 183.13641 + 18.211666 = 201.34808\n",
      "\n",
      "Epoch Number: 37\n",
      "Train Loss: 254.42783882585877 Train accuracy: 0.3199071114707421\n",
      "Test accuracy 0.2977\n",
      "MarginLoss + RegLoss: 170.39928 + 18.044333 = 188.4436\n",
      "\n",
      "Epoch Number: 38\n",
      "Train Loss: 197.2554639807731 Train accuracy: 0.32945627769279906\n",
      "Test accuracy 0.2682\n",
      "MarginLoss + RegLoss: 236.42155 + 17.879118 = 254.30067\n",
      "\n",
      "Epoch Number: 39\n",
      "Train Loss: 267.8562710338644 Train accuracy: 0.3064541955699835\n",
      "Test accuracy 0.3635\n",
      "MarginLoss + RegLoss: 225.04942 + 17.721476 = 242.7709\n",
      "\n",
      "Epoch Number: 40\n",
      "Train Loss: 238.48540989486625 Train accuracy: 0.3103579442463648\n",
      "Test accuracy 0.2698\n",
      "MarginLoss + RegLoss: 157.1173 + 17.54991 = 174.6672\n",
      "\n",
      "Epoch Number: 41\n",
      "Train Loss: 207.04015606935783 Train accuracy: 0.3136210753645063\n",
      "Test accuracy 0.3453\n",
      "MarginLoss + RegLoss: 155.1349 + 17.368114 = 172.50302\n",
      "\n",
      "Epoch Number: 42\n",
      "Train Loss: 151.79399570755893 Train accuracy: 0.34210842440213857\n",
      "Test accuracy 0.3853\n",
      "MarginLoss + RegLoss: 83.75355 + 17.187937 = 100.94148\n",
      "\n",
      "Epoch Number: 43\n",
      "Train Loss: 171.1582187601269 Train accuracy: 0.32286995622609227\n",
      "Test accuracy 0.3844\n",
      "MarginLoss + RegLoss: 91.300415 + 17.002777 = 108.30319\n",
      "\n",
      "Epoch Number: 44\n",
      "Train Loss: 127.15100515049134 Train accuracy: 0.33950592664325185\n",
      "Test accuracy 0.3476\n",
      "MarginLoss + RegLoss: 117.72635 + 16.817432 = 134.54378\n",
      "\n",
      "Epoch Number: 45\n",
      "Train Loss: 139.2182336302616 Train accuracy: 0.3285754330356025\n",
      "Test accuracy 0.3567\n",
      "MarginLoss + RegLoss: 80.08834 + 16.628183 = 96.71652\n",
      "\n",
      "Epoch Number: 46\n",
      "Train Loss: 119.17135240358087 Train accuracy: 0.33700352399338523\n",
      "Test accuracy 0.3252\n",
      "MarginLoss + RegLoss: 109.82392 + 16.442324 = 126.26624\n",
      "\n",
      "Epoch Number: 47\n",
      "Train Loss: 130.9599639824153 Train accuracy: 0.3336002556732417\n",
      "Test accuracy 0.3652\n",
      "MarginLoss + RegLoss: 72.81813 + 16.259975 = 89.07811\n",
      "\n",
      "Epoch Number: 48\n",
      "Train Loss: 215.02513998399402 Train accuracy: 0.30843609818695905\n",
      "Test accuracy 0.3512\n",
      "MarginLoss + RegLoss: 79.76055 + 16.088587 = 95.84914\n",
      "\n",
      "Epoch Number: 49\n",
      "Train Loss: 129.39237233662286 Train accuracy: 0.3335602181241117\n",
      "Test accuracy 0.4069\n",
      "MarginLoss + RegLoss: 111.895874 + 15.894254 = 127.79013\n",
      "\n",
      "Epoch Number: 50\n",
      "Train Loss: 112.78123511754879 Train accuracy: 0.3354220045521655\n",
      "Test accuracy 0.3623\n",
      "MarginLoss + RegLoss: 57.06662 + 15.704657 = 72.77128\n",
      "\n",
      "Epoch Number: 51\n",
      "Train Loss: 95.85014613540717 Train accuracy: 0.33548206184477014\n",
      "Test accuracy 0.232\n",
      "MarginLoss + RegLoss: 134.00677 + 15.512137 = 149.5189\n",
      "\n",
      "Epoch Number: 52\n",
      "Train Loss: 355.96044111038003 Train accuracy: 0.28435297913415014\n",
      "Test accuracy 0.352\n",
      "MarginLoss + RegLoss: 69.23719 + 15.377002 = 84.61419\n",
      "\n",
      "Epoch Number: 53\n",
      "Train Loss: 77.0430654859329 Train accuracy: 0.3591447796655877\n",
      "Test accuracy 0.3828\n",
      "MarginLoss + RegLoss: 41.683773 + 15.199764 = 56.883537\n",
      "\n",
      "Epoch Number: 54\n",
      "Train Loss: 331.97317087489927 Train accuracy: 0.2978259131746709\n",
      "Test accuracy 0.4049\n",
      "MarginLoss + RegLoss: 59.079742 + 15.058481 = 74.13822\n",
      "\n",
      "Epoch Number: 55\n",
      "Train Loss: 104.31329982056211 Train accuracy: 0.3365430808521707\n",
      "Test accuracy 0.3389\n",
      "MarginLoss + RegLoss: 76.651436 + 14.886109 = 91.537544\n",
      "\n",
      "Epoch Number: 56\n",
      "Train Loss: 131.25963409492252 Train accuracy: 0.338625080983735\n",
      "Test accuracy 0.4021\n",
      "MarginLoss + RegLoss: 58.957695 + 14.722505 = 73.6802\n",
      "\n",
      "Epoch Number: 57\n",
      "Train Loss: 51.9818077771653 Train accuracy: 0.37141656020297065\n",
      "Test accuracy 0.3556\n",
      "MarginLoss + RegLoss: 66.386116 + 14.562172 = 80.94829\n",
      "\n",
      "Epoch Number: 58\n",
      "Train Loss: 153.20325629058974 Train accuracy: 0.29660474108072676\n",
      "Test accuracy 0.3022\n",
      "MarginLoss + RegLoss: 54.728752 + 14.426126 = 69.15488\n",
      "\n",
      "Epoch Number: 59\n",
      "Train Loss: 51.62131436950957 Train accuracy: 0.3589045490919207\n",
      "Test accuracy 0.3432\n",
      "MarginLoss + RegLoss: 36.509438 + 14.258665 = 50.768105\n",
      "\n",
      "Epoch Number: 60\n",
      "Train Loss: 195.2856481748846 Train accuracy: 0.31117873164436743\n",
      "Test accuracy 0.322\n",
      "MarginLoss + RegLoss: 103.489334 + 14.134014 = 117.62335\n",
      "\n",
      "Epoch Number: 61\n",
      "Train Loss: 60.64767125903758 Train accuracy: 0.34383007676879385\n",
      "Test accuracy 0.4098\n",
      "MarginLoss + RegLoss: 22.609879 + 14.021968 = 36.631847\n",
      "\n",
      "Epoch Number: 62\n",
      "Train Loss: 179.30068394956032 Train accuracy: 0.3002482386715209\n",
      "Test accuracy 0.2595\n",
      "MarginLoss + RegLoss: 153.89317 + 13.944412 = 167.83759\n",
      "\n",
      "Epoch Number: 63\n",
      "Train Loss: 227.15732071538676 Train accuracy: 0.31638372793058644\n",
      "Test accuracy 0.4035\n",
      "MarginLoss + RegLoss: 22.034594 + 13.833147 = 35.86774\n",
      "\n",
      "Epoch Number: 64\n",
      "Train Loss: 35.256589051319345 Train accuracy: 0.37652146081218807\n",
      "Test accuracy 0.4185\n",
      "MarginLoss + RegLoss: 14.529812 + 13.676765 = 28.206577\n",
      "\n",
      "Epoch Number: 65\n",
      "Train Loss: 509.5324432304622 Train accuracy: 0.3294162404443651\n",
      "Test accuracy 0.0995\n",
      "MarginLoss + RegLoss: 8877.443 + 14.200429 = 8891.644\n",
      "\n",
      "Epoch Number: 66\n",
      "Train Loss: 2704.8674383462812 Train accuracy: 0.17286595124166643\n",
      "Test accuracy 0.2083\n",
      "MarginLoss + RegLoss: 462.1298 + 13.961879 = 476.09167\n",
      "\n",
      "Epoch Number: 67\n",
      "Train Loss: 399.91842363981925 Train accuracy: 0.24139173636254707\n",
      "Test accuracy 0.1944\n",
      "MarginLoss + RegLoss: 350.2489 + 13.7797 = 364.0286\n",
      "\n",
      "Epoch Number: 68\n",
      "Train Loss: 288.4579980619285 Train accuracy: 0.2517216530348688\n",
      "Test accuracy 0.2416\n",
      "MarginLoss + RegLoss: 257.0452 + 13.621158 = 270.66635\n",
      "\n",
      "Epoch Number: 69\n",
      "Train Loss: 227.41869484255668 Train accuracy: 0.2675168166446579\n",
      "Test accuracy 0.2928\n",
      "MarginLoss + RegLoss: 197.29582 + 13.474951 = 210.77077\n",
      "\n",
      "Epoch Number: 70\n",
      "Train Loss: 156.58962526877366 Train accuracy: 0.2892176493200486\n",
      "Test accuracy 0.2508\n",
      "MarginLoss + RegLoss: 160.8277 + 13.343375 = 174.17107\n",
      "\n",
      "Epoch Number: 71\n",
      "Train Loss: 128.71121942836606 Train accuracy: 0.29570387574455664\n",
      "Test accuracy 0.2633\n",
      "MarginLoss + RegLoss: 177.38177 + 13.220384 = 190.60216\n",
      "\n",
      "Epoch Number: 72\n",
      "Train Loss: 106.25297236763308 Train accuracy: 0.3132607312079502\n",
      "Test accuracy 0.3305\n",
      "MarginLoss + RegLoss: 83.61928 + 13.101949 = 96.72123\n",
      "\n",
      "Epoch Number: 73\n",
      "Train Loss: 84.68895341676446 Train accuracy: 0.3240711082925711\n",
      "Test accuracy 0.3114\n",
      "MarginLoss + RegLoss: 41.338238 + 12.991216 = 54.329453\n",
      "\n",
      "Epoch Number: 74\n",
      "Train Loss: 62.22634612284433 Train accuracy: 0.3431694422067548\n",
      "Test accuracy 0.3669\n",
      "MarginLoss + RegLoss: 35.914543 + 12.88763 = 48.802174\n",
      "\n",
      "Epoch Number: 75\n",
      "Train Loss: 88.88855868283943 Train accuracy: 0.33053731550817533\n",
      "Test accuracy 0.2776\n",
      "MarginLoss + RegLoss: 88.4715 + 12.781066 = 101.25256\n",
      "\n",
      "Epoch Number: 76\n",
      "Train Loss: 78.62089324745897 Train accuracy: 0.32401105059903834\n",
      "Test accuracy 0.3455\n",
      "MarginLoss + RegLoss: 52.184444 + 12.676038 = 64.86048\n",
      "\n",
      "Epoch Number: 77\n",
      "Train Loss: 79.48769332475192 Train accuracy: 0.3337604098122216\n",
      "Test accuracy 0.3684\n",
      "MarginLoss + RegLoss: 51.485176 + 12.57868 = 64.06386\n",
      "\n",
      "Epoch Number: 78\n",
      "Train Loss: 56.559112933719106 Train accuracy: 0.35790358724348215\n",
      "Test accuracy 0.3248\n",
      "MarginLoss + RegLoss: 37.074486 + 12.4872 = 49.561684\n",
      "\n",
      "Epoch Number: 79\n",
      "Train Loss: 43.24390445589485 Train accuracy: 0.36250800696189095\n",
      "Test accuracy 0.3124\n",
      "MarginLoss + RegLoss: 39.42566 + 12.399455 = 51.825115\n",
      "\n",
      "Epoch Number: 80\n",
      "Train Loss: 76.39992014281952 Train accuracy: 0.3163637098910563\n",
      "Test accuracy 0.3674\n",
      "MarginLoss + RegLoss: 58.960155 + 12.318684 = 71.27884\n",
      "\n",
      "Epoch Number: 81\n",
      "Train Loss: 105.66522663270412 Train accuracy: 0.3069746956028746\n",
      "Test accuracy 0.3329\n",
      "MarginLoss + RegLoss: 58.846428 + 12.2219305 = 71.06836\n",
      "\n",
      "Epoch Number: 82\n",
      "Train Loss: 62.53709499718362 Train accuracy: 0.34623238324049876\n",
      "Test accuracy 0.3121\n",
      "MarginLoss + RegLoss: 47.21764 + 12.143146 = 59.360786\n",
      "\n",
      "Epoch Number: 83\n",
      "Train Loss: 41.153904902026255 Train accuracy: 0.3637091606989035\n",
      "Test accuracy 0.3133\n",
      "MarginLoss + RegLoss: 32.642014 + 12.067232 = 44.709244\n",
      "\n",
      "Epoch Number: 84\n",
      "Train Loss: 39.37913140267 Train accuracy: 0.36010570155932764\n",
      "Test accuracy 0.2733\n",
      "MarginLoss + RegLoss: 44.270863 + 12.026147 = 56.29701\n",
      "\n",
      "Epoch Number: 85\n",
      "Train Loss: 72.59333649879079 Train accuracy: 0.3401065014068856\n",
      "Test accuracy 0.248\n",
      "MarginLoss + RegLoss: 37.93376 + 11.976394 = 49.910156\n",
      "\n",
      "Epoch Number: 86\n",
      "Train Loss: 32.73086032952963 Train accuracy: 0.3579236065860286\n",
      "Test accuracy 0.3538\n",
      "MarginLoss + RegLoss: 23.608856 + 11.904265 = 35.513123\n",
      "\n",
      "Epoch Number: 87\n",
      "Train Loss: 29.19536032911908 Train accuracy: 0.36649183327574364\n",
      "Test accuracy 0.3831\n",
      "MarginLoss + RegLoss: 14.627347 + 11.849673 = 26.47702\n",
      "\n",
      "Epoch Number: 88\n",
      "Train Loss: 97.91274629259323 Train accuracy: 0.3061138692859042\n",
      "Test accuracy 0.286\n",
      "MarginLoss + RegLoss: 83.44024 + 11.801956 = 95.242195\n",
      "\n",
      "Epoch Number: 89\n",
      "Train Loss: 190.09628521701146 Train accuracy: 0.28679532343897585\n",
      "Test accuracy 0.4037\n",
      "MarginLoss + RegLoss: 15.388399 + 11.92098 = 27.30938\n",
      "\n",
      "Epoch Number: 90\n",
      "Train Loss: 53.51725494487403 Train accuracy: 0.3229500323600833\n",
      "Test accuracy 0.3859\n",
      "MarginLoss + RegLoss: 14.363257 + 11.846442 = 26.2097\n",
      "\n",
      "Epoch Number: 91\n",
      "Train Loss: 27.97903319217699 Train accuracy: 0.36993513737425143\n",
      "Test accuracy 0.2826\n",
      "MarginLoss + RegLoss: 34.566612 + 11.759778 = 46.32639\n",
      "\n",
      "Epoch Number: 92\n",
      "Train Loss: 21.391312004739394 Train accuracy: 0.381005766413137\n",
      "Test accuracy 0.3972\n",
      "MarginLoss + RegLoss: 7.445221 + 11.672655 = 19.117876\n",
      "\n",
      "Epoch Number: 93\n",
      "Train Loss: 20.318353567422772 Train accuracy: 0.3675128119943388\n",
      "Test accuracy 0.4038\n",
      "MarginLoss + RegLoss: 6.7640247 + 11.598406 = 18.36243\n",
      "\n",
      "Epoch Number: 94\n",
      "Train Loss: 42.810686727275765 Train accuracy: 0.33137812345150874\n",
      "Test accuracy 0.3785\n",
      "MarginLoss + RegLoss: 11.1003685 + 11.54097 = 22.641338\n",
      "\n",
      "Epoch Number: 95\n",
      "Train Loss: 19.603509757550842 Train accuracy: 0.37303811819564064\n",
      "Test accuracy 0.3701\n",
      "MarginLoss + RegLoss: 7.0715036 + 11.480235 = 18.551739\n",
      "\n",
      "Epoch Number: 96\n",
      "Train Loss: 36.45411365662989 Train accuracy: 0.3216888222993757\n",
      "Test accuracy 0.3746\n",
      "MarginLoss + RegLoss: 12.584608 + 11.403692 = 23.9883\n",
      "\n",
      "Epoch Number: 97\n",
      "Train Loss: 299.91159149991023 Train accuracy: 0.2901385328628023\n",
      "Test accuracy 0.1923\n",
      "MarginLoss + RegLoss: 576.7645 + 11.471156 = 588.23566\n",
      "\n",
      "Epoch Number: 98\n",
      "Train Loss: 185.95670004703538 Train accuracy: 0.28285153804873137\n",
      "Test accuracy 0.3768\n",
      "MarginLoss + RegLoss: 14.168294 + 11.533444 = 25.701738\n",
      "\n",
      "Epoch Number: 99\n",
      "Train Loss: 1917.699623997436 Train accuracy: 0.234865471002366\n",
      "Test accuracy 0.0989\n",
      "MarginLoss + RegLoss: 5145.8765 + 12.741959 = 5158.6187\n",
      "\n",
      "Maximum Test accuracy at compressed model size(including early stopping): 0.4185 at Epoch: 65\n",
      "Final Test Accuracy: 0.0989\n"
     ]
    }
   ],
   "source": [
    "totalEpochs = 100\n",
    "batchSize = np.maximum(1000, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    zs = np.sum(np.abs(tree.Z.eval())<0.000000000000001)/(tree.Z.eval()!=None).sum()\n",
    "    ws = np.sum(np.abs(tree.W.eval())<0.000000000000001)/(tree.W.eval()!=None).sum()\n",
    "    vs = np.sum(np.abs(tree.V.eval())<0.000000000000001)/(tree.V.eval()!=None).sum()\n",
    "    ts = np.sum(np.abs(tree.T.eval())<0.000000000000001)/(tree.T.eval()!=None).sum()\n",
    "    print('Sparse ratios achieved...\\nW:',1-ws,'\\nV:',1-vs,'\\nT:',1-ts,'\\nZ:',1-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse ratios achieved...\n",
      "W: 0.30000000000000004 \n",
      "V: 0.30000000000000004 \n",
      "T: 0.6173469387755102 \n",
      "Z: 0.20011117287381874\n"
     ]
    }
   ],
   "source": [
    "calc_zero_ratios(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
