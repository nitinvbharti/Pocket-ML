{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is :  /media/abhikcr/New Volume/Study/M.tech 1st Sem/Into to ML/project/Project/Models/Bonsai/\n"
     ]
    }
   ],
   "source": [
    "dir_path = (os.getcwd() + \"\\\\\").replace(\"\\\\\",\"/\") # If it does not work change it to path where data is stored.\n",
    "print(\"Working directory is : \", dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'abalone', 'abalone_dataset.txt', 'cifar', 'cifar_preprocessed.zip', 'iris', 'iris_dataset.txt', 'madelon', 'Madelon-dataset-a-training-set-NH-539-b-test-set-NH-5097-c-training.png', 'madelon_preprocessed.zip', 'mnist_small', 'mnist_small.zip', 'notMNIST.pickle', 'preprocessed Char Dataset.zip', 'Untitled.ipynb', 'usps10_preprocessed', 'usps10_preprocessed.zip', 'Xtest.npy', 'Xtrain.npy', 'Xtrain2.npy', 'Xval.npy', 'Ytest.npy', 'Ytrain.npy', 'Ytrain2.npy', 'Yval.npy']\n"
     ]
    }
   ],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = dir_path + '/../../Datasets/'\n",
    "print(os.listdir(dirc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = np.load(dirc + 'Xtrain2.npy').reshape(-1,28*28)\n",
    "Ytrain = np.load(dirc + 'Ytrain2.npy')\n",
    "Xtest = np.load(dirc + 'Xtest.npy').reshape(-1,28*28)\n",
    "Ytest = np.load(dirc + 'Ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "Ytrain = mo2.fit_transform(mo1.fit_transform((Ytrain.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "Ytest = mo2.transform(mo1.transform((Ytest.ravel())).reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 784), (10000, 784), (100000, 10), (10000, 10))"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape,Ytrain.shape,Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 100000 ,Data Dims: 784 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "\n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "#         if nClasses == 2:\n",
    "#             self.nClasses = 1\n",
    "#         else:\n",
    "#             self.nClasses = nClasses\n",
    "        self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.dDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),self.pDims) # dimensions are D^x1\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx1\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x1\n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x1\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx1\n",
    "            \n",
    "            \n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 1):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, htc):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = 1 # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if (itersInPhase % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "                    Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (100 * self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 0.1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > htc*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhikcr/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims =20, tDepth = 3, sigma = 1.0)\n",
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.00001, lT = 0.00001, lV = 0.00001, lZ = 0.0000001, lr = 0.01, X = X, Y = Y,\n",
    "                              sZ = 0.6999, sW = 0.3999, sV = 0.3999, sT = 0.3999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "\n",
      "\n",
      "Hard Thresolding Started\n",
      "\n",
      "\n",
      "Train Loss: 1.4450122976303101 Train accuracy: 0.6526499985158444\n",
      "Test accuracy 0.5292\n",
      "MarginLoss + RegLoss: 2.159745 + 0.026180433 = 2.1859255\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 0.8142144817113877 Train accuracy: 0.7777999997138977\n",
      "Test accuracy 0.6954\n",
      "MarginLoss + RegLoss: 1.2455549 + 0.024836991 = 1.270392\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 0.686976028084755 Train accuracy: 0.8086700010299682\n",
      "Test accuracy 0.7259\n",
      "MarginLoss + RegLoss: 1.1251478 + 0.023618642 = 1.1487665\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 0.6295856809616089 Train accuracy: 0.8221000003814697\n",
      "Test accuracy 0.7526\n",
      "MarginLoss + RegLoss: 1.0398822 + 0.022514656 = 1.0623969\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 0.5920388013124466 Train accuracy: 0.8315800005197524\n",
      "Test accuracy 0.77\n",
      "MarginLoss + RegLoss: 0.977005 + 0.021454593 = 0.9984596\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: 0.5650784310698509 Train accuracy: 0.8381800001859665\n",
      "Test accuracy 0.7817\n",
      "MarginLoss + RegLoss: 0.8828699 + 0.020442918 = 0.9033128\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: 0.5439680385589599 Train accuracy: 0.8432400006055832\n",
      "Test accuracy 0.7957\n",
      "MarginLoss + RegLoss: 0.770875 + 0.019501017 = 0.790376\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: 0.5263675966858864 Train accuracy: 0.8481199991703033\n",
      "Test accuracy 0.8073\n",
      "MarginLoss + RegLoss: 0.7211301 + 0.018655872 = 0.73978597\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: 0.512157654762268 Train accuracy: 0.8519000005722046\n",
      "Test accuracy 0.8086\n",
      "MarginLoss + RegLoss: 0.70101297 + 0.017862672 = 0.71887565\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: 0.5005671057105064 Train accuracy: 0.8550000011920929\n",
      "Test accuracy 0.8104\n",
      "MarginLoss + RegLoss: 0.6976228 + 0.017103298 = 0.7147261\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: 0.4900236439704895 Train accuracy: 0.857810001373291\n",
      "Test accuracy 0.8119\n",
      "MarginLoss + RegLoss: 0.685644 + 0.016420916 = 0.7020649\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: 0.48186212956905367 Train accuracy: 0.8605400013923645\n",
      "Test accuracy 0.8027\n",
      "MarginLoss + RegLoss: 0.7021351 + 0.015782623 = 0.7179177\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: 0.47756988167762754 Train accuracy: 0.8614299964904785\n",
      "Test accuracy 0.815\n",
      "MarginLoss + RegLoss: 0.67735994 + 0.015179069 = 0.69253904\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: 0.47229005485773085 Train accuracy: 0.8618999975919723\n",
      "Test accuracy 0.8479\n",
      "MarginLoss + RegLoss: 0.5552203 + 0.01462764 = 0.56984794\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: 0.46737313449382784 Train accuracy: 0.8636500006914138\n",
      "Test accuracy 0.8207\n",
      "MarginLoss + RegLoss: 0.646032 + 0.014119227 = 0.6601512\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: 0.46733411490917204 Train accuracy: 0.8636900001764297\n",
      "Test accuracy 0.8523\n",
      "MarginLoss + RegLoss: 0.516755 + 0.013617908 = 0.5303729\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: 0.459574713408947 Train accuracy: 0.8658900010585785\n",
      "Test accuracy 0.8674\n",
      "MarginLoss + RegLoss: 0.460701 + 0.013261323 = 0.4739623\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: 0.45347827911376953 Train accuracy: 0.8673200005292893\n",
      "Test accuracy 0.8557\n",
      "MarginLoss + RegLoss: 0.50601107 + 0.012942175 = 0.51895326\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: 0.448019312620163 Train accuracy: 0.869119999408722\n",
      "Test accuracy 0.8522\n",
      "MarginLoss + RegLoss: 0.52264684 + 0.012661107 = 0.53530794\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: 0.4473846897482872 Train accuracy: 0.8691699999570847\n",
      "Test accuracy 0.8638\n",
      "MarginLoss + RegLoss: 0.47118956 + 0.012371996 = 0.48356155\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: 0.44254005163908006 Train accuracy: 0.8698400026559829\n",
      "Test accuracy 0.8679\n",
      "MarginLoss + RegLoss: 0.46569306 + 0.012141112 = 0.47783417\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: 0.44949558436870574 Train accuracy: 0.8682899987697601\n",
      "Test accuracy 0.9146\n",
      "MarginLoss + RegLoss: 0.2889051 + 0.011712337 = 0.30061746\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: 0.43921638309955596 Train accuracy: 0.8700799995660782\n",
      "Test accuracy 0.8771\n",
      "MarginLoss + RegLoss: 0.4234389 + 0.011482532 = 0.43492144\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: 0.4472523111104965 Train accuracy: 0.8679399985074997\n",
      "Test accuracy 0.9173\n",
      "MarginLoss + RegLoss: 0.2800927 + 0.011146627 = 0.29123932\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: 0.43589544504880906 Train accuracy: 0.8718599969148636\n",
      "Test accuracy 0.8926\n",
      "MarginLoss + RegLoss: 0.374037 + 0.010990867 = 0.38502786\n",
      "\n",
      "Epoch Number: 25\n",
      "Train Loss: 0.43362733066082 Train accuracy: 0.871800000667572\n",
      "Test accuracy 0.8854\n",
      "MarginLoss + RegLoss: 0.39394042 + 0.010791908 = 0.40473232\n",
      "\n",
      "Epoch Number: 26\n",
      "Train Loss: 0.4358843544125557 Train accuracy: 0.8712699979543685\n",
      "Test accuracy 0.9173\n",
      "MarginLoss + RegLoss: 0.28257868 + 0.01053647 = 0.29311514\n",
      "\n",
      "Epoch Number: 27\n",
      "Train Loss: 0.4381187951564789 Train accuracy: 0.8703899991512298\n",
      "Test accuracy 0.9187\n",
      "MarginLoss + RegLoss: 0.2733349 + 0.01031205 = 0.28364694\n",
      "\n",
      "Epoch Number: 28\n",
      "Train Loss: 0.440461733341217 Train accuracy: 0.8698199993371963\n",
      "Test accuracy 0.8993\n",
      "MarginLoss + RegLoss: 0.35177276 + 0.010034062 = 0.3618068\n",
      "\n",
      "Epoch Number: 29\n",
      "Train Loss: 0.43861577808856966 Train accuracy: 0.8711599999666214\n",
      "Test accuracy 0.9206\n",
      "MarginLoss + RegLoss: 0.26951802 + 0.009819534 = 0.27933756\n",
      "\n",
      "Epoch Number: 30\n",
      "Train Loss: 0.45307312190532684 Train accuracy: 0.8661499989032745\n",
      "Test accuracy 0.9204\n",
      "MarginLoss + RegLoss: 0.27225858 + 0.009372442 = 0.28163102\n",
      "\n",
      "Epoch Number: 31\n",
      "Train Loss: 0.4486469408869743 Train accuracy: 0.8675500005483627\n",
      "Test accuracy 0.9061\n",
      "MarginLoss + RegLoss: 0.3186103 + 0.009070218 = 0.32768053\n",
      "\n",
      "Epoch Number: 32\n",
      "Train Loss: 0.4596971616148949 Train accuracy: 0.8649299997091293\n",
      "Test accuracy 0.9057\n",
      "MarginLoss + RegLoss: 0.31796435 + 0.008713829 = 0.3266782\n",
      "\n",
      "Epoch Number: 33\n",
      "Train Loss: 0.46458765864372253 Train accuracy: 0.8635499984025955\n",
      "Test accuracy 0.9129\n",
      "MarginLoss + RegLoss: 0.2973296 + 0.008191226 = 0.30552083\n",
      "\n",
      "Epoch Number: 34\n",
      "Train Loss: 0.46267478704452514 Train accuracy: 0.8645199984312057\n",
      "Test accuracy 0.914\n",
      "MarginLoss + RegLoss: 0.2953537 + 0.0077748857 = 0.3031286\n",
      "\n",
      "Epoch Number: 35\n",
      "Train Loss: 0.45353896856307985 Train accuracy: 0.8658699989318848\n",
      "Test accuracy 0.9087\n",
      "MarginLoss + RegLoss: 0.3111647 + 0.0075662043 = 0.31873092\n",
      "\n",
      "Epoch Number: 36\n",
      "Train Loss: 0.4587093663215637 Train accuracy: 0.8645300018787384\n",
      "Test accuracy 0.9142\n",
      "MarginLoss + RegLoss: 0.28919205 + 0.0073702666 = 0.2965623\n",
      "\n",
      "Epoch Number: 37\n",
      "Train Loss: 0.444112289249897 Train accuracy: 0.8680099999904632\n",
      "Test accuracy 0.9153\n",
      "MarginLoss + RegLoss: 0.28351015 + 0.0071274783 = 0.2906376\n",
      "\n",
      "Epoch Number: 38\n",
      "Train Loss: 0.443122084736824 Train accuracy: 0.8687199997901917\n",
      "Test accuracy 0.9151\n",
      "MarginLoss + RegLoss: 0.28616285 + 0.006956125 = 0.29311898\n",
      "\n",
      "Epoch Number: 39\n",
      "Train Loss: 0.43954421520233156 Train accuracy: 0.8693299967050553\n",
      "Test accuracy 0.9161\n",
      "MarginLoss + RegLoss: 0.28311798 + 0.006762306 = 0.28988028\n",
      "\n",
      "Epoch Number: 40\n",
      "Train Loss: 0.4364351299405098 Train accuracy: 0.870920000076294\n",
      "Test accuracy 0.9152\n",
      "MarginLoss + RegLoss: 0.28585362 + 0.0066307094 = 0.29248434\n",
      "\n",
      "Epoch Number: 41\n",
      "Train Loss: 0.4358829700946808 Train accuracy: 0.8707100015878677\n",
      "Test accuracy 0.9154\n",
      "MarginLoss + RegLoss: 0.2874778 + 0.006479477 = 0.29395726\n",
      "\n",
      "Epoch Number: 42\n",
      "Train Loss: 0.4377123427391052 Train accuracy: 0.8704100006818771\n",
      "Test accuracy 0.9191\n",
      "MarginLoss + RegLoss: 0.2784678 + 0.0062504234 = 0.28471822\n",
      "\n",
      "Epoch Number: 43\n",
      "Train Loss: 0.4544146123528481 Train accuracy: 0.8644300019741058\n",
      "Test accuracy 0.9154\n",
      "MarginLoss + RegLoss: 0.28642952 + 0.006045108 = 0.29247463\n",
      "\n",
      "Epoch Number: 44\n",
      "Train Loss: 0.4508777275681496 Train accuracy: 0.8657800000905991\n",
      "Test accuracy 0.9086\n",
      "MarginLoss + RegLoss: 0.30024257 + 0.005862446 = 0.30610502\n",
      "\n",
      "Epoch Number: 45\n",
      "Train Loss: 0.4687793159484863 Train accuracy: 0.8603900021314621\n",
      "Test accuracy 0.9059\n",
      "MarginLoss + RegLoss: 0.32217327 + 0.00567651 = 0.32784978\n",
      "\n",
      "Epoch Number: 46\n",
      "Train Loss: 0.46844074070453645 Train accuracy: 0.8606699955463409\n",
      "Test accuracy 0.9122\n",
      "MarginLoss + RegLoss: 0.2917088 + 0.0054667015 = 0.2971755\n",
      "\n",
      "Epoch Number: 47\n",
      "Train Loss: 0.44383658170700074 Train accuracy: 0.8675899988412857\n",
      "Test accuracy 0.9165\n",
      "MarginLoss + RegLoss: 0.28427517 + 0.005418479 = 0.28969365\n",
      "\n",
      "Epoch Number: 48\n",
      "Train Loss: 0.4411578133702278 Train accuracy: 0.8686800014972687\n",
      "Test accuracy 0.9127\n",
      "MarginLoss + RegLoss: 0.28955936 + 0.0054170936 = 0.29497647\n",
      "\n",
      "Epoch Number: 49\n",
      "Train Loss: 0.4356828466057777 Train accuracy: 0.8708299994468689\n",
      "Test accuracy 0.9185\n",
      "MarginLoss + RegLoss: 0.2807345 + 0.005347442 = 0.28608194\n",
      "\n",
      "Epoch Number: 50\n",
      "Train Loss: 0.43319501608610156 Train accuracy: 0.8708900028467178\n",
      "Test accuracy 0.9199\n",
      "MarginLoss + RegLoss: 0.2760815 + 0.005309298 = 0.2813908\n",
      "\n",
      "Epoch Number: 51\n",
      "Train Loss: 0.4402401441335678 Train accuracy: 0.8689800006151199\n",
      "Test accuracy 0.9186\n",
      "MarginLoss + RegLoss: 0.27576008 + 0.005235609 = 0.2809957\n",
      "\n",
      "Epoch Number: 52\n",
      "Train Loss: 0.45803541749715804 Train accuracy: 0.8641499990224838\n",
      "Test accuracy 0.9174\n",
      "MarginLoss + RegLoss: 0.27743593 + 0.005077313 = 0.28251323\n",
      "\n",
      "Epoch Number: 53\n",
      "Train Loss: 0.44292329788208007 Train accuracy: 0.8678500014543533\n",
      "Test accuracy 0.9171\n",
      "MarginLoss + RegLoss: 0.2808661 + 0.005023731 = 0.2858898\n",
      "\n",
      "Epoch Number: 54\n",
      "Train Loss: 0.43367685496807096 Train accuracy: 0.8706000006198883\n",
      "Test accuracy 0.9202\n",
      "MarginLoss + RegLoss: 0.27141264 + 0.0050071646 = 0.27641982\n",
      "\n",
      "Epoch Number: 55\n",
      "Train Loss: 0.43095154702663424 Train accuracy: 0.8714599996805191\n",
      "Test accuracy 0.9184\n",
      "MarginLoss + RegLoss: 0.2812776 + 0.0049740816 = 0.28625166\n",
      "\n",
      "Epoch Number: 56\n",
      "Train Loss: 0.43284265786409376 Train accuracy: 0.8703799986839295\n",
      "Test accuracy 0.921\n",
      "MarginLoss + RegLoss: 0.27037895 + 0.004960526 = 0.27533948\n",
      "\n",
      "Epoch Number: 57\n",
      "Train Loss: 0.43718635857105254 Train accuracy: 0.8695699948072434\n",
      "Test accuracy 0.9199\n",
      "MarginLoss + RegLoss: 0.27567008 + 0.004903869 = 0.28057396\n",
      "\n",
      "Epoch Number: 58\n",
      "Train Loss: 0.4368586966395378 Train accuracy: 0.8703799957036972\n",
      "Test accuracy 0.9219\n",
      "MarginLoss + RegLoss: 0.26784873 + 0.004927632 = 0.27277637\n",
      "\n",
      "Epoch Number: 59\n",
      "Train Loss: 0.4286736378073692 Train accuracy: 0.8725600022077561\n",
      "Test accuracy 0.9212\n",
      "MarginLoss + RegLoss: 0.26907128 + 0.0048761135 = 0.2739474\n",
      "\n",
      "Epoch Number: 60\n",
      "Train Loss: 0.42776508927345275 Train accuracy: 0.8720300012826919\n",
      "Test accuracy 0.9173\n",
      "MarginLoss + RegLoss: 0.2807672 + 0.0048672236 = 0.28563443\n",
      "\n",
      "Epoch Number: 61\n",
      "Train Loss: 0.4372358864545822 Train accuracy: 0.8697099989652634\n",
      "Test accuracy 0.9189\n",
      "MarginLoss + RegLoss: 0.27298737 + 0.004766347 = 0.2777537\n",
      "\n",
      "Epoch Number: 62\n",
      "Train Loss: 0.42720330506563187 Train accuracy: 0.8728700029850006\n",
      "Test accuracy 0.9181\n",
      "MarginLoss + RegLoss: 0.27736342 + 0.0047203056 = 0.28208372\n",
      "\n",
      "Epoch Number: 63\n",
      "Train Loss: 0.42690260738134383 Train accuracy: 0.8726700001955032\n",
      "Test accuracy 0.9179\n",
      "MarginLoss + RegLoss: 0.2793508 + 0.0047213775 = 0.28407216\n",
      "\n",
      "Epoch Number: 64\n",
      "Train Loss: 0.42981425166130066 Train accuracy: 0.8713099992275238\n",
      "Test accuracy 0.9171\n",
      "MarginLoss + RegLoss: 0.2759172 + 0.0047269627 = 0.28064418\n",
      "\n",
      "Epoch Number: 65\n",
      "Train Loss: 0.4223952063918114 Train accuracy: 0.8739899998903274\n",
      "Test accuracy 0.9208\n",
      "MarginLoss + RegLoss: 0.26695058 + 0.004729686 = 0.27168027\n",
      "\n",
      "Epoch Number: 66\n",
      "Train Loss: 0.4220477399230003 Train accuracy: 0.8740599977970124\n",
      "Test accuracy 0.9187\n",
      "MarginLoss + RegLoss: 0.26885137 + 0.0047551664 = 0.27360654\n",
      "\n",
      "Epoch Number: 67\n",
      "Train Loss: 0.4150046527385712 Train accuracy: 0.8761899983882904\n",
      "Test accuracy 0.9195\n",
      "MarginLoss + RegLoss: 0.26598677 + 0.0047587906 = 0.27074558\n",
      "\n",
      "Epoch Number: 68\n",
      "Train Loss: 0.42540995419025424 Train accuracy: 0.8726900017261505\n",
      "Test accuracy 0.9151\n",
      "MarginLoss + RegLoss: 0.2832246 + 0.0047402694 = 0.28796488\n",
      "\n",
      "Epoch Number: 69\n",
      "Train Loss: 0.4228228148818016 Train accuracy: 0.8743300014734268\n",
      "Test accuracy 0.921\n",
      "MarginLoss + RegLoss: 0.26837006 + 0.0046736235 = 0.2730437\n",
      "\n",
      "Epoch Number: 70\n",
      "Train Loss: 0.42557827293872835 Train accuracy: 0.872869998216629\n",
      "Test accuracy 0.9186\n",
      "MarginLoss + RegLoss: 0.27569842 + 0.0046240734 = 0.2803225\n",
      "\n",
      "Epoch Number: 71\n",
      "Train Loss: 0.4193492555618286 Train accuracy: 0.8762600004673005\n",
      "Test accuracy 0.9189\n",
      "MarginLoss + RegLoss: 0.27021715 + 0.0046056057 = 0.27482274\n",
      "\n",
      "Epoch Number: 72\n",
      "Train Loss: 0.4173300689458847 Train accuracy: 0.8751399976015091\n",
      "Test accuracy 0.9184\n",
      "MarginLoss + RegLoss: 0.27216956 + 0.0046041925 = 0.27677375\n",
      "\n",
      "Epoch Number: 73\n",
      "Train Loss: 0.42077242404222487 Train accuracy: 0.874060002565384\n",
      "Test accuracy 0.9209\n",
      "MarginLoss + RegLoss: 0.26749247 + 0.004630679 = 0.27212316\n",
      "\n",
      "Epoch Number: 74\n",
      "Train Loss: 0.4113321331143379 Train accuracy: 0.8777400010824203\n",
      "Test accuracy 0.9229\n",
      "MarginLoss + RegLoss: 0.26461804 + 0.004661311 = 0.26927936\n",
      "\n",
      "Epoch Number: 75\n",
      "Train Loss: 0.40653873383998873 Train accuracy: 0.8789900010824203\n",
      "Test accuracy 0.9237\n",
      "MarginLoss + RegLoss: 0.26079795 + 0.004701173 = 0.26549911\n",
      "\n",
      "Epoch Number: 76\n",
      "Train Loss: 0.4078743052482605 Train accuracy: 0.8786200004816055\n",
      "Test accuracy 0.9223\n",
      "MarginLoss + RegLoss: 0.2650711 + 0.0047150427 = 0.26978615\n",
      "\n",
      "Epoch Number: 77\n",
      "Train Loss: 0.4064615133404732 Train accuracy: 0.8793100011348725\n",
      "Test accuracy 0.9227\n",
      "MarginLoss + RegLoss: 0.26772022 + 0.004737531 = 0.27245775\n",
      "\n",
      "Epoch Number: 78\n",
      "Train Loss: 0.40639283806085585 Train accuracy: 0.8799000000953674\n",
      "Test accuracy 0.9237\n",
      "MarginLoss + RegLoss: 0.2690699 + 0.0047898176 = 0.27385974\n",
      "\n",
      "Epoch Number: 79\n",
      "Train Loss: 0.4172294920682907 Train accuracy: 0.8762900024652481\n",
      "Test accuracy 0.9215\n",
      "MarginLoss + RegLoss: 0.26915595 + 0.004747463 = 0.2739034\n",
      "\n",
      "Epoch Number: 80\n",
      "Train Loss: 0.419650160074234 Train accuracy: 0.8752800011634827\n",
      "Test accuracy 0.9243\n",
      "MarginLoss + RegLoss: 0.25990152 + 0.0046768542 = 0.26457837\n",
      "\n",
      "Epoch Number: 81\n",
      "Train Loss: 0.4077576020359993 Train accuracy: 0.8784200012683868\n",
      "Test accuracy 0.9238\n",
      "MarginLoss + RegLoss: 0.26315758 + 0.0047076885 = 0.26786527\n",
      "\n",
      "Epoch Number: 82\n",
      "Train Loss: 0.4081274911761284 Train accuracy: 0.8789600020647049\n",
      "Test accuracy 0.9215\n",
      "MarginLoss + RegLoss: 0.2698831 + 0.0047065536 = 0.27458966\n",
      "\n",
      "Epoch Number: 83\n",
      "Train Loss: 0.405053443312645 Train accuracy: 0.8796599996089935\n",
      "Test accuracy 0.9227\n",
      "MarginLoss + RegLoss: 0.265199 + 0.0047336244 = 0.26993263\n",
      "\n",
      "Epoch Number: 84\n",
      "Train Loss: 0.41224699050188063 Train accuracy: 0.8781499987840653\n",
      "Test accuracy 0.9217\n",
      "MarginLoss + RegLoss: 0.26921976 + 0.0047428184 = 0.2739626\n",
      "\n",
      "Epoch Number: 85\n",
      "Train Loss: 0.4093010202050209 Train accuracy: 0.8784400010108948\n",
      "Test accuracy 0.9228\n",
      "MarginLoss + RegLoss: 0.2647243 + 0.004756612 = 0.2694809\n",
      "\n",
      "Epoch Number: 86\n",
      "Train Loss: 0.4033836382627487 Train accuracy: 0.8799999988079071\n",
      "Test accuracy 0.9239\n",
      "MarginLoss + RegLoss: 0.26177016 + 0.0047793733 = 0.26654953\n",
      "\n",
      "Epoch Number: 87\n",
      "Train Loss: 0.4048889511823654 Train accuracy: 0.8791500014066697\n",
      "Test accuracy 0.9229\n",
      "MarginLoss + RegLoss: 0.26806983 + 0.0047721057 = 0.27284193\n",
      "\n",
      "Epoch Number: 88\n",
      "Train Loss: 0.410538749396801 Train accuracy: 0.8777900016307831\n",
      "Test accuracy 0.9214\n",
      "MarginLoss + RegLoss: 0.26647535 + 0.0047596 = 0.27123496\n",
      "\n",
      "Epoch Number: 89\n",
      "Train Loss: 0.4182350528240204 Train accuracy: 0.8761700016260147\n",
      "Test accuracy 0.9244\n",
      "MarginLoss + RegLoss: 0.26709923 + 0.00472355 = 0.27182278\n",
      "\n",
      "Epoch Number: 90\n",
      "Train Loss: 0.404929381608963 Train accuracy: 0.8802100014686585\n",
      "Test accuracy 0.9221\n",
      "MarginLoss + RegLoss: 0.26904607 + 0.004747653 = 0.27379373\n",
      "\n",
      "Epoch Number: 91\n",
      "Train Loss: 0.4040800416469574 Train accuracy: 0.879840002655983\n",
      "Test accuracy 0.9235\n",
      "MarginLoss + RegLoss: 0.2682003 + 0.004757539 = 0.27295786\n",
      "\n",
      "Epoch Number: 92\n",
      "Train Loss: 0.402271184027195 Train accuracy: 0.880210000872612\n",
      "Test accuracy 0.9175\n",
      "MarginLoss + RegLoss: 0.29109287 + 0.0047353087 = 0.2958282\n",
      "\n",
      "Epoch Number: 93\n",
      "Train Loss: 0.425934174656868 Train accuracy: 0.8725600010156631\n",
      "Test accuracy 0.9221\n",
      "MarginLoss + RegLoss: 0.27291343 + 0.004658727 = 0.27757215\n",
      "\n",
      "Epoch Number: 94\n",
      "Train Loss: 0.4106481420993805 Train accuracy: 0.8772299998998642\n",
      "Test accuracy 0.9235\n",
      "MarginLoss + RegLoss: 0.26880458 + 0.00464702 = 0.2734516\n",
      "\n",
      "Epoch Number: 95\n",
      "Train Loss: 0.4084032714366913 Train accuracy: 0.8783200019598008\n",
      "Test accuracy 0.9224\n",
      "MarginLoss + RegLoss: 0.2686782 + 0.0046388456 = 0.27331704\n",
      "\n",
      "Epoch Number: 96\n",
      "Train Loss: 0.40265644997358324 Train accuracy: 0.8797800004482269\n",
      "Test accuracy 0.9231\n",
      "MarginLoss + RegLoss: 0.26737788 + 0.0046477206 = 0.27202561\n",
      "\n",
      "Epoch Number: 97\n",
      "Train Loss: 0.4038955843448639 Train accuracy: 0.8800200009346009\n",
      "Test accuracy 0.9233\n",
      "MarginLoss + RegLoss: 0.27038488 + 0.0046520927 = 0.27503696\n",
      "\n",
      "Epoch Number: 98\n",
      "Train Loss: 0.415634765625 Train accuracy: 0.8760199987888336\n",
      "Test accuracy 0.9224\n",
      "MarginLoss + RegLoss: 0.27231944 + 0.0046077045 = 0.27692714\n",
      "\n",
      "Epoch Number: 99\n",
      "Train Loss: 0.4119900372624397 Train accuracy: 0.8775799959897995\n",
      "Test accuracy 0.9241\n",
      "MarginLoss + RegLoss: 0.26384616 + 0.0045957253 = 0.2684419\n",
      "\n",
      "Maximum Test accuracy at compressed model size(including early stopping): 0.9244 at Epoch: 90\n",
      "Final Test Accuracy: 0.9241\n"
     ]
    }
   ],
   "source": [
    "totalEpochs = 100\n",
    "batchSize = np.maximum(1000, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest, htc = 0.00)\n",
    "# print('Time taken',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    xZ = tree.Z.eval()\n",
    "    xW = tree.W.eval()\n",
    "    xV = tree.V.eval()\n",
    "    xT = tree.T.eval()\n",
    "    zs = np.sum(np.abs(xZ)>0.0000000000000001)\n",
    "    ws = np.sum(np.abs(xW)>0.0000000000000001)\n",
    "    vs = np.sum(np.abs(xV)>0.0000000000000001)\n",
    "    ts = np.sum(np.abs(xT)>0.0000000000000001)\n",
    "    print('Sparse ratios achieved...\\nW:',ws,xW.shape,'\\nV:',vs,xV.shape,'\\nT:',ts,xT.shape,'\\nZ:',zs,xZ.shape)\n",
    "    _feed_dict = {bonsaiTrainer.X: Xtest, bonsaiTrainer.Y: Ytest,\n",
    "                            bonsaiTrainer.sigmaI: 10e9}\n",
    "    print('Net',ws+zs+vs+ts)\n",
    "    start = time.time()\n",
    "    sess.run(bonsaiTrainer.tree.prediction, feed_dict=_feed_dict)\n",
    "    end = time.time()\n",
    "    print('Time taken :', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse ratios achieved...\n",
      "W: 1200 (150, 20) \n",
      "V: 1200 (150, 20) \n",
      "T: 56 (7, 20) \n",
      "Z: 10974 (20, 784)\n",
      "Net 13430\n",
      "Time taken : 0.19434881210327148\n"
     ]
    }
   ],
   "source": [
    "calc_zero_ratios(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
