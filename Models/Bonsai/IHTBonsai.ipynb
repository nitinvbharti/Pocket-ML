{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is :  /media/abhikcr/New Volume/Study/M.tech 1st Sem/Into to ML/project/Project/Models/Bonsai/\n"
     ]
    }
   ],
   "source": [
    "dir_path = (os.getcwd() + \"\\\\\").replace(\"\\\\\",\"/\") # If it does not work change it to path where data is stored.\n",
    "print(\"Working directory is : \", dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessed iris.zip', 'X_test.npy', 'X_train.npy', 'Y_test.npy', 'Y_train.npy']\n"
     ]
    }
   ],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = dir_path + '/../../Datasets/iris/'\n",
    "print(os.listdir(dirc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = np.load(dirc + 'X_train.npy')\n",
    "Ytrain = np.load(dirc + 'Y_train.npy')\n",
    "Xtest = np.load(dirc + 'X_test.npy')\n",
    "Ytest = np.load(dirc + 'Y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "mo1 = LE()\n",
    "mo2 = OHE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytrain = mo2.fit_transform(mo1.fit_transform((Ytrain)).reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "Ytest = mo2.transform(mo1.transform((Ytest)).reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 4), (38, 4), (112, 3), (38, 3))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape,Ytrain.shape,Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 112 ,Data Dims: 4 ,No. Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "\n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.dDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),self.pDims) # dimensions are D^x1\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx1\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x1\n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x1\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx1\n",
    "            \n",
    "            \n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, htc):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = 1 # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if (itersInPhase % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "                    Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (100 * self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 0.1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > htc*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 3, tDepth = 2, sigma = 1.0)\n",
    "\n",
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "\n",
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.001, lT = 0.001, lV = 0.001, lZ = 0.0001, lr = 0.01, X = X, Y = Y,\n",
    "                              sZ = 0.2999, sW = 0.2999, sV = 0.2999, sT = 0.2999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "\n",
      "\n",
      "Hard Thresolding Started\n",
      "\n",
      "\n",
      "Train Loss: 1.1960808277130126 Train accuracy: 0.3900000035762787\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.4425656 + 0.057546273 = 1.5001118\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 1.0455241680145264 Train accuracy: 0.5700000047683715\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.2820411 + 0.05648101 = 1.3385221\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 0.9584359288215637 Train accuracy: 0.6399999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.1473861 + 0.055320602 = 1.2027067\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 0.8921676397323608 Train accuracy: 0.6399999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0533763 + 0.054092903 = 1.1074692\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 0.8498525857925415 Train accuracy: 0.6399999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.001907 + 0.052897226 = 1.0548042\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: 0.8248063206672669 Train accuracy: 0.6399999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 0.9829647 + 0.051801138 = 1.0347658\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: 0.8066212892532348 Train accuracy: 0.800000011920929\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 0.9817255 + 0.050830755 = 1.0325563\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: 0.7880166530609131 Train accuracy: 0.7899999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 0.9885449 + 0.04998318 = 1.0385281\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: 0.7674865961074829 Train accuracy: 0.8200000047683715\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0003352 + 0.049239524 = 1.0495747\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: 0.7468565583229065 Train accuracy: 0.7400000095367432\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0157433 + 0.04857326 = 1.0643165\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: 0.7277873277664184 Train accuracy: 0.6899999976158142\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0320927 + 0.04795691 = 1.0800496\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: 0.7103813409805297 Train accuracy: 0.6599999904632569\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0462483 + 0.04736904 = 1.0936173\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: 0.6939534664154052 Train accuracy: 0.6599999904632569\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0566574 + 0.04679914 = 1.1034566\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: 0.6781403541564941 Train accuracy: 0.6599999904632569\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0639989 + 0.046247274 = 1.1102462\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: 0.6630824327468872 Train accuracy: 0.699999988079071\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0702734 + 0.0457194 = 1.1159928\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: 0.648951256275177 Train accuracy: 0.7600000023841857\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0774848 + 0.045221627 = 1.1227064\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: 0.6356574535369873 Train accuracy: 0.7700000047683716\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0868257 + 0.044756282 = 1.131582\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: 0.6229879856109619 Train accuracy: 0.7900000095367432\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0984632 + 0.04432105 = 1.1427842\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: 0.6108142733573914 Train accuracy: 0.800000011920929\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.1117028 + 0.043910667 = 1.1556134\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: 0.5991029500961303 Train accuracy: 0.800000011920929\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.1254472 + 0.043519862 = 1.168967\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: 1.0755560874938965 Train accuracy: 0.3700000047683716\n",
      "Test accuracy 0.342105\n",
      "MarginLoss + RegLoss: 1.0734465 + 0.042938106 = 1.1163846\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: 1.0696722984313964 Train accuracy: 0.3700000047683716\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 1.0062913 + 0.042139422 = 1.0484307\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: 1.0110092401504516 Train accuracy: 0.549999988079071\n",
      "Test accuracy 0.394737\n",
      "MarginLoss + RegLoss: 1.0035385 + 0.041342244 = 1.0448807\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: 0.9964571356773376 Train accuracy: 0.4399999976158142\n",
      "Test accuracy 0.394737\n",
      "MarginLoss + RegLoss: 0.9749012 + 0.04061689 = 1.0155181\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: 0.9665884971618652 Train accuracy: 0.5200000047683716\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.9251778 + 0.039966393 = 0.9651442\n",
      "\n",
      "Epoch Number: 25\n",
      "Train Loss: 0.9377553343772889 Train accuracy: 0.5599999904632569\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.89245486 + 0.039368875 = 0.93182373\n",
      "\n",
      "Epoch Number: 26\n",
      "Train Loss: 0.9227605819702148 Train accuracy: 0.47000000476837156\n",
      "Test accuracy 0.473684\n",
      "MarginLoss + RegLoss: 0.8766173 + 0.038801372 = 0.9154187\n",
      "\n",
      "Epoch Number: 27\n",
      "Train Loss: 0.913977324962616 Train accuracy: 0.4199999988079071\n",
      "Test accuracy 0.473684\n",
      "MarginLoss + RegLoss: 0.8689999 + 0.03825562 = 0.90725553\n",
      "\n",
      "Epoch Number: 28\n",
      "Train Loss: 0.907559072971344 Train accuracy: 0.47000000476837156\n",
      "Test accuracy 0.526316\n",
      "MarginLoss + RegLoss: 0.8665741 + 0.037735425 = 0.9043095\n",
      "\n",
      "Epoch Number: 29\n",
      "Train Loss: 0.9039129614830017 Train accuracy: 0.5100000023841857\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.86661166 + 0.03724573 = 0.9038574\n",
      "\n",
      "Epoch Number: 30\n",
      "Train Loss: 0.9019766211509704 Train accuracy: 0.5199999928474426\n",
      "Test accuracy 0.578947\n",
      "MarginLoss + RegLoss: 0.86582303 + 0.036787305 = 0.90261036\n",
      "\n",
      "Epoch Number: 31\n",
      "Train Loss: 0.8999349713325501 Train accuracy: 0.5299999952316284\n",
      "Test accuracy 0.578947\n",
      "MarginLoss + RegLoss: 0.86303604 + 0.036357135 = 0.8993932\n",
      "\n",
      "Epoch Number: 32\n",
      "Train Loss: 0.8972513318061829 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.85927564 + 0.03595048 = 0.8952261\n",
      "\n",
      "Epoch Number: 33\n",
      "Train Loss: 0.8943300247192383 Train accuracy: 0.5099999964237213\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.85583067 + 0.03556268 = 0.89139336\n",
      "\n",
      "Epoch Number: 34\n",
      "Train Loss: 0.891457486152649 Train accuracy: 0.5\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8531783 + 0.035190698 = 0.888369\n",
      "\n",
      "Epoch Number: 35\n",
      "Train Loss: 0.8886530637741089 Train accuracy: 0.5\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.85115844 + 0.034833815 = 0.8859922\n",
      "\n",
      "Epoch Number: 36\n",
      "Train Loss: 0.8859354257583618 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.84934145 + 0.034492854 = 0.8838343\n",
      "\n",
      "Epoch Number: 37\n",
      "Train Loss: 0.883315360546112 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.84726465 + 0.03416881 = 0.8814335\n",
      "\n",
      "Epoch Number: 38\n",
      "Train Loss: 0.8807132601737976 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8446903 + 0.03386178 = 0.8785521\n",
      "\n",
      "Epoch Number: 39\n",
      "Train Loss: 0.8780351758003235 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8417184 + 0.033570882 = 0.87528926\n",
      "\n",
      "Epoch Number: 40\n",
      "Train Loss: 0.8752513766288758 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.83861583 + 0.03329485 = 0.8719107\n",
      "\n",
      "Epoch Number: 41\n",
      "Train Loss: 0.8723780035972595 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.83557785 + 0.033032663 = 0.8686105\n",
      "\n",
      "Epoch Number: 42\n",
      "Train Loss: 0.8694427251815796 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8326331 + 0.03278388 = 0.86541694\n",
      "\n",
      "Epoch Number: 43\n",
      "Train Loss: 0.8664683699607849 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8296909 + 0.032548472 = 0.86223936\n",
      "\n",
      "Epoch Number: 44\n",
      "Train Loss: 0.8634581446647644 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.82664585 + 0.032326423 = 0.85897225\n",
      "\n",
      "Epoch Number: 45\n",
      "Train Loss: 0.8581972360610962 Train accuracy: 0.5100000023841857\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.8114809 + 0.032119904 = 0.8436008\n",
      "\n",
      "Epoch Number: 46\n",
      "Train Loss: 0.8426691293716431 Train accuracy: 0.5100000023841857\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.7935923 + 0.03194177 = 0.82553405\n",
      "\n",
      "Epoch Number: 47\n",
      "Train Loss: 0.8266314625740051 Train accuracy: 0.5199999988079071\n",
      "Test accuracy 0.552632\n",
      "MarginLoss + RegLoss: 0.7753845 + 0.031800114 = 0.8071846\n",
      "\n",
      "Epoch Number: 48\n",
      "Train Loss: 0.809640622138977 Train accuracy: 0.5399999916553497\n",
      "Test accuracy 0.710526\n",
      "MarginLoss + RegLoss: 0.7564705 + 0.031698562 = 0.7881691\n",
      "\n",
      "Epoch Number: 49\n",
      "Train Loss: 0.791220486164093 Train accuracy: 0.6900000095367431\n",
      "Test accuracy 0.842105\n",
      "MarginLoss + RegLoss: 0.73679876 + 0.03163967 = 0.76843846\n",
      "\n",
      "Maximum Test accuracy at compressed model size(including early stopping): 0.84210527 at Epoch: 50\n",
      "Final Test Accuracy: 0.84210527\n"
     ]
    }
   ],
   "source": [
    "totalEpochs = 50\n",
    "batchSize = np.maximum(20, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest, htc = 0)\n",
    "# print('Time taken',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    xZ = tree.Z.eval()\n",
    "    xW = tree.W.eval()\n",
    "    xV = tree.V.eval()\n",
    "    xT = tree.T.eval()\n",
    "    zs = np.sum(np.abs(xZ)>0.000000000000001)\n",
    "    ws = np.sum(np.abs(xW)>0.000000000000001)\n",
    "    vs = np.sum(np.abs(xV)>0.000000000000001)\n",
    "    ts = np.sum(np.abs(xT)>0.000000000000001)\n",
    "    print('Sparse ratios achieved...\\nW:',ws,xW.shape,'\\nV:',vs,xV.shape,'\\nT:',ts,xT.shape,'\\nZ:',zs,xZ.shape)\n",
    "    _feed_dict = {bonsaiTrainer.X: Xtest, bonsaiTrainer.Y: Ytest,\n",
    "                            bonsaiTrainer.sigmaI: 10e9}\n",
    "    start = time.time()\n",
    "    sess.run(bonsaiTrainer.tree.prediction, feed_dict=_feed_dict)\n",
    "    end = time.time()\n",
    "    print('Time taken :', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse ratios achieved...\n",
      "W: 44 (21, 3) \n",
      "V: 44 (21, 3) \n",
      "T: 6 (3, 3) \n",
      "Z: 8 (3, 4)\n",
      "Time taken : 1.6680240631103516\n"
     ]
    }
   ],
   "source": [
    "calc_zero_ratios(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
