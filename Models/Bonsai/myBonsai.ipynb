{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import helpermethods\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS Data\n",
    "\n",
    "It is assumed that the USPS data has already been downloaded and set up with the help of [fetch_usps.py](fetch_usps.py) and is present in the `./usps10` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading and Pre-processing dataset for Bonsai\n",
    "dataDir = \"usps10/\"\n",
    "train_data = np.load(dataDir+'train.npy')\n",
    "test_data = np.load(dataDir+'test.npy')\n",
    "\n",
    "\n",
    "(dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest) = helpermethods.preProcessData(dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 7291 ,Data Dims: 257 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# X_train = train_data[:,1:]\n",
    "# Y_train = train_data[:,0]-1\n",
    "# X_test = test_data[:,1:]\n",
    "# Y_test = test_data[:,0]-1\n",
    "\n",
    "# X_train = Xtrain[:,:-1]\n",
    "# Y_train = Ytrain\n",
    "# X_test = Xtest[:,:-1]\n",
    "# Y_test = Ytest\n",
    "\n",
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = numClasses\n",
    "\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "obj = Xtrain[i][:-1]\n",
    "plt.imshow(obj.reshape(16,16))\n",
    "plt.show()\n",
    "print(Ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "\n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.dDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),self.pDims) # dimensions are D^x1\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx1\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x1\n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x1\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx1\n",
    "            \n",
    "            \n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 28, tDepth = 3, sigma = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "#         # sparsity parameters (scalars all...)\n",
    "#         self.sW = sW \n",
    "#         self.sV = sV\n",
    "#         self.sT = sT\n",
    "#         self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # loss function selection wweather to use multiclass hinge loss or cross entropy...\n",
    "#         self.useMCHLoss = useMCHLoss\n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "#         if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "#             self.isDenseTraining = True\n",
    "#         else:\n",
    "#             self.isDenseTraining = False\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy function analys ????????????????????????????\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction))\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            self.marginLoss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y))\n",
    "        else:\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "#         assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "#         assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "#         assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "#         assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval):\n",
    "        \n",
    "        numIters = Xtrain.shape[0] / batchSize\n",
    "        totalBatches = numIters * totalEpochs\n",
    "        treeSigmaI = 1\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            \n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            for j in range(numIters):\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            \n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\")\n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.001, lT = 0.001, lV = 0.001, lZ = 0.0001, lr = 0.01, X = X, Y = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "Train Loss: [5.496841  5.991543  5.1780963 5.613557  5.9743466 5.963231  5.5959854\n",
      " 5.5178466 5.2486873 5.419047  5.965454  5.154045  5.840745  5.3643093\n",
      " 5.2653184 5.4758587 5.464982  5.9576488 5.7589307 5.273904  5.6338477\n",
      " 5.342203  6.1539364 5.364893  5.611688  5.7077227 5.480014  5.329619\n",
      " 5.78118   5.5566998 5.5087605 5.1771703 5.1951795 4.861643  5.392177\n",
      " 4.891657  6.110908  5.9574146 5.7543745 6.2443395 5.3728204 6.029709\n",
      " 5.2663918 5.7345004 5.3765974 5.550967  5.7815404 5.9799137 6.111258\n",
      " 6.034113  5.331949  5.7413025 5.143195  5.784162  5.595525  5.9848123\n",
      " 6.087693  5.3438053 5.497574  5.428857  5.601908  5.617603  5.656782\n",
      " 6.632741  5.173471  5.4560423 5.615888  5.6333613 5.487074  5.2410474\n",
      " 5.8371983 5.8218417 5.3654537 5.449     5.898903  5.404603  5.658823\n",
      " 5.5090137 5.784151  5.3345275 5.8109503 6.1126957 5.737497  5.6498604\n",
      " 5.3324842 4.951092  5.274816  5.587141  6.9094157 6.040583  5.9504557\n",
      " 5.8429875 5.493472  5.5589595 5.9251294 5.72906   5.4952407 5.5094123\n",
      " 5.5963397 5.4134994] Train accuracy: 0.6624999980752667\n",
      "Test accuracy 0.791729\n",
      "MarginLoss + RegLoss: [0.16917014 0.23059845 0.00640154 ... 0.39762044 0.         0.00504231] + 3.3216 = [3.49077   3.5521984 3.3280015 ... 3.7192204 3.3216    3.3266423]\n",
      "\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: [3.358334  3.1841717 3.2372653 3.312514  3.1747122 3.274332  3.1496665\n",
      " 3.234423  3.0964682 3.1539066 3.2267733 3.082973  3.0178938 3.2499902\n",
      " 3.210372  3.0326526 3.1507168 3.1624107 3.3241794 3.0762131 3.364717\n",
      " 3.2098453 3.2943892 3.1733994 3.175411  3.4384096 3.203829  3.176024\n",
      " 3.3356285 3.150443  3.4913797 3.1720614 2.9951413 3.165061  3.0390973\n",
      " 2.9720743 3.5611203 3.5200427 3.5539548 3.482933  3.459682  3.3550746\n",
      " 3.109511  3.1941419 3.181654  3.1447492 3.3006697 3.2520247 3.2730494\n",
      " 3.2753267 3.1138132 3.1205099 3.141883  3.2889013 3.2279863 3.4556482\n",
      " 3.4018612 3.2988384 3.1298761 3.321609  3.1424584 3.0993397 3.3246095\n",
      " 3.607     3.1865246 3.3405592 3.249654  3.41469   3.4429061 3.0340128\n",
      " 3.5041943 3.0149639 3.2547157 2.976914  3.2186813 3.0971994 3.141286\n",
      " 3.2660346 3.195094  3.080124  3.1037471 3.3614364 3.3571234 3.570119\n",
      " 3.066034  3.0783248 3.1673906 3.3936644 4.011429  3.551216  3.4155555\n",
      " 3.1210105 3.1583395 3.3397226 3.3770118 3.1922483 3.3081384 3.2966409\n",
      " 3.317123  3.1681376] Train accuracy: 0.8838888886902068\n",
      "Test accuracy 0.844544\n",
      "MarginLoss + RegLoss: [0.03482676 0.01660514 0.00622225 ... 0.01498199 0.         0.00194979] + 2.3886406 = [2.4234674 2.4052458 2.394863  ... 2.4036226 2.3886406 2.3905904]\n",
      "\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: [2.394886  2.21198   2.3006752 2.3305712 2.235444  2.3418245 2.2411823\n",
      " 2.2773888 2.2176154 2.1763678 2.2976868 2.193925  2.1314104 2.3200197\n",
      " 2.260881  2.1512904 2.273694  2.2158601 2.3007267 2.203401  2.301754\n",
      " 2.2971265 2.363147  2.264676  2.2246885 2.3565712 2.23178   2.2534974\n",
      " 2.2894182 2.253354  2.394039  2.2798195 2.1338673 2.2040045 2.1387115\n",
      " 2.119056  2.4978628 2.453021  2.5114813 2.429672  2.4122458 2.3390265\n",
      " 2.24882   2.2264006 2.2622006 2.292176  2.37495   2.3152657 2.2481718\n",
      " 2.3109133 2.178312  2.2028148 2.161128  2.2528152 2.2667553 2.432364\n",
      " 2.341602  2.2921033 2.1741116 2.3713725 2.2803953 2.207625  2.3518317\n",
      " 2.5500193 2.2758074 2.311893  2.264005  2.4612393 2.3674963 2.1300077\n",
      " 2.4168115 2.1565123 2.3361855 2.1170483 2.2547753 2.212491  2.23292\n",
      " 2.320362  2.248811  2.1957164 2.2108839 2.3112311 2.3553114 2.4970741\n",
      " 2.1848426 2.1773157 2.2506328 2.3034117 2.778357  2.4392123 2.4002848\n",
      " 2.2063518 2.2773035 2.2913713 2.344041  2.3265715 2.476601  2.3287375\n",
      " 2.305502  2.202119 ] Train accuracy: 0.9333333323399226\n",
      "Test accuracy 0.871948\n",
      "MarginLoss + RegLoss: [1.1874795e-02 4.8605204e-03 8.3647966e-03 ... 1.3375878e-02 3.5762787e-07\n",
      " 1.5256405e-03] + 1.7504274 = [1.7623022 1.7552879 1.7587922 ... 1.7638032 1.7504277 1.751953 ]\n",
      "\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: [1.7874966 1.6400256 1.6874038 1.6967334 1.6743529 1.7375324 1.6533906\n",
      " 1.6855359 1.6503419 1.6284845 1.6946555 1.6307184 1.5879619 1.744999\n",
      " 1.7075197 1.5928506 1.7064912 1.6547841 1.7057874 1.6498729 1.7156501\n",
      " 1.7038257 1.7743412 1.7075992 1.6410174 1.708178  1.6814148 1.6803486\n",
      " 1.6395597 1.6684417 1.7437238 1.7260394 1.5826908 1.6573926 1.5854431\n",
      " 1.5795507 1.866172  1.8318068 1.8418994 1.8071327 1.7993948 1.6813862\n",
      " 1.6999948 1.6687783 1.6968411 1.7223605 1.7232515 1.6819072 1.6585315\n",
      " 1.7243081 1.5949354 1.6404111 1.6041151 1.6858629 1.6911348 1.7583578\n",
      " 1.7343756 1.6583374 1.6317589 1.7241917 1.7133232 1.6498572 1.7316725\n",
      " 1.8731327 1.7055287 1.681414  1.6615782 1.8712881 1.723493  1.5952\n",
      " 1.8090248 1.5943986 1.727367  1.575391  1.6592743 1.653172  1.6515188\n",
      " 1.6864626 1.7531679 1.6437058 1.6363289 1.6883297 1.727644  1.8559625\n",
      " 1.6314131 1.6110787 1.6463556 1.656202  1.9715497 1.7951268 1.768585\n",
      " 1.6344762 1.6968533 1.6811234 1.7239012 1.7545496 1.8503221 1.7126715\n",
      " 1.6805549 1.622603 ] Train accuracy: 0.9538888906439146\n",
      "Test accuracy 0.887394\n",
      "MarginLoss + RegLoss: [7.3556900e-03 7.6866150e-03 3.4999847e-03 ... 1.3899922e-02 1.9073486e-06\n",
      " 1.1295080e-03] + 1.3227643 = [1.33012   1.3304509 1.3262643 ... 1.3366642 1.3227662 1.3238938]\n",
      "\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: [1.3873905 1.2723829 1.3023962 1.3144208 1.3062643 1.3571771 1.2759141\n",
      " 1.3020754 1.2795761 1.2678821 1.3015149 1.2605515 1.2220297 1.3519114\n",
      " 1.33376   1.2310357 1.3392943 1.2825267 1.293584  1.2780194 1.3383032\n",
      " 1.3258004 1.3888705 1.3359578 1.2730702 1.300852  1.3046155 1.3134351\n",
      " 1.2496521 1.2708901 1.3375823 1.3497671 1.2137461 1.2799809 1.2196189\n",
      " 1.2165719 1.4242226 1.4589753 1.3816228 1.4002883 1.402579  1.2917598\n",
      " 1.3336792 1.30122   1.3205132 1.3455987 1.3230135 1.2910275 1.2850207\n",
      " 1.3352178 1.2299163 1.2740337 1.2328992 1.3206549 1.318454  1.3339095\n",
      " 1.3428078 1.2843969 1.2662946 1.3325701 1.3281434 1.2753925 1.3339419\n",
      " 1.4461118 1.337948  1.2738934 1.2767699 1.4763595 1.3358369 1.2345293\n",
      " 1.4296342 1.2282342 1.3228027 1.2190062 1.2758771 1.2705032 1.2607316\n",
      " 1.2965109 1.31023   1.2666013 1.258638  1.2764587 1.37453   1.4305091\n",
      " 1.2619863 1.2369858 1.2555145 1.2653475 1.5056173 1.3755935 1.3798399\n",
      " 1.2683244 1.3036214 1.2990429 1.3316371 1.3652287 1.4440181 1.2996836\n",
      " 1.283583  1.2670778] Train accuracy: 0.9626388963725832\n",
      "Test accuracy 0.898356\n",
      "MarginLoss + RegLoss: [4.5135021e-03 1.5613675e-02 1.3470650e-03 ... 2.0610929e-02 6.7949295e-06\n",
      " 9.2935562e-04] + 1.0346882 = [1.0392017 1.0503019 1.0360353 ... 1.0552992 1.034695  1.0356176]\n",
      "\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: [1.1301665  1.0249915  1.0479481  1.0540438  1.0408466  1.0920424\n",
      " 1.0207976  1.0375857  1.0389053  1.0257926  1.0452169  1.0115657\n",
      " 0.9745313  1.0786796  1.0812409  0.9805726  1.0855972  1.0301069\n",
      " 1.0294988  1.0312327  1.0895405  1.082207   1.1214951  1.0731969\n",
      " 1.0257024  1.0198121  1.0495007  1.0600162  0.9957873  1.0048106\n",
      " 1.0648141  1.0902772  0.96497154 1.023804   0.9722376  0.9741824\n",
      " 1.1339782  1.2118484  1.0971546  1.1240789  1.1318604  1.0371594\n",
      " 1.0821857  1.0489718  1.0665708  1.0864406  1.0582316  1.0302327\n",
      " 1.0141165  1.0768185  0.9982057  1.0253569  0.9976087  1.060574\n",
      " 1.070621   1.0565376  1.0763532  1.0316504  1.0219959  1.0636435\n",
      " 1.064202   1.02037    1.0720086  1.1679432  1.0944611  1.0180968\n",
      " 1.0249401  1.1925797  1.0804124  0.9898741  1.167922   0.97462994\n",
      " 1.0561488  0.9742377  1.0196186  1.0211223  1.0021756  1.0650724\n",
      " 1.040442   1.0086862  1.0066967  1.0287678  1.0640938  1.1485672\n",
      " 1.0288541  0.9897461  0.9997215  1.0206084  1.1728517  1.1032718\n",
      " 1.1082741  1.0261605  1.0378762  1.0480753  1.0667554  1.1012851\n",
      " 1.1546865  1.0291419  1.0223767  0.99839646] Train accuracy: 0.9694444545441203\n",
      "Test accuracy 0.906826\n",
      "MarginLoss + RegLoss: [3.4419894e-03 2.0944834e-02 6.4402819e-04 ... 3.3411980e-02 3.0994415e-06\n",
      " 7.8958273e-04] + 0.8367765 = [0.8402185 0.8577213 0.8374205 ... 0.8701885 0.8367796 0.8375661]\n",
      "\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: [0.95533776 0.8548622  0.8756459  0.8843514  0.86008215 0.9034225\n",
      " 0.8490004  0.8570595  0.86767805 0.8586838  0.8754147  0.83912677\n",
      " 0.802099   0.8847432  0.9046361  0.81451327 0.9026334  0.86249256\n",
      " 0.8476733  0.85780716 0.916319   0.91244465 0.9302518  0.9024957\n",
      " 0.86260927 0.84931046 0.87612224 0.8762684  0.8264659  0.8253053\n",
      " 0.8851846  0.9067619  0.7941076  0.8475438  0.802856   0.8090794\n",
      " 0.94186944 1.0528274  0.90525985 0.9272144  0.94844365 0.8603746\n",
      " 0.9090072  0.8755629  0.8963945  0.9129617  0.8773396  0.855245\n",
      " 0.8397805  0.89756274 0.83028996 0.84667695 0.81240153 0.8911743\n",
      " 0.90020967 0.88188833 0.8921884  0.8577012  0.8544878  0.8908529\n",
      " 0.8836267  0.8452515  0.8875435  0.9701428  0.922434   0.845791\n",
      " 0.8518894  0.9897322  0.90038735 0.81696934 0.9908145  0.8046911\n",
      " 0.877229   0.8049563  0.8420484  0.85329956 0.82837373 0.86495465\n",
      " 0.85126024 0.8368849  0.8332581  0.8461703  0.8792245  0.95675987\n",
      " 0.85756654 0.81596255 0.826308   0.8326877  0.9704301  0.9173639\n",
      " 0.9281826  0.8589311  0.8562825  0.8816775  0.8805488  0.9111696\n",
      " 0.96339947 0.85293835 0.8441944  0.8273448 ] Train accuracy: 0.973333343035645\n",
      "Test accuracy 0.913802\n",
      "MarginLoss + RegLoss: [2.2118688e-03 2.5926232e-02 4.5168400e-04 ... 3.9530098e-02 2.8610229e-06\n",
      " 7.0333481e-04] + 0.69863665 = [0.7008485  0.7245629  0.69908834 ... 0.73816675 0.6986395  0.69934   ]\n",
      "\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: [0.83464724 0.74126077 0.7593982  0.75819457 0.7323653  0.7699588\n",
      " 0.7237483  0.7271192  0.74826837 0.73963046 0.74825907 0.71581143\n",
      " 0.6816255  0.7498728  0.7787593  0.6938083  0.7718362  0.74009126\n",
      " 0.7188416  0.7375211  0.79480463 0.792339   0.7873977  0.78005105\n",
      " 0.7467925  0.7177517  0.7565024  0.76067376 0.70250684 0.6992142\n",
      " 0.75196904 0.7805268  0.6727134  0.7256854  0.6850858  0.6936255\n",
      " 0.8008845  0.9107949  0.7737304  0.7927387  0.8140209  0.73351085\n",
      " 0.7841056  0.75251365 0.7698973  0.78935874 0.7496239  0.7309775\n",
      " 0.72408605 0.77072746 0.7098476  0.7284496  0.69464684 0.76823115\n",
      " 0.7734756  0.750005   0.7579024  0.7268023  0.7348373  0.7627797\n",
      " 0.7565787  0.7215934  0.7572872  0.824902   0.79923755 0.7205693\n",
      " 0.7273192  0.8450971  0.7725794  0.69465595 0.86463237 0.6852691\n",
      " 0.74466217 0.6821688  0.7141485  0.734597   0.7073496  0.7441528\n",
      " 0.73348683 0.7137573  0.71145225 0.71224034 0.75165796 0.823685\n",
      " 0.73600763 0.69334483 0.70323443 0.70793724 0.8189824  0.783361\n",
      " 0.79291654 0.74243635 0.7290385  0.76685685 0.7503062  0.77406055\n",
      " 0.8347854  0.7302817  0.71897304 0.70260465] Train accuracy: 0.9765277869171567\n",
      "Test accuracy 0.917289\n",
      "MarginLoss + RegLoss: [1.6132593e-03 2.9396474e-02 3.4803152e-04 ... 3.7627161e-02 2.6226044e-06\n",
      " 6.2578917e-04] + 0.5996639 = [0.6012772  0.6290604  0.60001194 ... 0.6372911  0.59966654 0.6002897 ]\n",
      "\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: [0.7483035  0.6535162  0.6730913  0.66519904 0.6342215  0.6773374\n",
      " 0.6329196  0.6340309  0.6574148  0.6521685  0.66239583 0.62499183\n",
      " 0.59289    0.64480513 0.68642235 0.6084335  0.6777139  0.6527493\n",
      " 0.62631345 0.6484427  0.710066   0.70644027 0.6828369  0.6945007\n",
      " 0.66281193 0.6326163  0.66855913 0.6713973  0.61650753 0.608809\n",
      " 0.6583308  0.6873924  0.5852326  0.6375505  0.5985363  0.6106916\n",
      " 0.70340455 0.81541765 0.67721045 0.69487643 0.7191937  0.6415251\n",
      " 0.69504607 0.6652345  0.68494105 0.70325065 0.66053194 0.63588476\n",
      " 0.6396783  0.6800302  0.6244821  0.63390255 0.60518545 0.68245345\n",
      " 0.67988354 0.6556675  0.66183734 0.63340145 0.6476028  0.67554617\n",
      " 0.6672294  0.632258   0.6622135  0.72658205 0.70898736 0.6323507\n",
      " 0.6386941  0.73899156 0.6804839  0.60336536 0.7709748  0.6007194\n",
      " 0.6528852  0.59520996 0.6217852  0.6484149  0.61895883 0.6480577\n",
      " 0.6314972  0.6268139  0.6241631  0.6240242  0.66184086 0.7274703\n",
      " 0.64508367 0.6042394  0.6161275  0.61810935 0.7290609  0.6882982\n",
      " 0.69289136 0.6596341  0.6446717  0.6833597  0.657892   0.67508173\n",
      " 0.74638796 0.6392695  0.6294325  0.6137667 ] Train accuracy: 0.9783333440621694\n",
      "Test accuracy 0.923269\n",
      "MarginLoss + RegLoss: [1.2366176e-03 2.5606096e-02 2.7543306e-04 ... 3.0849099e-02 2.3841858e-06\n",
      " 5.3489208e-04] + 0.52746516 = [0.5287018  0.55307126 0.5277406  ... 0.55831426 0.52746755 0.52800006]\n",
      "\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: [0.68531215 0.58976895 0.60511005 0.5945829  0.563268   0.6089343\n",
      " 0.56544876 0.565247   0.5916153  0.5862333  0.5982542  0.5563696\n",
      " 0.5278162  0.5659089  0.61640376 0.5440555  0.60757744 0.5855079\n",
      " 0.5572096  0.5820979  0.65397364 0.64048886 0.6102546  0.63209754\n",
      " 0.601045   0.56413454 0.6047152  0.60475105 0.5534263  0.5415975\n",
      " 0.5887436  0.620063   0.52062726 0.5716767  0.5337007  0.5495641\n",
      " 0.63029957 0.741533   0.61032796 0.6212045  0.6485517  0.5728851\n",
      " 0.62899566 0.600945   0.6226962  0.638279   0.59179497 0.5693354\n",
      " 0.5793383  0.61355233 0.5622714  0.5639868  0.54183114 0.6193953\n",
      " 0.6097439  0.5850754  0.5879342  0.564561   0.5813201  0.6101916\n",
      " 0.6014807  0.56530136 0.59211475 0.652614   0.6380205  0.56842446\n",
      " 0.57134986 0.66175026 0.61448765 0.5358654  0.7003101  0.53951824\n",
      " 0.5847672  0.53144425 0.5534105  0.5838482  0.5542273  0.5759231\n",
      " 0.55548155 0.5625806  0.5604589  0.55823016 0.5931294  0.6518128\n",
      " 0.5785289  0.5375164  0.55088246 0.55120736 0.6567509  0.61636233\n",
      " 0.6188499  0.5994286  0.58275175 0.62113327 0.5894449  0.6003983\n",
      " 0.67963254 0.57065004 0.5629093  0.5476678 ] Train accuracy: 0.97902778784434\n",
      "Test accuracy 0.922272\n",
      "MarginLoss + RegLoss: [1.0498166e-03 1.7735571e-02 2.6652217e-04 ... 2.5941253e-02 1.9073486e-06\n",
      " 4.3657422e-04] + 0.47328538 = [0.4743352  0.49102095 0.4735519  ... 0.49922663 0.47328728 0.47372195]\n",
      "\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: [0.6374638  0.5417702  0.550503   0.5403726  0.50995946 0.5583343\n",
      " 0.5140474  0.5143923  0.5404205  0.53502625 0.54917747 0.50280803\n",
      " 0.4791496  0.50644153 0.56079054 0.4934659  0.5537138  0.5328405\n",
      " 0.5037019  0.5315852  0.6081708  0.5890229  0.55542606 0.5820605\n",
      " 0.55631477 0.5114304  0.55754465 0.5538295  0.50635403 0.4902222\n",
      " 0.5361359  0.5694411  0.47185633 0.52114654 0.4841991  0.50336593\n",
      " 0.5699135  0.68139386 0.55708754 0.565236   0.59389716 0.52112347\n",
      " 0.5780728  0.55050445 0.5751402  0.58700234 0.5415885  0.5202925\n",
      " 0.53490555 0.5637901  0.51451397 0.51228607 0.49425542 0.57195926\n",
      " 0.5554353  0.52970904 0.5322164  0.5110592  0.5286056  0.5621773\n",
      " 0.5512092  0.51461715 0.53809404 0.59630483 0.58152264 0.5225902\n",
      " 0.51838756 0.604566   0.56414485 0.48496294 0.64610076 0.4942947\n",
      " 0.5329063  0.48067585 0.5016562  0.532928   0.50412285 0.5231669\n",
      " 0.49577698 0.5140784  0.5130391  0.50639176 0.5406802  0.5926795\n",
      " 0.5273508  0.4865728  0.50044185 0.5040642  0.6010436  0.5615354\n",
      " 0.5628164  0.55264795 0.53677416 0.5745774  0.5382534  0.5424395\n",
      " 0.62862784 0.5158409  0.5129747  0.4968455 ] Train accuracy: 0.9800000091393789\n",
      "Test accuracy 0.92277\n",
      "MarginLoss + RegLoss: [8.4924698e-04 1.2982249e-02 2.0965934e-04 ... 2.2647530e-02 1.4305115e-06\n",
      " 3.5578012e-04] + 0.431482 = [0.43233123 0.44446424 0.43169165 ... 0.45412952 0.43148342 0.43183777]\n",
      "\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: [0.5982835  0.50231755 0.50295407 0.49776638 0.46864012 0.51820886\n",
      " 0.473592   0.47595876 0.4995222  0.49469104 0.5119337  0.45929635\n",
      " 0.44201475 0.46021545 0.5155259  0.45353305 0.5107906  0.48975208\n",
      " 0.46192694 0.49035168 0.57106185 0.5488661  0.5112425  0.54239035\n",
      " 0.51985925 0.47160047 0.5206401  0.5141015  0.47144005 0.4500338\n",
      " 0.49598318 0.52995193 0.43413103 0.4803695  0.44508383 0.4674238\n",
      " 0.5147284  0.632643   0.5094874  0.52180743 0.55173326 0.48014417\n",
      " 0.53780895 0.51022375 0.53827125 0.54361033 0.49871075 0.48186976\n",
      " 0.49823543 0.5245665  0.47776234 0.472163   0.45810494 0.53468704\n",
      " 0.5120933  0.48454    0.48919603 0.4682392  0.48537683 0.5262015\n",
      " 0.51115614 0.47405922 0.49613482 0.5504788  0.5351603  0.4898705\n",
      " 0.475748   0.5595964  0.5245717  0.44560343 0.60141397 0.4581638\n",
      " 0.49039587 0.44016504 0.4612106  0.49129164 0.4632784  0.48133627\n",
      " 0.44977468 0.47623315 0.47603995 0.4656169  0.5001123  0.5439941\n",
      " 0.4878967  0.4463712  0.46078756 0.46534765 0.5522233  0.5180255\n",
      " 0.5189205  0.51559156 0.50263953 0.5375717  0.4961826  0.4954084\n",
      " 0.5879341  0.47255114 0.47463465 0.45693508] Train accuracy: 0.9809722329179446\n",
      "Test accuracy 0.92277\n",
      "MarginLoss + RegLoss: [6.2802434e-04 9.4519854e-03 1.8152595e-04 ... 2.0335168e-02 1.1920929e-06\n",
      " 2.9927492e-04] + 0.3981938 = [0.39882183 0.4076458  0.39837533 ... 0.41852897 0.398195   0.39849308]\n",
      "\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: [0.5643262  0.4683103  0.4604304  0.46199098 0.43679494 0.48624876\n",
      " 0.44109237 0.44587225 0.46602583 0.46152878 0.48139423 0.42320442\n",
      " 0.41221893 0.4248485  0.4767018  0.42017138 0.47560018 0.45447695\n",
      " 0.42727283 0.4564393  0.5405886  0.5154223  0.4749285  0.5083864\n",
      " 0.48842674 0.43831372 0.4913713  0.48291126 0.4445597  0.41780412\n",
      " 0.46377093 0.49434873 0.40401217 0.44731602 0.41394502 0.43847883\n",
      " 0.46686733 0.58999574 0.46733624 0.48708093 0.5194581  0.44754076\n",
      " 0.5047541  0.4770084  0.50712293 0.5077098  0.46384996 0.44921175\n",
      " 0.4659675  0.49312675 0.44851732 0.44013083 0.42900512 0.50364035\n",
      " 0.47624943 0.44647682 0.45730767 0.4351334  0.4492417  0.49526608\n",
      " 0.478351   0.44050658 0.46323115 0.5139865  0.49714816 0.4641768\n",
      " 0.44103348 0.51990914 0.49270904 0.41391358 0.5640538  0.42775717\n",
      " 0.4545419  0.40794456 0.42935032 0.45537457 0.42917565 0.4458046\n",
      " 0.41418415 0.44494662 0.44611347 0.4317043  0.46937323 0.5044324\n",
      " 0.45840326 0.4139063  0.4288599  0.43454537 0.5122508  0.48130953\n",
      " 0.48188585 0.48357323 0.47276613 0.50670254 0.46110228 0.45669973\n",
      " 0.5537387  0.43799952 0.4448544  0.42648283] Train accuracy: 0.9823611237936549\n",
      "Test accuracy 0.923269\n",
      "MarginLoss + RegLoss: [4.4956803e-04 5.5142939e-03 1.4448166e-04 ... 1.6981214e-02 9.5367432e-07\n",
      " 2.5936961e-04] + 0.37091663 = [0.3713662  0.37643093 0.37106112 ... 0.38789785 0.3709176  0.371176  ]\n",
      "\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: [0.53429985 0.43854174 0.42545164 0.43209484 0.41000545 0.45941842\n",
      " 0.41548288 0.42001954 0.43909687 0.43423328 0.4542201  0.39337677\n",
      " 0.38618597 0.39622617 0.44316995 0.39209974 0.4454659  0.42487982\n",
      " 0.40016067 0.4287976  0.51498556 0.4857105  0.44563845 0.48122495\n",
      " 0.45778963 0.40980375 0.46575034 0.45488447 0.42132205 0.39063683\n",
      " 0.43631694 0.46176732 0.37903535 0.42028037 0.38784704 0.4137573\n",
      " 0.4322793  0.553739   0.4347247  0.4582084  0.4924251  0.41995943\n",
      " 0.47658396 0.44922146 0.48146263 0.478487   0.4343464  0.42038956\n",
      " 0.43899333 0.4661861  0.42635068 0.4140522  0.4059145  0.4785604\n",
      " 0.44642237 0.4148239  0.43120658 0.40729523 0.41842714 0.46710205\n",
      " 0.45072216 0.41188395 0.43474418 0.4829883  0.46427453 0.43597996\n",
      " 0.41247785 0.48398623 0.46415627 0.38730645 0.53243715 0.40234092\n",
      " 0.42343274 0.38196787 0.40214336 0.42378747 0.40164652 0.41679013\n",
      " 0.385136   0.41962028 0.41711652 0.40299085 0.44584247 0.47188365\n",
      " 0.43653497 0.38667256 0.4019692  0.40463486 0.4779992  0.44979846\n",
      " 0.45205015 0.457182   0.44634375 0.47994238 0.4309858  0.42491388\n",
      " 0.5218654  0.40882695 0.4198972  0.40129465] Train accuracy: 0.9823611229658127\n",
      "Test accuracy 0.92277\n",
      "MarginLoss + RegLoss: [3.1536818e-04 3.3454895e-03 1.2087822e-04 ... 1.3000131e-02 7.1525574e-07\n",
      " 2.1907687e-04] + 0.3479243 = [0.34823966 0.35126978 0.34804517 ... 0.36092442 0.347925   0.34814337]\n",
      "\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: [0.5085329  0.41294292 0.39653298 0.40637448 0.3872444  0.43625304\n",
      " 0.39438525 0.39672104 0.41747192 0.4110096  0.42970052 0.36831033\n",
      " 0.3626061  0.37293264 0.41413498 0.3684513  0.41913474 0.40117168\n",
      " 0.37814105 0.40582386 0.49215132 0.45935425 0.42120942 0.45915854\n",
      " 0.43103313 0.38554618 0.44412476 0.43149683 0.4003937  0.36702526\n",
      " 0.41219428 0.43296635 0.35783604 0.3979579  0.36597446 0.3922111\n",
      " 0.40604267 0.5221279  0.41023263 0.43460047 0.46976674 0.39609745\n",
      " 0.45152712 0.42600405 0.4598185  0.45297542 0.40836436 0.39506608\n",
      " 0.4159721  0.44339824 0.40708965 0.39227352 0.38653615 0.45648834\n",
      " 0.41961536 0.3892307  0.41070008 0.38254875 0.3919051  0.44196922\n",
      " 0.42654908 0.38704026 0.41062173 0.4592142  0.43492842 0.40952304\n",
      " 0.38853705 0.45413795 0.4383805  0.36495632 0.50392854 0.37976122\n",
      " 0.39794388 0.36079478 0.37888858 0.39712968 0.3797246  0.3926084\n",
      " 0.36190665 0.39785507 0.39074874 0.37809128 0.42581624 0.4453856\n",
      " 0.41723475 0.36372194 0.37895048 0.38491863 0.45001957 0.42240238\n",
      " 0.42776865 0.43453646 0.42247525 0.45819664 0.40544948 0.3980335\n",
      " 0.49248302 0.38763925 0.3991288  0.3791561 ] Train accuracy: 0.9820833462807868\n",
      "Test accuracy 0.923767\n",
      "MarginLoss + RegLoss: [2.3195148e-04 1.7328858e-03 1.0085106e-04 ... 8.8452101e-03 5.9604645e-07\n",
      " 1.8331409e-04] + 0.32809415 = [0.3283261  0.32982704 0.328195   ... 0.33693936 0.32809475 0.32827747]\n",
      "\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: [0.48553276 0.3906333  0.37364918 0.38422683 0.3694216  0.41678992\n",
      " 0.37663952 0.37537375 0.39862907 0.3913755  0.41061598 0.3476495\n",
      " 0.34274012 0.3522155  0.38948077 0.34851304 0.39627016 0.3801097\n",
      " 0.36045408 0.38720006 0.47214848 0.4392338  0.39809185 0.43927476\n",
      " 0.4087406  0.36510947 0.42407665 0.4103751  0.38176334 0.3466153\n",
      " 0.39194763 0.40960202 0.33943093 0.37505612 0.3470258  0.37243378\n",
      " 0.38430548 0.49893755 0.38966683 0.41387624 0.4494441  0.37424254\n",
      " 0.42699277 0.4057834  0.44089854 0.4291187  0.38731718 0.37584826\n",
      " 0.3972273  0.4221944  0.39071876 0.37288302 0.36990318 0.43686935\n",
      " 0.40013617 0.36756635 0.3851292  0.3602815  0.3679117  0.4209602\n",
      " 0.40693128 0.36549422 0.38891256 0.43827313 0.41152084 0.388094\n",
      " 0.36704755 0.43002263 0.4139424  0.3453639  0.47784016 0.35961363\n",
      " 0.37561223 0.34182465 0.3574109  0.37488592 0.36114803 0.37131178\n",
      " 0.34192163 0.3798238  0.36872637 0.3568632  0.40767473 0.42474416\n",
      " 0.4027942  0.3446806  0.35943106 0.36001745 0.4259391  0.39957497\n",
      " 0.40821257 0.41440997 0.40357292 0.438712   0.3849556  0.3752023\n",
      " 0.46911976 0.36247778 0.37466696 0.35926732] Train accuracy: 0.9827777900629573\n",
      "Test accuracy 0.924763\n",
      "MarginLoss + RegLoss: [1.8653274e-04 1.8349588e-03 9.1075897e-05 ... 5.3660870e-03 4.7683716e-07\n",
      " 1.5401840e-04] + 0.3107919 = [0.31097844 0.31262687 0.310883   ... 0.316158   0.3107924  0.31094593]\n",
      "\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: [0.46447265 0.37152517 0.35262892 0.36566457 0.3515095  0.39877176\n",
      " 0.36082986 0.35700715 0.38297394 0.37286222 0.39113286 0.32876232\n",
      " 0.32428774 0.33604813 0.36791685 0.33064762 0.37578338 0.36300713\n",
      " 0.34361523 0.36941218 0.4539538  0.41739643 0.37810025 0.42237082\n",
      " 0.38945097 0.34716508 0.40564916 0.39212132 0.363392   0.3292218\n",
      " 0.37354323 0.3910543  0.32371125 0.3602593  0.3293707  0.35494307\n",
      " 0.364742   0.47405455 0.37469617 0.39488947 0.43172204 0.35554677\n",
      " 0.4079     0.38854015 0.42464674 0.409052   0.36900294 0.35510632\n",
      " 0.38069713 0.40436152 0.37291062 0.35583636 0.35335693 0.4168538\n",
      " 0.37593082 0.34747413 0.37079945 0.34178674 0.3490398  0.40276155\n",
      " 0.38856876 0.34670514 0.3716754  0.42148483 0.39025706 0.3647756\n",
      " 0.3492741  0.40916482 0.39431316 0.32880285 0.4534357  0.3427083\n",
      " 0.35805967 0.32686868 0.33852398 0.35545993 0.34516954 0.35529602\n",
      " 0.32412183 0.3620784  0.3495995  0.33791462 0.38946903 0.4046998\n",
      " 0.3852419  0.32797542 0.34186742 0.3499371  0.40105984 0.37968206\n",
      " 0.38928378 0.39787042 0.38551882 0.41985682 0.3661072  0.3549899\n",
      " 0.4476759  0.35161164 0.36097467 0.34139717] Train accuracy: 0.982777789235115\n",
      "Test accuracy 0.92576\n",
      "MarginLoss + RegLoss: [1.6438961e-04 6.5031648e-04 8.5353851e-05 ... 3.2362938e-03 3.5762787e-07\n",
      " 1.2958050e-04] + 0.29530916 = [0.29547355 0.29595947 0.2953945  ... 0.29854545 0.2953095  0.29543874]\n",
      "\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: [0.4470935  0.35421845 0.3357991  0.3483638  0.3369029  0.38361838\n",
      " 0.347934   0.3394011  0.36732683 0.35695305 0.37844172 0.31297794\n",
      " 0.309912   0.3192053  0.34938982 0.31623238 0.35823643 0.34617293\n",
      " 0.33073857 0.35668185 0.43762952 0.40360552 0.35820824 0.4054564\n",
      " 0.3727833  0.3327969  0.38923785 0.37569872 0.34512046 0.31350672\n",
      " 0.35762757 0.37370217 0.30898255 0.3358714  0.314546   0.33826077\n",
      " 0.34882486 0.45889202 0.35653582 0.3842158  0.41479814 0.3378245\n",
      " 0.38779867 0.3730606  0.40756655 0.38973472 0.356643   0.34090045\n",
      " 0.36383224 0.38629484 0.35612902 0.34018305 0.33895487 0.3985467\n",
      " 0.36556897 0.332201   0.34623975 0.3239397  0.33067822 0.3872498\n",
      " 0.37445295 0.3298506  0.35264784 0.40534538 0.3733207  0.34698758\n",
      " 0.3303425  0.39091408 0.37288728 0.3126662  0.4311911  0.3296522\n",
      " 0.33935475 0.31153837 0.32148015 0.33992487 0.33012936 0.33647335\n",
      " 0.31306303 0.34763727 0.33327946 0.32160753 0.3745668  0.39027184\n",
      " 0.37829107 0.31492317 0.32622987 0.32759863 0.38340884 0.3635836\n",
      " 0.37479386 0.38185966 0.37212813 0.40185606 0.35132697 0.3374449\n",
      " 0.42977968 0.32583264 0.33801877 0.32521662] Train accuracy: 0.9836111242572466\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: [1.4209747e-04 1.7463267e-03 8.0823898e-05 ... 2.0656586e-03 2.3841858e-07\n",
      " 1.1110306e-04] + 0.28154588 = [0.28168797 0.2832922  0.2816267  ... 0.28361154 0.28154612 0.28165698]\n",
      "\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: [0.42927775 0.33976826 0.3189748  0.33533627 0.32379708 0.3688745\n",
      " 0.3340538  0.324654   0.3534712  0.34235442 0.36134458 0.29908288\n",
      " 0.29430485 0.30838597 0.3331695  0.3016753  0.3424304  0.3323523\n",
      " 0.31618762 0.34058338 0.41863558 0.38221014 0.34426537 0.39154172\n",
      " 0.35599858 0.32086062 0.3720538  0.35928938 0.33262017 0.29969296\n",
      " 0.34424672 0.3605613  0.29690164 0.328736   0.30067047 0.32645172\n",
      " 0.33164674 0.43986332 0.34684622 0.36406362 0.4015034  0.3242709\n",
      " 0.37385118 0.35892832 0.39514887 0.37233454 0.34121922 0.32221353\n",
      " 0.35268342 0.37137824 0.33337948 0.32575133 0.32350773 0.37995616\n",
      " 0.3422807  0.3141936  0.3364625  0.312179   0.31516105 0.37483498\n",
      " 0.36020607 0.31467494 0.33981824 0.39342982 0.35453066 0.32879382\n",
      " 0.3159408  0.37398922 0.35806715 0.2991395  0.40857604 0.31500325\n",
      " 0.32874626 0.2996218  0.30594334 0.32360494 0.3160016  0.33097926\n",
      " 0.2943128  0.33176124 0.31848773 0.30661312 0.35440817 0.37125528\n",
      " 0.36029798 0.30242005 0.31171742 0.3207018  0.36336797 0.34800246\n",
      " 0.35590792 0.36833498 0.35742813 0.38612995 0.33553222 0.32150084\n",
      " 0.41547152 0.32253554 0.34614813 0.3106531 ] Train accuracy: 0.9838889009422727\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: [1.2576580e-04 4.9638748e-04 8.6188316e-05 ... 1.2947321e-03 1.6363561e-03\n",
      " 9.1910362e-05] + 0.26894888 = [0.26907465 0.26944527 0.26903507 ... 0.27024361 0.27058524 0.2690408 ]\n",
      "\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: [0.41079745 0.3265792  0.30608988 0.3214238  0.3198811  0.3541667\n",
      " 0.32189062 0.31082654 0.33825117 0.3285071  0.3555706  0.28860807\n",
      " 0.2826178  0.292207   0.31956685 0.28901923 0.32952392 0.31567097\n",
      " 0.30514276 0.3314954  0.4000566  0.3737226  0.32925162 0.37708366\n",
      " 0.34447753 0.30880308 0.3637581  0.34037292 0.31358725 0.2866101\n",
      " 0.33139092 0.34457418 0.28386867 0.30607736 0.28509876 0.31632456\n",
      " 0.32003814 0.42092013 0.32882243 0.35862908 0.3853419  0.30950972\n",
      " 0.35828102 0.34620324 0.3789462  0.3599864  0.33299252 0.30933723\n",
      " 0.33516845 0.35631377 0.31484556 0.3130619  0.3152445  0.36036378\n",
      " 0.33045694 0.30600777 0.32121384 0.29515794 0.30027705 0.36333853\n",
      " 0.35004506 0.30112806 0.32325113 0.3808185  0.34636152 0.31241402\n",
      " 0.30481565 0.359347   0.34197316 0.28382728 0.38955745 0.30504334\n",
      " 0.30667007 0.28736478 0.29456705 0.30760705 0.29653257 0.34739485\n",
      " 0.2886268  0.31844246 0.305462   0.29461738 0.34330797 0.35642332\n",
      " 0.3559642  0.29737294 0.29909205 0.2999555  0.34695205 0.33478603\n",
      " 0.3478488  0.35402942 0.34463334 0.36666128 0.32472813 0.30732724\n",
      " 0.3966279  0.30246055 0.3495183  0.30225942] Train accuracy: 0.9851389014058642\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: [1.1396408e-04 1.1395812e-03 8.0347061e-05 ... 9.6976757e-04 1.0544360e-03\n",
      " 8.1539154e-05] + 0.25765064 = [0.2577646  0.25879022 0.257731   ... 0.2586204  0.25870508 0.25773218]\n",
      "\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: [0.40160996 0.3169167  0.29109895 0.3096769  0.2955645  0.34422052\n",
      " 0.31334555 0.2999555  0.327332   0.315905   0.33354276 0.27315512\n",
      " 0.2703352  0.2882189  0.30539617 0.27692977 0.31678265 0.30695748\n",
      " 0.29431298 0.31831446 0.3871907  0.3485097  0.31345192 0.36353835\n",
      " 0.3300122  0.30103713 0.34827745 0.3312439  0.30013725 0.27539298\n",
      " 0.3181196  0.3371557  0.2758466  0.29672122 0.27691683 0.3050551\n",
      " 0.30589703 0.41974795 0.32187265 0.34450242 0.3728253  0.30004275\n",
      " 0.3446604  0.33558118 0.3723248  0.34023464 0.30747986 0.2945333\n",
      " 0.33099413 0.3442581  0.30711886 0.29983324 0.30268002 0.3648785\n",
      " 0.32264626 0.28669655 0.31001168 0.28858054 0.28879526 0.34774813\n",
      " 0.3391307  0.2887226  0.31258142 0.37170509 0.3243074  0.29722834\n",
      " 0.2901088  0.34438044 0.32922658 0.27551785 0.36977065 0.2951205\n",
      " 0.3061345  0.27621654 0.27928835 0.29911548 0.30340445 0.30000436\n",
      " 0.26793438 0.30629256 0.29544404 0.28201962 0.33211657 0.34388363\n",
      " 0.3396616  0.27783242 0.28646573 0.29455265 0.3323267  0.32106945\n",
      " 0.33056238 0.34449586 0.33319554 0.36228347 0.31097504 0.29526138\n",
      " 0.39356437 0.29743588 0.29888773 0.28443646] Train accuracy: 0.9844444567958514\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: [1.04784966e-04 4.83632088e-04 7.56978989e-05 ... 9.68590379e-04\n",
      " 6.02781773e-04 7.04526901e-05] + 0.2468362 = [0.24694099 0.24731983 0.2469119  ... 0.24780479 0.24743898 0.24690665]\n",
      "\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: [0.38694084 0.30660796 0.28250596 0.29735604 0.28867805 0.32980707\n",
      " 0.29991382 0.2850146  0.31403053 0.3014849  0.33374673 0.26232433\n",
      " 0.259593   0.27114397 0.29428533 0.26604488 0.30634516 0.29075503\n",
      " 0.27965963 0.30850688 0.37018347 0.33926082 0.3033514  0.3521566\n",
      " 0.32408905 0.28880885 0.3407234  0.3146278  0.29339206 0.26302877\n",
      " 0.30360794 0.3234256  0.262935   0.27836743 0.27199945 0.29098877\n",
      " 0.29479563 0.3965822  0.30996475 0.3324139  0.35966548 0.28323913\n",
      " 0.32876801 0.32449552 0.34812996 0.3349406  0.30037495 0.2896818\n",
      " 0.32435298 0.33598423 0.28585637 0.29308212 0.29174265 0.33728516\n",
      " 0.31553555 0.28491548 0.29920968 0.27139255 0.27346584 0.33587876\n",
      " 0.3289865  0.27759007 0.30393183 0.3553826  0.30863392 0.2958807\n",
      " 0.28307652 0.33507955 0.31259334 0.26240048 0.35498354 0.27855524\n",
      " 0.2802236  0.26565897 0.27051216 0.28620264 0.27491605 0.2862624\n",
      " 0.2612466  0.29492822 0.28188336 0.27436683 0.3187918  0.33187526\n",
      " 0.33189437 0.27022025 0.27456957 0.28137413 0.32819092 0.3147348\n",
      " 0.3182455  0.33329868 0.31516397 0.3486916  0.30239496 0.28166404\n",
      " 0.37932414 0.28205353 0.2882906  0.2737271 ] Train accuracy: 0.9856944547759162\n",
      "Test accuracy 0.924265\n",
      "MarginLoss + RegLoss: [1.4375150e-04 7.5609982e-04 6.6399574e-05 ... 5.8621168e-04 7.0355833e-04\n",
      " 6.8426132e-05] + 0.23682131 = [0.23696506 0.23757741 0.23688771 ... 0.23740752 0.23752487 0.23688973]\n",
      "\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: [0.37582362 0.29639104 0.2690476  0.2881775  0.2742979  0.32030338\n",
      " 0.28927267 0.2765018  0.30643243 0.29141995 0.3115162  0.25278056\n",
      " 0.25067553 0.26893294 0.28248164 0.25772962 0.29529196 0.28156087\n",
      " 0.26887977 0.29533097 0.35733205 0.32970333 0.2915197  0.33963695\n",
      " 0.314458   0.28424224 0.32602677 0.30699727 0.2726952  0.2541132\n",
      " 0.29688695 0.31568274 0.25732616 0.27175626 0.25508824 0.28473383\n",
      " 0.28017032 0.39118597 0.2967232  0.31997678 0.3448187  0.2790979\n",
      " 0.32115382 0.3124954  0.33940086 0.31503534 0.28747457 0.2716962\n",
      " 0.3110497  0.31815633 0.28197873 0.27889198 0.28308603 0.33140478\n",
      " 0.29807103 0.26788616 0.2859197  0.26456925 0.26192355 0.32830262\n",
      " 0.32082048 0.26537243 0.2888085  0.35346282 0.29832923 0.27531385\n",
      " 0.2654002  0.32028648 0.30384818 0.25400236 0.33869642 0.26967278\n",
      " 0.2749057  0.2569841  0.25517797 0.27823323 0.26440045 0.27837616\n",
      " 0.24964243 0.28425094 0.2734535  0.2628936  0.31032258 0.31613716\n",
      " 0.3216883  0.25966352 0.26473406 0.26397708 0.3045493  0.3005704\n",
      " 0.31103894 0.32517165 0.31017375 0.34062102 0.28788745 0.2696769\n",
      " 0.37169096 0.2777983  0.28162718 0.26161015] Train accuracy: 0.9855555660194821\n",
      "Test accuracy 0.92576\n",
      "MarginLoss + RegLoss: [9.5129013e-05 5.3605437e-04 7.6055527e-05 ... 8.0743432e-04 3.5017729e-04\n",
      " 6.0081482e-05] + 0.22743449 = [0.22752962 0.22797054 0.22751054 ... 0.22824192 0.22778466 0.22749457]\n",
      "\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: [0.35820433 0.28708327 0.2588445  0.27829966 0.26281574 0.3106878\n",
      " 0.28051913 0.26388064 0.29577813 0.2785706  0.30668494 0.24091373\n",
      " 0.23998253 0.2549118  0.27347767 0.24588579 0.28417575 0.26948225\n",
      " 0.25780553 0.28719118 0.34538114 0.31367016 0.28211468 0.32822585\n",
      " 0.3048563  0.2729385  0.30999258 0.28826317 0.2661022  0.24363725\n",
      " 0.28148067 0.30734688 0.24866247 0.258169   0.24900106 0.27254063\n",
      " 0.27086166 0.37726894 0.28801063 0.30899516 0.33663234 0.26583126\n",
      " 0.3101392  0.30739358 0.3209926  0.30828857 0.2776192  0.26464927\n",
      " 0.3064644  0.31296813 0.26590955 0.27019182 0.26971596 0.31772503\n",
      " 0.2858792  0.25674677 0.2799641  0.25506184 0.25231612 0.3176974\n",
      " 0.3116879  0.2561367  0.2856866  0.3337335  0.2887234  0.27539304\n",
      " 0.25516096 0.31056198 0.28935492 0.24444978 0.3281531  0.25900245\n",
      " 0.26248282 0.25067326 0.24729088 0.27024838 0.2565418  0.2657846\n",
      " 0.23756263 0.27484545 0.26297915 0.25620258 0.30324984 0.31124762\n",
      " 0.30692837 0.25399214 0.25504202 0.27388558 0.2995088  0.29167023\n",
      " 0.29890057 0.31447423 0.29423654 0.32888806 0.27501014 0.25999436\n",
      " 0.36512932 0.26911777 0.26652175 0.25090256] Train accuracy: 0.9861111218730608\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: [1.6401708e-04 3.6685169e-04 7.3194504e-05 ... 5.8716536e-04 1.7915666e-04\n",
      " 5.8054924e-05] + 0.21853866 = [0.21870267 0.21890551 0.21861185 ... 0.21912582 0.21871781 0.21859671]\n",
      "\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: [0.35097447 0.28052446 0.25043267 0.2704093  0.25646576 0.29992247\n",
      " 0.27331507 0.25382504 0.28615844 0.2700465  0.29791144 0.23457207\n",
      " 0.23208019 0.24711187 0.26301822 0.24160081 0.2751711  0.2597574\n",
      " 0.24903303 0.2786461  0.3334366  0.31020352 0.2744767  0.32052052\n",
      " 0.29107893 0.2689047  0.30917633 0.28543195 0.26071617 0.2364071\n",
      " 0.2731778  0.2974274  0.24089824 0.24691214 0.24009326 0.26574504\n",
      " 0.2578567  0.36651647 0.2745139  0.29718533 0.33248472 0.25629005\n",
      " 0.2979328  0.29692733 0.31219897 0.29823378 0.26907542 0.25328302\n",
      " 0.29509977 0.29853308 0.26121512 0.2617733  0.262752   0.3009396\n",
      " 0.27925298 0.24833876 0.26772803 0.24659102 0.24124895 0.3134329\n",
      " 0.3044566  0.24696738 0.26931152 0.33523268 0.27808747 0.2679749\n",
      " 0.24798761 0.299415   0.2819371  0.23605935 0.31664604 0.24816388\n",
      " 0.25808883 0.23689736 0.23887253 0.25983152 0.25436386 0.26799476\n",
      " 0.22907788 0.263926   0.25604832 0.24656337 0.2845432  0.3001031\n",
      " 0.30553773 0.24438292 0.24627478 0.25269255 0.2926985  0.2854129\n",
      " 0.2946224  0.30765718 0.28986412 0.32180312 0.2699023  0.2489748\n",
      " 0.35342902 0.25026703 0.270344   0.24117237] Train accuracy: 0.9861111210452186\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: [1.0347366e-04 5.5858493e-04 6.6995621e-05 ... 6.4054132e-04 2.2920966e-04\n",
      " 5.7101250e-05] + 0.21084374 = [0.21094722 0.21140233 0.21091074 ... 0.21148428 0.21107295 0.21090084]\n",
      "\n",
      "\n",
      "Epoch Number: 25\n",
      "Train Loss: [0.33321655 0.2681081  0.24126619 0.26272127 0.24877657 0.29150885\n",
      " 0.261153   0.2469306  0.27856323 0.2636565  0.2963501  0.22495225\n",
      " 0.22305343 0.2377708  0.2551047  0.22953719 0.26647335 0.2485674\n",
      " 0.24133155 0.2735606  0.32268083 0.29672652 0.26587677 0.31151944\n",
      " 0.28603393 0.25831002 0.29013088 0.2689275  0.24176928 0.22740977\n",
      " 0.26227033 0.28911662 0.23335311 0.24027549 0.22736552 0.25880548\n",
      " 0.25394836 0.35831106 0.2666166  0.29429686 0.31686318 0.24872443\n",
      " 0.29450992 0.29082713 0.3007063  0.29212868 0.27622268 0.24598694\n",
      " 0.28191426 0.28988236 0.24650925 0.2507944  0.25680995 0.29287732\n",
      " 0.27289698 0.23948362 0.25563526 0.23694131 0.23849504 0.30351555\n",
      " 0.29787004 0.23929422 0.26780188 0.31984404 0.275112   0.25389352\n",
      " 0.24483578 0.28971714 0.27457315 0.22652735 0.3068776  0.24365804\n",
      " 0.24063934 0.22910401 0.23186901 0.2524687  0.23561382 0.26071787\n",
      " 0.2257031  0.2570761  0.24723881 0.23998107 0.27951038 0.29326618\n",
      " 0.29250687 0.2364156  0.23926088 0.246754   0.27579528 0.27637255\n",
      " 0.28855804 0.2950001  0.27949685 0.30622232 0.2595511  0.24228899\n",
      " 0.34469995 0.26235548 0.2587866  0.23531328] Train accuracy: 0.9861111210452186\n",
      "Test accuracy 0.92576\n",
      "MarginLoss + RegLoss: [1.4673173e-04 2.7723610e-04 7.6532364e-05 ... 5.8645010e-04 8.8056922e-04\n",
      " 5.3167343e-05] + 0.20354472 = [0.20369145 0.20382196 0.20362125 ... 0.20413117 0.20442529 0.20359789]\n",
      "\n",
      "\n",
      "Epoch Number: 26\n",
      "Train Loss: [0.32619283 0.26346603 0.23256098 0.2558107  0.24193266 0.28418395\n",
      " 0.25444815 0.240504   0.2728097  0.2553362  0.2838737  0.21812929\n",
      " 0.21662417 0.23220335 0.24569194 0.22333938 0.25659424 0.24539617\n",
      " 0.2348179  0.2675212  0.31683964 0.28390393 0.2604451  0.30388907\n",
      " 0.27739766 0.25436097 0.28172716 0.27026948 0.23969714 0.22103603\n",
      " 0.2568063  0.28165337 0.22805926 0.23171285 0.22345635 0.25041643\n",
      " 0.2409415  0.34743223 0.2606343  0.29540634 0.3148104  0.2401559\n",
      " 0.29617321 0.28443182 0.29105055 0.2800682  0.24872316 0.23765376\n",
      " 0.27646866 0.2832377  0.24489291 0.24266611 0.25086835 0.29165995\n",
      " 0.25945583 0.2307431  0.24828236 0.23374516 0.22801298 0.29667464\n",
      " 0.29205602 0.23131531 0.2600745  0.31923944 0.26752937 0.24614729\n",
      " 0.22947042 0.27997562 0.26721913 0.22155173 0.29927066 0.24526398\n",
      " 0.23437674 0.22829539 0.22135696 0.24846265 0.23340853 0.2496392\n",
      " 0.21301726 0.24777916 0.2422044  0.23108642 0.27666104 0.28442955\n",
      " 0.28919414 0.22944096 0.23132192 0.23441291 0.2750069  0.26620927\n",
      " 0.27385405 0.2898512  0.2736026  0.30890238 0.24756108 0.23332088\n",
      " 0.34132987 0.23818138 0.24388391 0.22730783] Train accuracy: 0.9863888977302445\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [1.2753904e-04 6.4387918e-04 7.1644783e-05 ... 6.1433017e-04 2.3898482e-04\n",
      " 5.0425529e-05] + 0.1965375 = [0.19666503 0.19718137 0.19660914 ... 0.19715182 0.19677648 0.19658792]\n",
      "\n",
      "\n",
      "Epoch Number: 27\n",
      "Train Loss: [0.31498277 0.25502482 0.22650187 0.24879342 0.23362592 0.2734085\n",
      " 0.24855457 0.228807   0.26525474 0.24636753 0.2831658  0.20952053\n",
      " 0.21044357 0.22318612 0.23817764 0.21676557 0.2528479  0.23694271\n",
      " 0.2257523  0.25632972 0.30607027 0.28063452 0.2491157  0.29813123\n",
      " 0.26885703 0.24515025 0.26957363 0.2488734  0.23352824 0.21329497\n",
      " 0.24569768 0.27669248 0.21972826 0.22565028 0.21545906 0.24322833\n",
      " 0.23866701 0.33758274 0.2508952  0.26622775 0.30179995 0.23186305\n",
      " 0.27200747 0.27824777 0.27673328 0.27265888 0.23971528 0.23369044\n",
      " 0.27379176 0.2779001  0.23023288 0.23681134 0.23347415 0.27889666\n",
      " 0.24894683 0.22396225 0.24479908 0.2210173  0.21944845 0.28585282\n",
      " 0.28325716 0.22365485 0.25426248 0.29840457 0.25889537 0.24639753\n",
      " 0.22775963 0.27076116 0.25817433 0.2149478  0.29313362 0.22609255\n",
      " 0.23139852 0.21241575 0.21299362 0.24218294 0.23732631 0.2425784\n",
      " 0.20868498 0.24227984 0.2335264  0.22559121 0.27110848 0.28008923\n",
      " 0.28155652 0.22093207 0.2248606  0.23582357 0.26874214 0.260378\n",
      " 0.26969126 0.2807651  0.25952834 0.30850524 0.24461049 0.22633515\n",
      " 0.3298268  0.2283268  0.23495899 0.22013311] Train accuracy: 0.986527787314521\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: [2.0395219e-04 3.6281347e-04 6.9975853e-05 ... 4.7540665e-04 9.0241432e-05\n",
      " 4.8995018e-05] + 0.18984383 = [0.19004779 0.19020665 0.18991381 ... 0.19031924 0.18993407 0.18989283]\n",
      "\n",
      "\n",
      "Epoch Number: 28\n",
      "Train Loss: [0.31304657 0.2514852  0.21914881 0.24405593 0.22954018 0.27397582\n",
      " 0.23916286 0.22778831 0.2646437  0.24122228 0.27162623 0.2044391\n",
      " 0.20395249 0.22425978 0.23138267 0.2133538  0.24324018 0.23925257\n",
      " 0.21822879 0.24886759 0.30552942 0.27289128 0.24822688 0.29279926\n",
      " 0.26295507 0.24142006 0.26315182 0.24750195 0.22603625 0.20738989\n",
      " 0.24227124 0.2700949  0.2180155  0.21855451 0.21340564 0.23908257\n",
      " 0.22714002 0.33608627 0.24101144 0.26657906 0.29954198 0.22855502\n",
      " 0.26936275 0.27341473 0.27877146 0.26681378 0.23869579 0.2224572\n",
      " 0.26961717 0.2698903  0.23228288 0.23371156 0.23155631 0.2747466\n",
      " 0.2512652  0.21841417 0.23700769 0.22058907 0.21421562 0.2788492\n",
      " 0.27713692 0.21798992 0.24311556 0.30114916 0.2562324  0.23950526\n",
      " 0.21798946 0.26618463 0.25281823 0.21000792 0.28594846 0.22250909\n",
      " 0.21557504 0.20736307 0.21150455 0.23449713 0.22296636 0.23282258\n",
      " 0.19641317 0.23291212 0.2281882  0.21709755 0.26020843 0.2724785\n",
      " 0.2779969  0.21550567 0.21757938 0.23384206 0.26427492 0.2548013\n",
      " 0.26679596 0.27246705 0.26016134 0.29682165 0.24279486 0.21927446\n",
      " 0.33107045 0.22270653 0.23480254 0.21390998] Train accuracy: 0.9869444544116656\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.0716915e-04 2.1062791e-04 7.4267387e-05 ... 5.5870414e-04 2.2229552e-04\n",
      " 4.8995018e-05] + 0.18459605 = [0.18470322 0.18480667 0.18467031 ... 0.18515475 0.18481834 0.18464504]\n",
      "\n",
      "\n",
      "Epoch Number: 29\n",
      "Train Loss: [0.29724315 0.23897974 0.21445028 0.23662694 0.22048986 0.26722744\n",
      " 0.23706709 0.22358677 0.2719363  0.23335515 0.26354468 0.19723877\n",
      " 0.19772144 0.2134207  0.22912043 0.20588177 0.24129531 0.21948302\n",
      " 0.212499   0.2444455  0.29430833 0.27606246 0.23849042 0.28466213\n",
      " 0.25807285 0.2311981  0.25655708 0.23634678 0.22545558 0.20168944\n",
      " 0.23123476 0.2593714  0.20635273 0.20779279 0.20110124 0.23226468\n",
      " 0.22711518 0.3252435  0.23598729 0.2547716  0.2917021  0.2228558\n",
      " 0.26873115 0.2686703  0.26104844 0.2639466  0.23351471 0.22106315\n",
      " 0.24602264 0.26324323 0.22081815 0.22419569 0.22776307 0.2589862\n",
      " 0.24096158 0.22090743 0.22995861 0.2127165  0.21081893 0.27585694\n",
      " 0.2726815  0.21141313 0.24429338 0.28289855 0.24784571 0.23681132\n",
      " 0.20741898 0.2573458  0.24722256 0.20236911 0.27960154 0.21710758\n",
      " 0.21274191 0.20629662 0.20351495 0.22862189 0.20919234 0.23590139\n",
      " 0.19438726 0.22825244 0.22128955 0.21850342 0.25435922 0.27073058\n",
      " 0.2710868  0.2114151  0.21446273 0.21891083 0.24898277 0.24876857\n",
      " 0.26115784 0.26529333 0.24900025 0.29112566 0.22783977 0.21260744\n",
      " 0.32751194 0.2223537  0.23072171 0.20780574] Train accuracy: 0.9863888977302445\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: [2.4017692e-04 2.4911761e-04 2.1561980e-04 ... 4.2667985e-04 2.1907687e-04\n",
      " 4.5657158e-05] + 0.17917494 = [0.17941512 0.17942406 0.17939056 ... 0.17960162 0.17939402 0.1792206 ]\n",
      "\n",
      "\n",
      "Epoch Number: 30\n",
      "Train Loss: [0.29348198 0.23396903 0.20701952 0.23543817 0.21754281 0.25566152\n",
      " 0.23049828 0.21776442 0.25512934 0.23229496 0.2621585  0.19510442\n",
      " 0.19146568 0.20975298 0.2174024  0.20372075 0.23057824 0.22695756\n",
      " 0.2086419  0.23683946 0.29007494 0.25954297 0.23677717 0.28291968\n",
      " 0.25352272 0.23648293 0.2500788  0.23305355 0.2170131  0.19798309\n",
      " 0.24388616 0.25824115 0.2036969  0.20900956 0.19732185 0.22651055\n",
      " 0.22500566 0.3162936  0.23951592 0.25279534 0.28949443 0.21758209\n",
      " 0.25011015 0.25789395 0.26056784 0.2620289  0.2318764  0.21110395\n",
      " 0.24906577 0.2936812  0.22475982 0.22427338 0.22145092 0.26513538\n",
      " 0.23620242 0.20775284 0.22143658 0.20952193 0.20643763 0.272259\n",
      " 0.27105978 0.20756434 0.23258612 0.29282397 0.24984206 0.22759777\n",
      " 0.20902367 0.2530622  0.24370949 0.19827072 0.27208146 0.2081307\n",
      " 0.21565753 0.20643306 0.19734497 0.22680004 0.22186671 0.2502365\n",
      " 0.18950199 0.2206372  0.21780539 0.20831631 0.25078577 0.2659235\n",
      " 0.2663219  0.20473339 0.20791967 0.21368194 0.23995537 0.24405578\n",
      " 0.2639114  0.26717934 0.25131166 0.28785676 0.22951378 0.2095083\n",
      " 0.32019684 0.23329693 0.22848725 0.20549929] Train accuracy: 0.9863888977302445\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: [1.18136406e-04 1.82017684e-04 2.02998519e-04 ... 7.04154372e-04\n",
      " 1.35526061e-04 5.05447388e-05] + 0.17476438 = [0.17488252 0.1749464  0.17496738 ... 0.17546853 0.1748999  0.17481492]\n",
      "\n",
      "\n",
      "Epoch Number: 31\n",
      "Train Loss: [0.2849713  0.22751032 0.20136003 0.22827858 0.21390726 0.26013693\n",
      " 0.22201851 0.20976563 0.24898335 0.2244435  0.26448357 0.18732777\n",
      " 0.18857415 0.20677713 0.21465108 0.1928648  0.2264424  0.21381173\n",
      " 0.20537756 0.23621014 0.2789648  0.25470647 0.22744076 0.27180305\n",
      " 0.25484756 0.21635836 0.24632782 0.2231898  0.20432624 0.19228646\n",
      " 0.22316808 0.25119057 0.19916858 0.20431246 0.19238752 0.22373573\n",
      " 0.21262482 0.31055146 0.22771923 0.2634989  0.27910292 0.21347816\n",
      " 0.2650577  0.25597167 0.2494842  0.25185668 0.2211526  0.20967665\n",
      " 0.24482316 0.24945466 0.21048254 0.20801093 0.21247058 0.25880873\n",
      " 0.23177798 0.19811386 0.2167029  0.20152742 0.20341004 0.2637186\n",
      " 0.26280847 0.20279893 0.23708333 0.27311265 0.24023516 0.2164925\n",
      " 0.20121753 0.24819306 0.23529322 0.19400084 0.2668417  0.20360218\n",
      " 0.20063424 0.19241768 0.19279009 0.22457603 0.20643428 0.22022454\n",
      " 0.18161322 0.21395256 0.21491742 0.20199744 0.24962051 0.2591871\n",
      " 0.25923756 0.20049249 0.20205668 0.2212804  0.23729031 0.23821479\n",
      " 0.24516304 0.25033972 0.24083672 0.2816484  0.2171577  0.20468819\n",
      " 0.31928435 0.20855366 0.2144779  0.19936937] Train accuracy: 0.9873611198531257\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [1.8905103e-04 8.2492828e-05 1.8142164e-04 ... 5.3724647e-04 2.7248263e-04\n",
      " 4.4822693e-05] + 0.16962068 = [0.16980973 0.16970317 0.1698021  ... 0.17015792 0.16989316 0.1696655 ]\n",
      "\n",
      "\n",
      "Epoch Number: 32\n",
      "Train Loss: [0.27823794 0.22552371 0.1975365  0.22514743 0.20741422 0.24784184\n",
      " 0.22062789 0.2055406  0.24591786 0.22373417 0.2659906  0.18306254\n",
      " 0.18417715 0.19297995 0.20795593 0.19117539 0.21925826 0.20954692\n",
      " 0.2014995  0.23906381 0.27387232 0.25195855 0.2276365  0.26828027\n",
      " 0.2423218  0.21370493 0.24354564 0.22318724 0.20829923 0.18970644\n",
      " 0.21903741 0.24785675 0.19153993 0.19804157 0.1885085  0.21365279\n",
      " 0.20592827 0.294485   0.224739   0.24386303 0.27830982 0.20443086\n",
      " 0.24513894 0.25500214 0.24816693 0.25715202 0.22339837 0.20210543\n",
      " 0.23311846 0.24665435 0.2110589  0.2095589  0.20652395 0.2499922\n",
      " 0.22234654 0.19905354 0.21960191 0.19577187 0.19770657 0.25695455\n",
      " 0.26323232 0.19781043 0.2236495  0.28265372 0.2393214  0.2165063\n",
      " 0.20438986 0.2379927  0.23166476 0.1918808  0.26637065 0.21872117\n",
      " 0.19934691 0.18409549 0.18999322 0.21639942 0.20332788 0.2169286\n",
      " 0.1939142  0.2075738  0.2119739  0.20018606 0.2528715  0.2566752\n",
      " 0.26233488 0.1930908  0.19729757 0.20038155 0.24544722 0.23610742\n",
      " 0.24570635 0.254787   0.23887412 0.27536836 0.22209884 0.19788964\n",
      " 0.3023554  0.19759072 0.21523212 0.19712894] Train accuracy: 0.9870833406845728\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [1.7546117e-04 4.8804283e-04 1.5686452e-04 ... 6.0683489e-04 7.6848269e-04\n",
      " 4.2915344e-05] + 0.16571909 = [0.16589455 0.16620713 0.16587596 ... 0.16632593 0.16648757 0.165762  ]\n",
      "\n",
      "\n",
      "Epoch Number: 33\n",
      "Train Loss: [0.27164155 0.22038019 0.19211586 0.22327304 0.2102937  0.25003418\n",
      " 0.21451303 0.20270127 0.2391279  0.21613675 0.25246158 0.17965867\n",
      " 0.18152845 0.1965361  0.20296937 0.18366651 0.21684442 0.21093962\n",
      " 0.19360292 0.2289039  0.270865   0.24739082 0.22088787 0.26773885\n",
      " 0.24025986 0.21545696 0.233098   0.20669378 0.20431465 0.18342662\n",
      " 0.22144872 0.24716298 0.19260997 0.19285366 0.18374954 0.21678594\n",
      " 0.20687464 0.29514787 0.21783565 0.22669032 0.27077532 0.20271932\n",
      " 0.2436577  0.25155455 0.24652815 0.24198572 0.20789954 0.19769548\n",
      " 0.2409508  0.24338903 0.20093524 0.20241226 0.20379825 0.2436033\n",
      " 0.2164367  0.19586878 0.20944613 0.20030029 0.1940495  0.26246613\n",
      " 0.25209388 0.1929167  0.22134541 0.26818243 0.23505987 0.21674\n",
      " 0.2003658  0.2344976  0.22704567 0.18560177 0.25739467 0.1958089\n",
      " 0.1906955  0.1865746  0.18772872 0.21074504 0.20107697 0.23616059\n",
      " 0.17192574 0.2065391  0.20602761 0.190997   0.23672165 0.2547626\n",
      " 0.24643472 0.19027229 0.1962588  0.20995873 0.23481338 0.2302445\n",
      " 0.23710854 0.24261777 0.23577063 0.27256453 0.21756144 0.1943363\n",
      " 0.31689754 0.20762414 0.23827069 0.195317  ] Train accuracy: 0.9862500098016527\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.7450750e-04 1.1277199e-04 1.9083917e-04 ... 4.8160553e-04 1.9870698e-04\n",
      " 3.5166740e-05] + 0.1622493 = [0.1624238  0.16236207 0.16244014 ... 0.1627309  0.162448   0.16228446]\n",
      "\n",
      "\n",
      "Epoch Number: 34\n",
      "Train Loss: [0.2686041  0.21178335 0.18921778 0.21852206 0.2030079  0.24603984\n",
      " 0.21270782 0.20031269 0.26432025 0.21526106 0.2453683  0.1758185\n",
      " 0.17737454 0.19083835 0.20254108 0.18323745 0.21204291 0.19933935\n",
      " 0.19617847 0.22154409 0.27578017 0.25818375 0.21924873 0.2594546\n",
      " 0.23517118 0.21684694 0.23818694 0.21138461 0.19864793 0.18122944\n",
      " 0.21074277 0.23873803 0.18692261 0.19096318 0.18190503 0.20879713\n",
      " 0.20211452 0.30112046 0.21101533 0.24895734 0.27069086 0.1995951\n",
      " 0.24837989 0.2473522  0.24778475 0.23652452 0.20467329 0.19794759\n",
      " 0.22025913 0.23948248 0.20587339 0.19951415 0.19991124 0.23726681\n",
      " 0.22895326 0.19863053 0.2034108  0.19155774 0.19198017 0.25310993\n",
      " 0.25562662 0.18905188 0.21647598 0.272475   0.2229789  0.20449752\n",
      " 0.19217753 0.22904271 0.22767285 0.1864413  0.2568965  0.20133488\n",
      " 0.19402964 0.18554418 0.18050799 0.20746624 0.20224908 0.21614788\n",
      " 0.17250739 0.19816282 0.20168485 0.19497667 0.23796555 0.25086698\n",
      " 0.25370473 0.18491933 0.18876503 0.2253201  0.22210293 0.22356397\n",
      " 0.23987502 0.23243597 0.23067069 0.27417627 0.20891622 0.19112843\n",
      " 0.30416608 0.20375556 0.20050094 0.18912321] Train accuracy: 0.9870833423402574\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: [2.0311773e-04 2.2360682e-04 1.5829504e-04 ... 6.0969591e-04 9.3936920e-05\n",
      " 3.0636787e-05] + 0.15939175 = [0.15959486 0.15961535 0.15955004 ... 0.16000144 0.15948568 0.15942238]\n",
      "\n",
      "\n",
      "Epoch Number: 35\n",
      "Train Loss: [0.26355788 0.20278296 0.18655981 0.2183965  0.1989705  0.23740429\n",
      " 0.20883965 0.19471428 0.23200978 0.21068946 0.23750117 0.17285848\n",
      " 0.1744265  0.18982585 0.19394329 0.18010654 0.21038367 0.1967863\n",
      " 0.19113515 0.22246541 0.26023012 0.23174885 0.21169919 0.25778264\n",
      " 0.2360194  0.21231705 0.23972842 0.2027153  0.19434096 0.1783135\n",
      " 0.22676018 0.23391491 0.18361445 0.18755122 0.17965966 0.20353948\n",
      " 0.2064503  0.28437346 0.21305723 0.23174757 0.26365024 0.19519693\n",
      " 0.23391373 0.24050617 0.24100034 0.23127548 0.19963934 0.20131546\n",
      " 0.22944982 0.2575069  0.20225042 0.20075995 0.19287196 0.23701368\n",
      " 0.21354386 0.19409946 0.20003672 0.1835431  0.18530408 0.24478634\n",
      " 0.24947378 0.1883919  0.21093917 0.26169154 0.22297361 0.24266392\n",
      " 0.19173636 0.22873071 0.2191668  0.18575317 0.2566392  0.1875834\n",
      " 0.19114211 0.17832156 0.17408691 0.20244206 0.20581657 0.21202014\n",
      " 0.17712192 0.19900018 0.20295042 0.18872672 0.23591264 0.2531973\n",
      " 0.24533537 0.18362154 0.18816014 0.1921496  0.21897204 0.23018128\n",
      " 0.23512343 0.24140951 0.22584772 0.27209914 0.20733999 0.18722342\n",
      " 0.3033003  0.1907763  0.19860363 0.18843077] Train accuracy: 0.9886111203167174\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.7856061e-04 5.7692826e-04 1.1563301e-04 ... 7.5300038e-04 3.1709671e-05\n",
      " 3.2544136e-05] + 0.15614119 = [0.15631975 0.15671812 0.15625682 ... 0.15689419 0.1561729  0.15617374]\n",
      "\n",
      "\n",
      "Epoch Number: 36\n",
      "Train Loss: [0.25702325 0.20356852 0.18116161 0.21399763 0.19831555 0.238213\n",
      " 0.20629129 0.19558552 0.22489224 0.20679365 0.23941854 0.17093244\n",
      " 0.17058367 0.18992877 0.19212672 0.17488284 0.19893703 0.1949157\n",
      " 0.18656617 0.2129371  0.27027205 0.23294656 0.2112286  0.25643873\n",
      " 0.22573543 0.20709981 0.22339231 0.20408101 0.19210544 0.17499332\n",
      " 0.21332681 0.2346143  0.18226308 0.18458763 0.17188318 0.20146453\n",
      " 0.18809552 0.27704853 0.20974709 0.20793833 0.26688588 0.19188064\n",
      " 0.2249195  0.24025862 0.23555848 0.23116523 0.19669598 0.18637449\n",
      " 0.22893725 0.23073417 0.19599734 0.19182757 0.19070187 0.23393948\n",
      " 0.20985198 0.18686002 0.19886334 0.18839602 0.1882655  0.24727982\n",
      " 0.24807991 0.18367103 0.21035458 0.2625708  0.22216365 0.20121866\n",
      " 0.18581372 0.22127125 0.2165609  0.1757117  0.24710189 0.181451\n",
      " 0.19453114 0.17131715 0.17439231 0.2008508  0.19044684 0.2326699\n",
      " 0.16080818 0.19194306 0.19152552 0.18398802 0.2321542  0.25162318\n",
      " 0.23344183 0.18224609 0.18711916 0.20314366 0.21841596 0.22095126\n",
      " 0.22971116 0.23421669 0.22262454 0.27271244 0.19890513 0.18439128\n",
      " 0.29369918 0.19944347 0.20800906 0.1846631 ] Train accuracy: 0.9873611190252833\n",
      "Test accuracy 0.935227\n",
      "MarginLoss + RegLoss: [1.9381940e-04 3.6293268e-04 1.5459955e-04 ... 5.1806867e-04 3.8027763e-05\n",
      " 2.6702881e-05] + 0.15317695 = [0.15337077 0.15353988 0.15333155 ... 0.15369502 0.15321498 0.15320365]\n",
      "\n",
      "\n",
      "Epoch Number: 37\n",
      "Train Loss: [0.2526062  0.19462638 0.17982927 0.21187383 0.19472003 0.23726243\n",
      " 0.20544478 0.19263123 0.23109743 0.20604867 0.24976839 0.16699436\n",
      " 0.16672371 0.18492547 0.19003266 0.17381865 0.20457344 0.18936093\n",
      " 0.18828104 0.2176006  0.25774634 0.23551732 0.20673154 0.25224438\n",
      " 0.22702953 0.19930348 0.22761117 0.19331007 0.18826793 0.17033955\n",
      " 0.20455208 0.22519681 0.17535463 0.18479857 0.17482679 0.20019713\n",
      " 0.22935078 0.33282202 0.20013407 0.22086984 0.26182276 0.19818796\n",
      " 0.22468683 0.23970243 0.23793721 0.22351125 0.22125281 0.18878546\n",
      " 0.21071091 0.22341555 0.19543622 0.1884789  0.18780357 0.23556954\n",
      " 0.22038469 0.18809348 0.190133   0.18376619 0.18038672 0.23649782\n",
      " 0.24410282 0.18099591 0.20890759 0.25718647 0.22926787 0.19651705\n",
      " 0.18258218 0.22025308 0.21434194 0.17722246 0.2481716  0.18451127\n",
      " 0.17736648 0.16711502 0.17234421 0.20242992 0.1977176  0.21004681\n",
      " 0.1692255  0.18982725 0.19273224 0.17808919 0.23179708 0.24578735\n",
      " 0.240529   0.17755048 0.18181714 0.19485411 0.22188503 0.2212566\n",
      " 0.22548366 0.22758195 0.21890405 0.24328637 0.19970152 0.1838313\n",
      " 0.28794622 0.19092423 0.19185942 0.18435532] Train accuracy: 0.9872222302688493\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: [2.4029613e-04 3.2800436e-04 1.5030801e-04 ... 4.6636164e-04 2.1848083e-04\n",
      " 2.4914742e-05] + 0.15114915 = [0.15138945 0.15147716 0.15129946 ... 0.15161552 0.15136763 0.15117407]\n",
      "\n",
      "\n",
      "Epoch Number: 38\n",
      "Train Loss: [0.25085542 0.20806763 0.17822367 0.22189415 0.19138724 0.2433533\n",
      " 0.203257   0.20825419 0.21803662 0.20226258 0.25362754 0.16731507\n",
      " 0.16992828 0.18142232 0.18961763 0.172084   0.20398127 0.19218047\n",
      " 0.1858632  0.21847177 0.26221862 0.23930413 0.21335942 0.2550284\n",
      " 0.23966688 0.21735765 0.21988983 0.19905604 0.1982202  0.1685664\n",
      " 0.21268375 0.22793405 0.17919189 0.1898782  0.1708991  0.20771097\n",
      " 0.19650742 0.28073758 0.26260158 0.22067702 0.26232854 0.19035737\n",
      " 0.24569708 0.24103805 0.24593367 0.22207445 0.2042265  0.18877992\n",
      " 0.21428095 0.22676897 0.1971036  0.19410698 0.18680798 0.24987589\n",
      " 0.2154777  0.19906965 0.19138516 0.18692493 0.20221592 0.24670158\n",
      " 0.25298488 0.18164617 0.20636976 0.26342264 0.21552692 0.20391665\n",
      " 0.18490554 0.21970399 0.22264668 0.17288646 0.24592906 0.19200756\n",
      " 0.19140604 0.17235878 0.17979945 0.20404667 0.182415   0.23180348\n",
      " 0.16760217 0.18431844 0.19980954 0.18006809 0.24207765 0.2490654\n",
      " 0.22977635 0.17313188 0.17750908 0.19743724 0.20989683 0.2289603\n",
      " 0.2212776  0.24263448 0.2258311  0.27282488 0.20083688 0.18062797\n",
      " 0.3025869  0.18684387 0.19758113 0.18520826] Train accuracy: 0.9850000101659033\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [2.1728873e-04 7.6055527e-05 9.9420547e-05 ... 2.7093291e-04 5.0210953e-04\n",
      " 1.8477440e-05] + 0.1514515 = [0.15166879 0.15152755 0.15155092 ... 0.15172243 0.15195361 0.15146998]\n",
      "\n",
      "\n",
      "Epoch Number: 39\n",
      "Train Loss: [0.2520347  0.19449778 0.17688596 0.21989994 0.19129834 0.23861271\n",
      " 0.20744407 0.18904239 0.2312053  0.20733006 0.24508378 0.1662705\n",
      " 0.1700831  0.17866744 0.18671827 0.17486632 0.19481428 0.18507555\n",
      " 0.18110305 0.21664788 0.25995398 0.23936947 0.20758139 0.24259554\n",
      " 0.22720414 0.2063824  0.22024947 0.18361636 0.19000724 0.17096464\n",
      " 0.21022622 0.23222248 0.17431425 0.17805746 0.16754395 0.19555889\n",
      " 0.18836053 0.27018574 0.21511678 0.22615197 0.26386374 0.18461369\n",
      " 0.22823633 0.23982804 0.22796586 0.22272372 0.1901875  0.18841773\n",
      " 0.20920113 0.22588934 0.20081824 0.1897831  0.18609013 0.21413821\n",
      " 0.20596749 0.18027864 0.19448115 0.18540232 0.18118353 0.24186952\n",
      " 0.23919164 0.17727101 0.19890009 0.26249558 0.2137559  0.19428474\n",
      " 0.18720914 0.22742255 0.21846022 0.17211287 0.24380748 0.20912266\n",
      " 0.17802468 0.16837654 0.16942756 0.19221158 0.17965662 0.22101706\n",
      " 0.15900354 0.18702346 0.20180885 0.1818845  0.22695194 0.24771792\n",
      " 0.2364716  0.17328243 0.17879759 0.19593224 0.23083417 0.21204092\n",
      " 0.22045979 0.22436015 0.22262174 0.25459185 0.20067751 0.18042229\n",
      " 0.3066851  0.19571069 0.1942095  0.17919755] Train accuracy: 0.9870833415124152\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.6067922e-04 7.7670813e-04 1.7355382e-04 ... 6.6544116e-04 5.6624413e-05\n",
      " 3.4689903e-05] + 0.14828059 = [0.14844127 0.1490573  0.14845414 ... 0.14894603 0.14833722 0.14831528]\n",
      "\n",
      "\n",
      "Epoch Number: 40\n",
      "Train Loss: [0.24072663 0.19327392 0.1741563  0.21390319 0.19970496 0.23269004\n",
      " 0.1979366  0.18154672 0.21761031 0.20078403 0.2376569  0.16268699\n",
      " 0.16691987 0.17960489 0.1857797  0.16537571 0.1959547  0.20518436\n",
      " 0.18471463 0.21632592 0.2529252  0.221268   0.1994428  0.2441091\n",
      " 0.21964216 0.19658422 0.22336286 0.19349177 0.18883546 0.16649348\n",
      " 0.2093926  0.22118038 0.17371182 0.17853633 0.16617054 0.19568291\n",
      " 0.19350933 0.2739188  0.1985428  0.20955211 0.25414294 0.18264934\n",
      " 0.21343228 0.23065135 0.22666568 0.21766934 0.18921268 0.17918581\n",
      " 0.21433008 0.22000651 0.1806426  0.18508352 0.18619525 0.23355359\n",
      " 0.20511359 0.1967996  0.18517783 0.17467196 0.17757712 0.23473239\n",
      " 0.2412451  0.17557219 0.20099358 0.2484371  0.21131858 0.19495244\n",
      " 0.19662426 0.20645279 0.20889637 0.17103256 0.23712431 0.17151755\n",
      " 0.17362945 0.16634461 0.17186642 0.18890499 0.19220297 0.2117462\n",
      " 0.16869229 0.1816908  0.18445335 0.17470038 0.22306664 0.23721562\n",
      " 0.22766882 0.16745013 0.17758699 0.17671522 0.21384205 0.22029097\n",
      " 0.22540842 0.22565511 0.21346454 0.26723623 0.20939729 0.17831555\n",
      " 0.28698316 0.187111   0.18604136 0.18193778] Train accuracy: 0.9873611190252833\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [2.2968650e-04 2.5042892e-04 1.1217594e-04 ... 1.9000471e-04 2.4768710e-04\n",
      " 3.1232834e-05] + 0.14681855 = [0.14704823 0.14706898 0.14693072 ... 0.14700855 0.14706624 0.14684978]\n",
      "\n",
      "\n",
      "Epoch Number: 41\n",
      "Train Loss: [0.2440305  0.19368164 0.17102501 0.21214849 0.18590158 0.2481339\n",
      " 0.19938639 0.18704113 0.224986   0.19955964 0.23141694 0.16165157\n",
      " 0.16618334 0.18404698 0.18355986 0.16666113 0.19092947 0.1841146\n",
      " 0.18389368 0.21076483 0.24868172 0.22356974 0.20492119 0.24454315\n",
      " 0.22481757 0.19885631 0.21506754 0.1906265  0.19500472 0.16568255\n",
      " 0.20616576 0.22123733 0.17731969 0.17981252 0.16703202 0.19173107\n",
      " 0.18329939 0.26858875 0.19611028 0.22264221 0.2592659  0.18346575\n",
      " 0.22250138 0.22972243 0.23444158 0.21347456 0.18731648 0.18415035\n",
      " 0.21145537 0.2203132  0.19018996 0.18181601 0.17857794 0.22474892\n",
      " 0.20603809 0.17991473 0.19494456 0.18095508 0.1766525  0.23107532\n",
      " 0.23500125 0.17633313 0.20542115 0.25794166 0.21239783 0.20404483\n",
      " 0.17741594 0.21202229 0.20590883 0.17073458 0.23401387 0.17555957\n",
      " 0.17264266 0.16754873 0.16725157 0.19011919 0.18531634 0.20821631\n",
      " 0.15345575 0.18155962 0.1869954  0.17806521 0.22654213 0.24225783\n",
      " 0.22375867 0.16624016 0.174632   0.20724915 0.20683433 0.21406907\n",
      " 0.21134472 0.21754473 0.21576013 0.2571834  0.18966788 0.17287913\n",
      " 0.31003234 0.1931233  0.18834214 0.17634219] Train accuracy: 0.9873611198531257\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [2.9511750e-04 4.5086443e-04 1.4756620e-04 ... 3.8389862e-04 4.2200089e-05\n",
      " 2.5749207e-05] + 0.14551778 = [0.1458129  0.14596865 0.14566535 ... 0.14590168 0.14555998 0.14554353]\n",
      "\n",
      "\n",
      "Epoch Number: 42\n",
      "Train Loss: [0.23747213 0.18750006 0.1707997  0.21694908 0.18448626 0.2282173\n",
      " 0.19756277 0.1789326  0.20960653 0.20188245 0.2322796  0.15957947\n",
      " 0.16361436 0.17211778 0.18160304 0.16456501 0.19101733 0.18755698\n",
      " 0.17828867 0.20799287 0.24363562 0.21776934 0.19652031 0.24366151\n",
      " 0.22537653 0.1971401  0.22100748 0.18142144 0.1886891  0.16403402\n",
      " 0.2118808  0.21905977 0.16874121 0.17328633 0.16334389 0.19077262\n",
      " 0.18563366 0.26978183 0.19929264 0.20929542 0.25234243 0.18078995\n",
      " 0.21568257 0.22972189 0.23093526 0.20671034 0.1888778  0.18524915\n",
      " 0.20372608 0.21677536 0.18275085 0.18566519 0.17363025 0.22278827\n",
      " 0.19418815 0.18422803 0.18380398 0.17155059 0.17084225 0.23240301\n",
      " 0.2414062  0.17357197 0.19675006 0.24354675 0.21977599 0.20563394\n",
      " 0.19085234 0.20653851 0.20525652 0.17149514 0.24323136 0.17764382\n",
      " 0.17476329 0.15862162 0.16634724 0.18912414 0.18448494 0.20134413\n",
      " 0.17084569 0.18406275 0.18983614 0.17104661 0.22380501 0.24341604\n",
      " 0.2253725  0.16513187 0.17392501 0.17630537 0.20614173 0.22006188\n",
      " 0.21499287 0.22874941 0.20734508 0.24674763 0.19942045 0.17068833\n",
      " 0.28496343 0.17666142 0.18563934 0.17692274] Train accuracy: 0.988472230732441\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [3.2871962e-04 5.3998828e-04 1.3469160e-04 ... 3.0978024e-04 8.4280968e-05\n",
      " 2.8252602e-05] + 0.14383836 = [0.14416708 0.14437835 0.14397305 ... 0.14414814 0.14392264 0.14386661]\n",
      "\n",
      "\n",
      "Epoch Number: 43\n",
      "Train Loss: [0.23477541 0.19182894 0.16829926 0.21334924 0.18579918 0.2401825\n",
      " 0.19919492 0.18616208 0.21544825 0.19716156 0.23067938 0.15804668\n",
      " 0.16238238 0.18142514 0.18185201 0.16399537 0.18621683 0.18005587\n",
      " 0.17688833 0.2116576  0.2516548  0.22121064 0.1987302  0.24618377\n",
      " 0.22206438 0.1987007  0.21310382 0.18581969 0.18587545 0.16253012\n",
      " 0.20586689 0.21934572 0.17285965 0.1766852  0.16190422 0.1916737\n",
      " 0.17961441 0.26674    0.19843291 0.20423482 0.2549783  0.1836736\n",
      " 0.21332477 0.23240095 0.22389844 0.21861306 0.18678702 0.1747629\n",
      " 0.20871045 0.21276516 0.1949892  0.18235461 0.18894696 0.21652752\n",
      " 0.19229442 0.17960118 0.19679388 0.18106641 0.17307056 0.23292463\n",
      " 0.2314554  0.17242013 0.19858798 0.25779942 0.20953886 0.19120784\n",
      " 0.16871087 0.20368691 0.20239222 0.16273952 0.23270181 0.16937947\n",
      " 0.17106375 0.15792122 0.1713575  0.18522239 0.1772586  0.21226095\n",
      " 0.15067911 0.1787393  0.18237393 0.17237224 0.22666492 0.24206646\n",
      " 0.21065138 0.16771224 0.1746664  0.18255651 0.2165214  0.20835766\n",
      " 0.21425337 0.22510709 0.21205856 0.25424325 0.1859173  0.16829142\n",
      " 0.28837755 0.19148329 0.20605017 0.17420256] Train accuracy: 0.9872222310966916\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: [3.5089254e-04 3.6756694e-04 1.1920929e-04 ... 3.9975345e-04 7.7605247e-05\n",
      " 2.5153160e-05] + 0.14314939 = [0.14350028 0.14351696 0.1432686  ... 0.14354914 0.143227   0.14317454]\n",
      "\n",
      "\n",
      "Epoch Number: 44\n",
      "Train Loss: [0.2297807  0.18001065 0.16869049 0.21537109 0.18352419 0.22815329\n",
      " 0.20140857 0.18469174 0.22938998 0.19617648 0.22146448 0.15852146\n",
      " 0.15981652 0.1712846  0.18213409 0.16406396 0.18839745 0.18137172\n",
      " 0.17761321 0.20590483 0.24779752 0.2467337  0.19195686 0.2446616\n",
      " 0.21986789 0.19801582 0.22886997 0.1782442  0.18020138 0.16170369\n",
      " 0.19926871 0.21793361 0.16651906 0.1723245  0.16402051 0.19071375\n",
      " 0.19412702 0.27633405 0.19798453 0.21003015 0.25236502 0.1791775\n",
      " 0.2137116  0.23026572 0.22615163 0.21369492 0.19163197 0.18261495\n",
      " 0.2033925  0.2164157  0.18522187 0.17921825 0.18282032 0.22262461\n",
      " 0.21314554 0.18101163 0.17890836 0.17101733 0.16996159 0.22717923\n",
      " 0.23633021 0.17047675 0.1912311  0.24602954 0.21349509 0.19072114\n",
      " 0.17891096 0.20946509 0.2024561  0.16880149 0.23545212 0.18026322\n",
      " 0.17338161 0.15847275 0.16566633 0.18761382 0.18921685 0.21060789\n",
      " 0.1520852  0.17962089 0.18239261 0.17642066 0.22140399 0.24577421\n",
      " 0.22507292 0.16233832 0.17275316 0.18916348 0.21406782 0.20558162\n",
      " 0.24123062 0.21851842 0.208432   0.24860872 0.19101235 0.17102455\n",
      " 0.2751991  0.2020679  0.18373522 0.17526317] Train accuracy: 0.9877777869502703\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [2.7890503e-04 4.7886372e-04 2.3505092e-04 ... 5.0282478e-04 1.2813509e-04\n",
      " 2.6226044e-05] + 0.14243612 = [0.14271502 0.14291498 0.14267117 ... 0.14293894 0.14256425 0.14246234]\n",
      "\n",
      "\n",
      "Epoch Number: 45\n",
      "Train Loss: [0.23283124 0.1879676  0.16896547 0.21466476 0.17738044 0.23587169\n",
      " 0.19730891 0.18449196 0.21540684 0.196416   0.23402599 0.15866102\n",
      " 0.16743521 0.17918961 0.1784028  0.16368024 0.19186325 0.18503216\n",
      " 0.18027443 0.21433789 0.2532782  0.20941304 0.19563441 0.23911592\n",
      " 0.22357996 0.2009376  0.2459585  0.17913695 0.18573488 0.16238895\n",
      " 0.20994045 0.21807161 0.16779196 0.17239586 0.16387069 0.18859272\n",
      " 0.18740326 0.2676698  0.20347142 0.22268009 0.25153834 0.17643285\n",
      " 0.22154029 0.23412432 0.23211966 0.20576724 0.18727401 0.1874282\n",
      " 0.21033496 0.22645672 0.18478027 0.18031964 0.18027937 0.21649832\n",
      " 0.19976975 0.1901309  0.18264197 0.17417021 0.1699821  0.23127873\n",
      " 0.23632261 0.17322609 0.18509173 0.25487673 0.22063032 0.23869035\n",
      " 0.18984175 0.210005   0.21303374 0.1696953  0.24599501 0.18859899\n",
      " 0.17642978 0.15768197 0.16816951 0.1823067  0.19131385 0.20871681\n",
      " 0.18049154 0.18011165 0.18130197 0.1732894  0.22548325 0.24020675\n",
      " 0.22318262 0.16638751 0.16877192 0.18944854 0.20520097 0.22401956\n",
      " 0.2137055  0.24117023 0.21825676 0.24637717 0.19560234 0.16874918\n",
      " 0.28486735 0.17776865 0.193927   0.1766098 ] Train accuracy: 0.9870833431680998\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: [2.8212368e-04 8.0922246e-04 1.2515485e-04 ... 3.8509071e-04 6.1087310e-04\n",
      " 2.5629997e-05] + 0.14312156 = [0.14340368 0.14393078 0.14324671 ... 0.14350665 0.14373243 0.14314719]\n",
      "\n",
      "\n",
      "Epoch Number: 46\n",
      "Train Loss: [0.2375785  0.18381368 0.16539246 0.21522027 0.18701898 0.23592514\n",
      " 0.199254   0.1864848  0.20778035 0.1930999  0.22609527 0.15866847\n",
      " 0.16230477 0.18511286 0.17554335 0.16476138 0.18181689 0.19410735\n",
      " 0.18005568 0.2046259  0.2583176  0.21093662 0.20339777 0.2451734\n",
      " 0.22586428 0.19751213 0.20559716 0.18365516 0.189603   0.16163649\n",
      " 0.20394687 0.2209632  0.16802956 0.17175826 0.16196166 0.18578514\n",
      " 0.17247134 0.25504085 0.20180865 0.2075389  0.25167942 0.180503\n",
      " 0.20752808 0.22765806 0.22427814 0.2028802  0.18729581 0.17384669\n",
      " 0.20221832 0.2134834  0.19303544 0.17542933 0.16973343 0.21692817\n",
      " 0.20258662 0.1749299  0.18131006 0.17588988 0.1695266  0.22810501\n",
      " 0.23231715 0.17040218 0.190256   0.25310293 0.21424147 0.18499225\n",
      " 0.17243294 0.19883221 0.20489581 0.16398047 0.23283856 0.16863781\n",
      " 0.17085549 0.15607558 0.15800485 0.1844315  0.1703928  0.19896762\n",
      " 0.1485162  0.177947   0.18590389 0.1679138  0.21957727 0.23589264\n",
      " 0.21912034 0.1672773  0.17095669 0.21143624 0.21910183 0.20552216\n",
      " 0.20678346 0.23419605 0.21205753 0.24478742 0.18890737 0.16980398\n",
      " 0.2874335  0.18493542 0.18189062 0.1752995 ] Train accuracy: 0.9883333411481645\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [1.9167364e-04 5.8396161e-04 2.4816394e-04 ... 9.7298622e-04 4.6730042e-05\n",
      " 3.0636787e-05] + 0.14164795 = [0.14183962 0.14223191 0.14189611 ... 0.14262094 0.14169468 0.14167859]\n",
      "\n",
      "\n",
      "Epoch Number: 47\n",
      "Train Loss: [0.23366769 0.18408066 0.16810408 0.22340451 0.18236485 0.23665439\n",
      " 0.19989139 0.18773042 0.20937037 0.19340342 0.22356023 0.1566797\n",
      " 0.16299452 0.17404443 0.18119618 0.1632172  0.18608722 0.1790306\n",
      " 0.17697328 0.20822391 0.24455625 0.2343644  0.19381484 0.23672162\n",
      " 0.20797144 0.20037988 0.20945857 0.1750794  0.19592108 0.16048676\n",
      " 0.20228502 0.23638958 0.16610081 0.1805074  0.16601013 0.18369597\n",
      " 0.18807943 0.28196964 0.20091754 0.22078377 0.25892398 0.17932759\n",
      " 0.21037288 0.230499   0.22275758 0.20653236 0.19456412 0.1729058\n",
      " 0.20078248 0.20935236 0.18532488 0.17935382 0.17061383 0.22001499\n",
      " 0.20093739 0.18099815 0.1856024  0.17353424 0.18064275 0.23210104\n",
      " 0.22870082 0.17255844 0.19203556 0.24560942 0.23261966 0.19096345\n",
      " 0.17381568 0.20124514 0.20503595 0.16521408 0.23403269 0.18631135\n",
      " 0.18260133 0.15780047 0.17017074 0.18527417 0.21644264 0.2038889\n",
      " 0.15264872 0.17670786 0.177903   0.16779087 0.23074232 0.24509981\n",
      " 0.22084528 0.15971698 0.1686938  0.17612436 0.20755652 0.21193466\n",
      " 0.22986929 0.22101648 0.20675334 0.2487126  0.19783942 0.16730757\n",
      " 0.28774792 0.17879188 0.1830201  0.17554222] Train accuracy: 0.9862500089738104\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: [2.6270747e-04 1.4179498e-03 2.8808415e-04 ... 3.9164722e-04 4.2557716e-05\n",
      " 2.4676323e-05] + 0.14161222 = [0.14187492 0.14303017 0.1419003  ... 0.14200386 0.14165477 0.1416369 ]\n",
      "\n",
      "\n",
      "Epoch Number: 48\n",
      "Train Loss: [0.22769587 0.1982246  0.16617984 0.22989196 0.18126485 0.23815458\n",
      " 0.19772412 0.18178518 0.21566455 0.1916647  0.22251895 0.15778394\n",
      " 0.16431071 0.17754124 0.17932676 0.16170186 0.18766357 0.1884305\n",
      " 0.17486638 0.20981477 0.24405858 0.2252854  0.1978423  0.24437173\n",
      " 0.22553696 0.19308433 0.2167028  0.18346627 0.19612916 0.16077489\n",
      " 0.20422791 0.20982352 0.16972733 0.17089555 0.16013765 0.18555848\n",
      " 0.19383311 0.25934574 0.19733888 0.19724315 0.25246412 0.17783178\n",
      " 0.21381976 0.23046844 0.22386084 0.21228833 0.18920267 0.18094802\n",
      " 0.2049429  0.21552818 0.18826279 0.18080825 0.18215632 0.22300717\n",
      " 0.18252556 0.17536472 0.18534462 0.18874778 0.16499263 0.23493947\n",
      " 0.23396185 0.16990152 0.19956575 0.25729182 0.20860687 0.1925916\n",
      " 0.18069236 0.19937694 0.20141791 0.1635086  0.2312946  0.17358643\n",
      " 0.16564272 0.16001914 0.16184829 0.18355793 0.17170565 0.21134768\n",
      " 0.15189484 0.17916864 0.18489169 0.16320999 0.22307399 0.24258007\n",
      " 0.21311465 0.16780883 0.17625897 0.17108244 0.21561056 0.21343106\n",
      " 0.21270593 0.25220028 0.20672466 0.23732103 0.18751149 0.16713934\n",
      " 0.28039998 0.18618725 0.20536152 0.17614014] Train accuracy: 0.9869444535838233\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [3.2955408e-04 4.4788420e-04 1.4303625e-04 ... 2.1027029e-04 7.3194504e-05\n",
      " 2.5987625e-05] + 0.14105773 = [0.14138728 0.14150561 0.14120077 ... 0.141268   0.14113092 0.14108372]\n",
      "\n",
      "\n",
      "Epoch Number: 49\n",
      "Train Loss: [0.21648228 0.17660236 0.16429521 0.21287026 0.17885646 0.22552691\n",
      " 0.19973849 0.18179221 0.21378937 0.19148676 0.21739092 0.1542953\n",
      " 0.15730174 0.1672518  0.17637745 0.16467477 0.1788559  0.17276773\n",
      " 0.17864521 0.20277272 0.23390457 0.21996666 0.18638782 0.23491508\n",
      " 0.21285164 0.19103442 0.22241934 0.17485361 0.18234774 0.1598441\n",
      " 0.20251337 0.21342729 0.16145642 0.1680575  0.15753104 0.18160516\n",
      " 0.18073699 0.25791562 0.193901   0.20854972 0.25111878 0.1734525\n",
      " 0.2119924  0.22661442 0.21160384 0.20295018 0.1780713  0.18308571\n",
      " 0.19215329 0.20630461 0.18033588 0.1766721  0.17460865 0.21672198\n",
      " 0.21111378 0.17491482 0.18604732 0.17007397 0.16573653 0.22459191\n",
      " 0.23451182 0.1687375  0.18713614 0.24413085 0.21263263 0.1869748\n",
      " 0.17267027 0.20995443 0.19782025 0.16428192 0.23380783 0.17725886\n",
      " 0.17203791 0.15908927 0.16081557 0.18341678 0.17605065 0.1951975\n",
      " 0.14836967 0.17521316 0.17821786 0.1786775  0.21860518 0.24490847\n",
      " 0.21881346 0.16013415 0.17022109 0.17648289 0.2137106  0.20652752\n",
      " 0.2192969  0.20585887 0.20481038 0.23870526 0.18305668 0.16819112\n",
      " 0.27729297 0.17996    0.17882457 0.17280416] Train accuracy: 0.9890277857581774\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: [3.5530329e-04 2.6868284e-03 1.8261373e-04 ... 4.9352646e-04 0.0000000e+00\n",
      " 2.6583672e-05] + 0.13992752 = [0.14028282 0.14261435 0.14011014 ... 0.14042105 0.13992752 0.1399541 ]\n",
      "\n",
      "\n",
      "Epoch Number: 50\n",
      "Train Loss: [0.21893029 0.1788739  0.16307169 0.21975727 0.17485029 0.2299783\n",
      " 0.19282985 0.18217152 0.20686568 0.1920312  0.22797452 0.15422946\n",
      " 0.16215949 0.1703176  0.1734961  0.16053231 0.18158424 0.19177327\n",
      " 0.17893094 0.21103273 0.28111115 0.208837   0.18962215 0.24008796\n",
      " 0.2120747  0.1866679  0.21531588 0.17339809 0.18276946 0.15920502\n",
      " 0.2023497  0.21276933 0.16157274 0.16890457 0.15945357 0.18251695\n",
      " 0.17488645 0.25515875 0.1963704  0.19452085 0.24846369 0.17521079\n",
      " 0.20943756 0.2251491  0.22349551 0.19636461 0.18495074 0.17717212\n",
      " 0.21839526 0.20875624 0.17855662 0.17452022 0.17483926 0.22314411\n",
      " 0.19550955 0.18366139 0.17641546 0.16903538 0.16655663 0.22375022\n",
      " 0.23056586 0.16687047 0.18655519 0.24490325 0.20647474 0.19876981\n",
      " 0.17364863 0.20110083 0.19991973 0.15900996 0.24213229 0.17189935\n",
      " 0.16564444 0.15291534 0.15790172 0.18302882 0.16692531 0.18717301\n",
      " 0.14916468 0.17424136 0.17481622 0.16496421 0.22320311 0.23347574\n",
      " 0.21084043 0.16845739 0.16797704 0.19944112 0.20630047 0.21530306\n",
      " 0.21064995 0.23608859 0.20802876 0.24195522 0.19152603 0.16699812\n",
      " 0.26668796 0.17503631 0.182774   0.17831759] Train accuracy: 0.9879166757067045\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: [2.8224289e-04 2.0284355e-03 2.4840236e-04 ... 1.0157377e-03 2.3695827e-04\n",
      " 2.8371811e-05] + 0.13960414 = [0.13988638 0.14163257 0.13985254 ... 0.14061987 0.1398411  0.13963251]\n",
      "\n",
      "\n",
      "Epoch Number: 51\n",
      "Train Loss: [0.22445342 0.18508613 0.1674201  0.21756597 0.18862605 0.24829046\n",
      " 0.20362158 0.1922924  0.21339868 0.19352184 0.24174088 0.15737349\n",
      " 0.16453938 0.1731816  0.17955971 0.16251607 0.18327922 0.17943887\n",
      " 0.17844914 0.2087812  0.24753523 0.22895561 0.19699024 0.24810489\n",
      " 0.20167063 0.20801978 0.21283685 0.19067234 0.19206671 0.15923007\n",
      " 0.212277   0.2359643  0.16628812 0.20918746 0.16843008 0.1878702\n",
      " 0.17954305 0.28555265 0.24275309 0.22687006 0.25785053 0.17625543\n",
      " 0.21262597 0.22879331 0.23747261 0.20314881 0.21359581 0.16584682\n",
      " 0.23205675 0.20415615 0.20895152 0.17682673 0.17663261 0.22015122\n",
      " 0.19840147 0.2102024  0.18799376 0.17777678 0.18241423 0.23215893\n",
      " 0.22826351 0.17120035 0.19794637 0.26349652 0.22522262 0.20810458\n",
      " 0.19095592 0.20305388 0.2143072  0.16116159 0.24702024 0.19443488\n",
      " 0.18373    0.16951464 0.18453294 0.1844068  0.22468789 0.21765362\n",
      " 0.14935023 0.17587256 0.17512888 0.17177191 0.23023552 0.24504489\n",
      " 0.20798057 0.16365701 0.17046137 0.17639928 0.26243117 0.20940627\n",
      " 0.21778172 0.21797702 0.20905726 0.24751595 0.19269304 0.1686306\n",
      " 0.31001195 0.1890107  0.19041763 0.1793037 ] Train accuracy: 0.9843055647280481\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [2.2360682e-04 2.4540126e-03 2.9034913e-04 ... 7.9588592e-04 2.2292137e-05\n",
      " 2.7894974e-05] + 0.1419664 = [0.14219001 0.14442042 0.14225675 ... 0.14276229 0.1419887  0.1419943 ]\n",
      "\n",
      "\n",
      "Epoch Number: 52\n",
      "Train Loss: [0.21192609 0.1824078  0.16363388 0.22388002 0.17783657 0.23637503\n",
      " 0.196779   0.18865722 0.21819332 0.18802208 0.22009684 0.15791014\n",
      " 0.16273142 0.16946371 0.17883076 0.16256845 0.1820915  0.18484876\n",
      " 0.17299908 0.20974454 0.23484564 0.21799597 0.1938771  0.2380876\n",
      " 0.22069754 0.19779804 0.24895371 0.17454797 0.18544266 0.1605324\n",
      " 0.19581285 0.21577568 0.16685785 0.1665703  0.15835728 0.18503678\n",
      " 0.18184248 0.2596319  0.24914867 0.21879888 0.25171533 0.17700492\n",
      " 0.21798728 0.2266611  0.2149779  0.19875291 0.18131362 0.18690513\n",
      " 0.20671123 0.2168178  0.18128584 0.17254716 0.18605632 0.22302328\n",
      " 0.1941483  0.1830871  0.18409376 0.17309612 0.16531117 0.22382665\n",
      " 0.23555051 0.1681767  0.20651475 0.24426535 0.21187833 0.19526413\n",
      " 0.17340755 0.20667738 0.19846295 0.1634624  0.23327215 0.18910012\n",
      " 0.166999   0.16048683 0.15835878 0.18069091 0.1785706  0.2004418\n",
      " 0.14791673 0.17694005 0.18375185 0.17270441 0.22319847 0.24753343\n",
      " 0.21307455 0.16118781 0.17110468 0.17244683 0.19878857 0.20710436\n",
      " 0.22346112 0.23767962 0.21857738 0.23448698 0.19313523 0.16729915\n",
      " 0.28815013 0.18635918 0.1852627  0.18412872] Train accuracy: 0.9868055639995469\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: [2.6521087e-04 1.3691336e-03 2.0037591e-04 ... 1.6002953e-03 5.2332878e-05\n",
      " 3.3855438e-05] + 0.14036512 = [0.14063033 0.14173426 0.1405655  ... 0.14196542 0.14041746 0.14039898]\n",
      "\n",
      "\n",
      "Epoch Number: 53\n",
      "Train Loss: [0.21553892 0.17781627 0.16224644 0.21451336 0.17743176 0.22424714\n",
      " 0.19474107 0.18086722 0.21148688 0.19162719 0.21624061 0.15310627\n",
      " 0.15647279 0.1752299  0.1738209  0.15944187 0.17714435 0.18619113\n",
      " 0.17900842 0.20450512 0.2362756  0.19993848 0.18914811 0.24060273\n",
      " 0.20726697 0.18560807 0.20786941 0.18435404 0.18446629 0.16048779\n",
      " 0.20470054 0.2090587  0.16088541 0.16772646 0.15656589 0.18342823\n",
      " 0.17645878 0.2538367  0.19722667 0.19922027 0.2529309  0.17320079\n",
      " 0.20852755 0.22933803 0.21980228 0.19545776 0.17790268 0.17604877\n",
      " 0.20639773 0.21362522 0.17884022 0.17082845 0.1732236  0.21839996\n",
      " 0.1966006  0.17082986 0.18675429 0.16573626 0.16576391 0.21943933\n",
      " 0.23017403 0.16726828 0.18930274 0.25127184 0.21598096 0.18962844\n",
      " 0.16880123 0.18793567 0.20391393 0.16212705 0.24309352 0.16482523\n",
      " 0.16680628 0.15428722 0.16007662 0.17998941 0.17250389 0.19190598\n",
      " 0.14884347 0.17394488 0.1827539  0.16615166 0.22015248 0.23486339\n",
      " 0.21459487 0.15778449 0.17013687 0.19334072 0.2097986  0.21135601\n",
      " 0.20742393 0.22619325 0.20007364 0.22273326 0.18720716 0.16681552\n",
      " 0.26657784 0.17537169 0.18127942 0.17405313] Train accuracy: 0.9886111178331904\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [3.7567317e-04 1.6227961e-03 2.0991266e-04 ... 2.5412440e-04 0.0000000e+00\n",
      " 2.7537346e-05] + 0.13902551 = [0.13940118 0.1406483  0.13923542 ... 0.13927963 0.13902551 0.13905305]\n",
      "\n",
      "\n",
      "Epoch Number: 54\n",
      "Train Loss: [0.21206449 0.1768514  0.1642415  0.22160868 0.17424609 0.22945602\n",
      " 0.19318958 0.18899013 0.20666835 0.19388828 0.21333422 0.15392557\n",
      " 0.15386039 0.17059934 0.17465526 0.1640515  0.17954934 0.17971112\n",
      " 0.17580436 0.1985077  0.24324864 0.2143759  0.19087319 0.23987259\n",
      " 0.21908344 0.18966436 0.21698134 0.16776903 0.19417652 0.16059843\n",
      " 0.21482742 0.21782117 0.15727977 0.18043925 0.15374632 0.18081276\n",
      " 0.19151422 0.27235302 0.18921478 0.19832174 0.25484133 0.17314622\n",
      " 0.2058963  0.22651976 0.22209918 0.20104578 0.19292077 0.18257323\n",
      " 0.20585485 0.21125141 0.18244919 0.17501102 0.17980716 0.21121413\n",
      " 0.18116456 0.18306756 0.19545303 0.17494959 0.16405815 0.22717091\n",
      " 0.22734568 0.16493362 0.19347517 0.24394971 0.22310843 0.19100891\n",
      " 0.17730467 0.19384289 0.2008783  0.15948348 0.23470396 0.16874188\n",
      " 0.16699412 0.15341412 0.16232744 0.18320253 0.17149235 0.19817068\n",
      " 0.15119506 0.17661566 0.17791939 0.16551998 0.21986519 0.23969054\n",
      " 0.20879057 0.1560361  0.17115909 0.1701234  0.20633978 0.20159064\n",
      " 0.20768273 0.207138   0.20564276 0.22001928 0.1916804  0.16754177\n",
      " 0.2714426  0.17624824 0.2023583  0.171677  ] Train accuracy: 0.9876388965381516\n",
      "Test accuracy 0.935227\n",
      "MarginLoss + RegLoss: [2.2181869e-04 9.6917152e-04 1.5650690e-04 ... 6.6818297e-04 2.0860136e-04\n",
      " 3.0398369e-05] + 0.13921976 = [0.13944158 0.14018893 0.13937627 ... 0.13988794 0.13942836 0.13925016]\n",
      "\n",
      "\n",
      "Epoch Number: 55\n",
      "Train Loss: [0.21583846 0.17797059 0.16288215 0.21868956 0.17597498 0.2319\n",
      " 0.19831416 0.18343142 0.21312608 0.19100174 0.20888862 0.1536234\n",
      " 0.1548157  0.17231761 0.17823341 0.15961914 0.18283993 0.1868096\n",
      " 0.1781268  0.20466095 0.23312932 0.20741802 0.1851248  0.24311842\n",
      " 0.20401019 0.21248038 0.21910936 0.17431991 0.18772973 0.15850845\n",
      " 0.20170048 0.21542595 0.1635957  0.17038386 0.15780935 0.18241225\n",
      " 0.18469001 0.25685057 0.20155497 0.20574278 0.25049102 0.17140585\n",
      " 0.21674292 0.22976221 0.2168166  0.19405553 0.18411377 0.17236334\n",
      " 0.201112   0.20557721 0.18404901 0.1727833  0.1809172  0.22649015\n",
      " 0.20852031 0.16676706 0.17665255 0.17428835 0.16214496 0.22932628\n",
      " 0.22947401 0.16915223 0.18290573 0.2525836  0.21734299 0.1935134\n",
      " 0.17179479 0.18740356 0.19976963 0.1622305  0.24108264 0.1820019\n",
      " 0.1734031  0.16146481 0.16708598 0.18034132 0.1852084  0.1998468\n",
      " 0.14706679 0.17146929 0.18077283 0.16828279 0.21479014 0.23816554\n",
      " 0.21129692 0.15869576 0.17023754 0.19545196 0.20724364 0.21012361\n",
      " 0.21184611 0.20385101 0.20475335 0.2235105  0.18846177 0.16497426\n",
      " 0.27726626 0.17640357 0.17827734 0.17858274] Train accuracy: 0.9883333419760069\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: [5.2605569e-04 1.8655509e-03 2.3922324e-04 ... 5.2033365e-04 0.0000000e+00\n",
      " 2.4676323e-05] + 0.13962628 = [0.14015234 0.14149183 0.1398655  ... 0.14014661 0.13962628 0.13965096]\n",
      "\n",
      "\n",
      "Epoch Number: 56\n",
      "Train Loss: [0.2159244  0.18452482 0.16386853 0.22631884 0.17046684 0.22588651\n",
      " 0.19552892 0.18193182 0.21493174 0.19119608 0.21573028 0.15511963\n",
      " 0.15901111 0.17148985 0.17252322 0.16375929 0.18508832 0.20809181\n",
      " 0.17255113 0.20909998 0.23779263 0.22074495 0.1939743  0.23766729\n",
      " 0.20802073 0.18065597 0.22136897 0.1824188  0.20044468 0.1574553\n",
      " 0.21325468 0.2202162  0.15959698 0.16821334 0.15667696 0.17548442\n",
      " 0.19434683 0.2642768  0.20772211 0.20415802 0.24804491 0.17275175\n",
      " 0.21459106 0.22302927 0.22026883 0.20191438 0.18529789 0.17693238\n",
      " 0.18428007 0.2049823  0.17996746 0.17331529 0.17018524 0.20929985\n",
      " 0.18619286 0.18030888 0.17469858 0.16255057 0.17184931 0.22477335\n",
      " 0.23319755 0.1692038  0.18856372 0.25374556 0.22834812 0.20252365\n",
      " 0.19627401 0.19413339 0.21861827 0.1662617  0.24121213 0.17255318\n",
      " 0.17776395 0.15229267 0.16346653 0.1769878  0.19997622 0.20089915\n",
      " 0.1719665  0.17372584 0.18228029 0.16907246 0.23049603 0.24296941\n",
      " 0.2253157  0.1574306  0.1685428  0.17566271 0.20122558 0.2121019\n",
      " 0.22804192 0.22243261 0.20888056 0.22060683 0.19524823 0.165761\n",
      " 0.2733617  0.1745583  0.18600254 0.17842016] Train accuracy: 0.9868055656552315\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [2.6211143e-04 7.2297454e-04 1.6067922e-04 ... 5.0902367e-05 0.0000000e+00\n",
      " 2.6702881e-05] + 0.14023797 = [0.14050008 0.14096095 0.14039865 ... 0.14028887 0.14023797 0.14026468]\n",
      "\n",
      "\n",
      "Epoch Number: 57\n",
      "Train Loss: [0.2423301  0.17607996 0.16399843 0.23119007 0.18246633 0.25036705\n",
      " 0.1950614  0.18596509 0.22041786 0.19692343 0.21412718 0.1584267\n",
      " 0.16258357 0.18700272 0.17352854 0.16577184 0.175811   0.19424072\n",
      " 0.17474502 0.21389703 0.24916217 0.20647544 0.1898135  0.24272643\n",
      " 0.21911395 0.20505652 0.23229009 0.17968553 0.18596509 0.16233954\n",
      " 0.23971575 0.20838468 0.16476387 0.16562544 0.16581127 0.18082349\n",
      " 0.17439622 0.26125872 0.18790449 0.19978605 0.26426753 0.1750806\n",
      " 0.20922178 0.23085076 0.22109993 0.19587691 0.17836894 0.17713599\n",
      " 0.24246743 0.21624023 0.20235823 0.17260064 0.1789886  0.22525126\n",
      " 0.210157   0.17571244 0.17549857 0.18010136 0.1769526  0.2212305\n",
      " 0.2268036  0.16928269 0.17995368 0.25323474 0.22636744 0.21460155\n",
      " 0.17287463 0.20207089 0.21557967 0.1625951  0.24227174 0.17209171\n",
      " 0.17272831 0.15862297 0.1660394  0.18631299 0.1846631  0.19977792\n",
      " 0.14703089 0.17733067 0.18507889 0.16735105 0.22403683 0.24110399\n",
      " 0.21833894 0.16532363 0.17631249 0.19236508 0.23762687 0.1955339\n",
      " 0.21965605 0.24191706 0.20308411 0.26375687 0.18596444 0.17632142\n",
      " 0.28633463 0.19528782 0.18416584 0.17469352] Train accuracy: 0.9865277881423632\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: [8.7738037e-05 1.2984127e-03 2.0156801e-04 ... 5.9968233e-04 0.0000000e+00\n",
      " 4.7206879e-05] + 0.14030191 = [0.14038965 0.14160033 0.14050348 ... 0.1409016  0.14030191 0.14034912]\n",
      "\n",
      "\n",
      "Epoch Number: 58\n",
      "Train Loss: [0.23449408 0.17647578 0.16268796 0.22666475 0.18478078 0.23076269\n",
      " 0.19966467 0.17930071 0.21671225 0.19742288 0.20863068 0.15683347\n",
      " 0.15113002 0.17614028 0.18436848 0.1655133  0.18642704 0.18038648\n",
      " 0.18311618 0.2098425  0.25556812 0.22791618 0.19288601 0.23929636\n",
      " 0.21324998 0.19373731 0.23670429 0.17779164 0.1920169  0.16457058\n",
      " 0.20225213 0.2226474  0.15730053 0.16966313 0.15765086 0.17980331\n",
      " 0.21365847 0.29999897 0.20740159 0.18614015 0.26615632 0.18120098\n",
      " 0.20791283 0.23082201 0.22513326 0.22064331 0.2205509  0.18774489\n",
      " 0.2288101  0.2327963  0.19063956 0.17861798 0.17523134 0.24086674\n",
      " 0.18398008 0.21010762 0.19318315 0.18998933 0.17599037 0.22280012\n",
      " 0.22789659 0.17337355 0.1960858  0.2402358  0.24384832 0.19422129\n",
      " 0.20918193 0.19634937 0.20056772 0.162036   0.23371598 0.17789719\n",
      " 0.24842519 0.1565597  0.16693676 0.17940968 0.18871468 0.20400415\n",
      " 0.14755437 0.17570591 0.17778295 0.22703797 0.22207855 0.25149107\n",
      " 0.20911013 0.16610032 0.17753984 0.17083222 0.20691293 0.20437928\n",
      " 0.25869805 0.21331555 0.20100605 0.22181426 0.18491054 0.16725431\n",
      " 0.2985758  0.19877303 0.18974428 0.17910177] Train accuracy: 0.9848611197537847\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [2.4041533e-04 2.2322088e-03 3.3813715e-04 ... 1.3528764e-04 1.6878545e-04\n",
      " 4.1842461e-05] + 0.1411942 = [0.14143461 0.1434264  0.14153233 ... 0.14132948 0.14136298 0.14123604]\n",
      "\n",
      "\n",
      "Epoch Number: 59\n",
      "Train Loss: [0.1976715  0.18409504 0.16226012 0.23671657 0.17543301 0.24728833\n",
      " 0.1864333  0.19020604 0.23133826 0.19104046 0.22491108 0.15815732\n",
      " 0.15807578 0.17401455 0.18085864 0.15666813 0.18192178 0.19241002\n",
      " 0.17906322 0.20918533 0.23102202 0.20152542 0.1896111  0.2413222\n",
      " 0.21368435 0.20782515 0.2544346  0.17565589 0.19399036 0.16283044\n",
      " 0.18721211 0.21939707 0.16334507 0.17726257 0.16871189 0.18299857\n",
      " 0.18366267 0.278653   0.21577999 0.24389179 0.25088447 0.17486626\n",
      " 0.22424513 0.23657393 0.23121074 0.19055791 0.17983888 0.18860808\n",
      " 0.22110194 0.22624442 0.18771926 0.17163756 0.17500204 0.2307323\n",
      " 0.22389752 0.19573192 0.19217864 0.20302686 0.1682887  0.23628849\n",
      " 0.23391785 0.17263688 0.2031191  0.23203877 0.22284344 0.1883086\n",
      " 0.17627941 0.20392765 0.1999323  0.16174905 0.23480245 0.18908209\n",
      " 0.1679013  0.16148083 0.16126214 0.18041822 0.16863948 0.20420554\n",
      " 0.15107897 0.17775284 0.17755076 0.16807169 0.2314201  0.24716163\n",
      " 0.20626473 0.16294032 0.17481554 0.23086208 0.20867103 0.2323243\n",
      " 0.20679641 0.23915267 0.20875615 0.2391975  0.1820934  0.16342701\n",
      " 0.32936516 0.18139814 0.17597847 0.1759633 ] Train accuracy: 0.986944452755981\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [4.3693185e-04 2.9242188e-03 1.8559396e-04 ... 4.2012334e-04 4.4454634e-04\n",
      " 3.5762787e-05] + 0.14067198 = [0.14110892 0.1435962  0.14085758 ... 0.1410921  0.14111653 0.14070775]\n",
      "\n",
      "\n",
      "Epoch Number: 60\n",
      "Train Loss: [0.22032155 0.1837644  0.16278535 0.22994338 0.17921041 0.22308728\n",
      " 0.19028868 0.17933206 0.22368418 0.18926975 0.21301405 0.15637654\n",
      " 0.16191567 0.16743535 0.1773472  0.15998834 0.18298557 0.1895219\n",
      " 0.17322455 0.20402405 0.22834563 0.21265651 0.18647502 0.24131833\n",
      " 0.21735004 0.18497863 0.22148585 0.17473306 0.19416627 0.16156483\n",
      " 0.2034759  0.2167108  0.16240516 0.18561795 0.15652385 0.17982802\n",
      " 0.17949755 0.24975798 0.19836846 0.20005251 0.2576507  0.17142415\n",
      " 0.20658267 0.22925025 0.22001009 0.20050211 0.17858905 0.1800368\n",
      " 0.20726843 0.21167734 0.17707588 0.17392485 0.17883874 0.21177565\n",
      " 0.18082322 0.18041982 0.192499   0.16537134 0.17203194 0.22316074\n",
      " 0.22749703 0.16718808 0.18937886 0.24676779 0.21190818 0.1931511\n",
      " 0.18212534 0.18441495 0.19822985 0.16004102 0.25032103 0.166155\n",
      " 0.16563576 0.15595272 0.15993948 0.17822692 0.18298861 0.20377496\n",
      " 0.1608113  0.17743528 0.18127972 0.18104364 0.22311097 0.25085688\n",
      " 0.20697325 0.16057989 0.17005691 0.17227347 0.19701467 0.20290631\n",
      " 0.20595495 0.22366802 0.2000158  0.2350483  0.18956164 0.16611828\n",
      " 0.27005282 0.17532791 0.19457516 0.17595479] Train accuracy: 0.9886111186610328\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [3.4362078e-04 1.4230609e-03 2.9309094e-04 ... 3.7150085e-04 4.3034554e-05\n",
      " 4.2676926e-05] + 0.1394588 = [0.13980243 0.14088187 0.1397519  ... 0.1398303  0.13950184 0.13950148]\n",
      "\n",
      "\n",
      "Epoch Number: 61\n",
      "Train Loss: [0.2086348  0.17482889 0.1593725  0.2262489  0.17368068 0.23269974\n",
      " 0.19418435 0.18069759 0.2191211  0.19244581 0.20526761 0.15188125\n",
      " 0.15306783 0.17459582 0.17487606 0.16178192 0.17536251 0.19075917\n",
      " 0.17108002 0.20085146 0.23163435 0.20586434 0.18660723 0.23372239\n",
      " 0.21013236 0.19113591 0.21532093 0.17766774 0.18163985 0.15832107\n",
      " 0.22089206 0.2132581  0.16385256 0.16858831 0.1587386  0.1804459\n",
      " 0.17540722 0.25821674 0.1868891  0.21288794 0.24773683 0.17366642\n",
      " 0.21236658 0.22911525 0.22012056 0.18712395 0.17655067 0.1780029\n",
      " 0.20407419 0.20807397 0.17793089 0.17098488 0.17906773 0.22423172\n",
      " 0.21375799 0.1685737  0.17683327 0.16559578 0.16206092 0.21950155\n",
      " 0.22343792 0.1664877  0.19316515 0.23548052 0.21717328 0.20238176\n",
      " 0.16898416 0.19347753 0.19426277 0.1603162  0.2483827  0.17808455\n",
      " 0.16653275 0.15202993 0.16512182 0.1865426  0.17066479 0.19549215\n",
      " 0.14504476 0.17450947 0.18034475 0.16915463 0.22000305 0.2395752\n",
      " 0.20529309 0.15860777 0.16642062 0.17711806 0.2031588  0.19758175\n",
      " 0.20610368 0.20759644 0.19807264 0.22136673 0.18202934 0.16648561\n",
      " 0.2811626  0.17260078 0.17844127 0.17093314] Train accuracy: 0.9893055632710457\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: [2.5364757e-04 2.6788563e-03 3.9021671e-04 ... 3.6531687e-04 0.0000000e+00\n",
      " 3.9815903e-05] + 0.13814896 = [0.13840261 0.14082782 0.13853918 ... 0.13851428 0.13814896 0.13818878]\n",
      "\n",
      "\n",
      "Epoch Number: 62\n",
      "Train Loss: [0.21366772 0.17968746 0.15923432 0.22122237 0.16929446 0.23209614\n",
      " 0.1906517  0.17791419 0.2078527  0.1912259  0.2109225  0.15072405\n",
      " 0.15283942 0.17435654 0.17897981 0.1588886  0.17698859 0.18850198\n",
      " 0.16964905 0.20419042 0.23126653 0.20725276 0.18683992 0.24040335\n",
      " 0.20721143 0.18728012 0.21547812 0.17145203 0.19022417 0.15660188\n",
      " 0.21245795 0.21216817 0.16345482 0.1712973  0.15452987 0.1792133\n",
      " 0.17687532 0.25678918 0.19282931 0.18728335 0.24644221 0.17301816\n",
      " 0.20748483 0.22812945 0.22035311 0.20571783 0.18112853 0.17242017\n",
      " 0.2069069  0.20989    0.1870964  0.1712899  0.1963593  0.22222437\n",
      " 0.18101056 0.17994364 0.18655366 0.16581444 0.15942442 0.21813208\n",
      " 0.22012779 0.16559924 0.18644415 0.24440919 0.21460775 0.19844928\n",
      " 0.17311071 0.18296978 0.19881725 0.15706213 0.24456845 0.1629313\n",
      " 0.16176605 0.15251137 0.16918835 0.18345898 0.16837355 0.19949763\n",
      " 0.14510167 0.17272899 0.17527328 0.1677732  0.22112888 0.23766671\n",
      " 0.19891407 0.16150299 0.17018744 0.16566278 0.21674077 0.1965225\n",
      " 0.21186846 0.22526608 0.20146517 0.23001586 0.18464291 0.16662532\n",
      " 0.27801052 0.1880128  0.1865368  0.17378716] Train accuracy: 0.988472230732441\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [2.8760731e-04 1.5987456e-03 2.4613738e-04 ... 4.0261447e-04 0.0000000e+00\n",
      " 3.3020973e-05] + 0.13810849 = [0.1383961  0.13970724 0.13835463 ... 0.1385111  0.13810849 0.13814151]\n",
      "\n",
      "\n",
      "Epoch Number: 63\n",
      "Train Loss: [0.20388728 0.18175848 0.16047756 0.22501606 0.1735983  0.22623608\n",
      " 0.1911207  0.1811286  0.21039361 0.19007762 0.2096501  0.15186751\n",
      " 0.15569413 0.16635238 0.17559028 0.16046624 0.17594531 0.18825515\n",
      " 0.17293009 0.20255488 0.24641722 0.21435314 0.18602517 0.2420842\n",
      " 0.20984165 0.18507293 0.22415273 0.17818487 0.1791325  0.15796828\n",
      " 0.20189959 0.21058112 0.15961996 0.16895086 0.16101469 0.1748532\n",
      " 0.18471456 0.2583649  0.18661618 0.1995039  0.24813549 0.16981551\n",
      " 0.20710933 0.22579779 0.21547657 0.18984193 0.1899751  0.16974932\n",
      " 0.21025917 0.20849924 0.18235482 0.16753899 0.16950983 0.22831066\n",
      " 0.2024723  0.17478341 0.17769954 0.16701223 0.16943657 0.22168154\n",
      " 0.22138049 0.16609927 0.18813948 0.23559168 0.2117107  0.19380666\n",
      " 0.1738757  0.18629521 0.19903009 0.16164644 0.2417344  0.18465132\n",
      " 0.1640572  0.15061472 0.16189896 0.17988013 0.17996567 0.19016802\n",
      " 0.14860368 0.17220093 0.17544444 0.16873083 0.2146288  0.23588668\n",
      " 0.21023907 0.16697548 0.167879   0.18965295 0.21589343 0.20665509\n",
      " 0.21433944 0.21650471 0.1999935  0.22837597 0.18180455 0.16745406\n",
      " 0.26769277 0.17231777 0.18068646 0.1726473 ] Train accuracy: 0.988055562807454\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: [3.0036271e-04 3.1076074e-03 3.6352873e-04 ... 9.5036626e-04 2.8729439e-05\n",
      " 3.5643578e-05] + 0.13835849 = [0.13865885 0.1414661  0.13872202 ... 0.13930885 0.13838722 0.13839413]\n",
      "\n",
      "\n",
      "Epoch Number: 64\n",
      "Train Loss: [0.21940197 0.18024513 0.16035283 0.23037992 0.1692101  0.24140617\n",
      " 0.19150175 0.18584688 0.2130748  0.19069375 0.21064323 0.15407437\n",
      " 0.15776439 0.17962152 0.17878492 0.15713662 0.18566515 0.18465462\n",
      " 0.17285629 0.21083248 0.23261338 0.21326071 0.18734659 0.2460995\n",
      " 0.20409209 0.2064774  0.21963197 0.17522931 0.18933403 0.15961376\n",
      " 0.19824213 0.2173315  0.16336946 0.17827488 0.16018713 0.1789928\n",
      " 0.18514659 0.25793213 0.19801635 0.22277911 0.2543697  0.17068659\n",
      " 0.209858   0.23380218 0.22993265 0.19206868 0.18743047 0.18048225\n",
      " 0.21452542 0.20520139 0.17943317 0.17101364 0.18272768 0.22353551\n",
      " 0.1835205  0.18829945 0.1783242  0.16805108 0.16411409 0.22299868\n",
      " 0.22192356 0.16778822 0.19139639 0.23519556 0.22228731 0.21920025\n",
      " 0.17416298 0.20477715 0.20243534 0.16766933 0.25099653 0.17833748\n",
      " 0.16839647 0.15194957 0.16761318 0.18223256 0.17286065 0.20055741\n",
      " 0.15246648 0.17389902 0.17989604 0.16902293 0.23346537 0.25306478\n",
      " 0.21300918 0.15354104 0.16839454 0.18374151 0.21049719 0.21058673\n",
      " 0.21116668 0.22255985 0.20223346 0.22402433 0.18568653 0.16395448\n",
      " 0.3042397  0.17061335 0.18611465 0.17800564] Train accuracy: 0.9863888960745599\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.6485155e-04 1.6695559e-03 4.2441487e-04 ... 4.4696480e-03 1.8571317e-04\n",
      " 2.9087067e-05] + 0.13945691 = [0.13962176 0.14112647 0.13988133 ... 0.14392656 0.13964263 0.139486  ]\n",
      "\n",
      "\n",
      "Epoch Number: 65\n",
      "Train Loss: [0.2082391  0.17725825 0.16176204 0.22326463 0.17225556 0.2246733\n",
      " 0.19430223 0.17659852 0.21383627 0.18957342 0.20889169 0.1533033\n",
      " 0.15356532 0.17058443 0.17801982 0.16357654 0.17691043 0.18333147\n",
      " 0.1711002  0.20408    0.23812765 0.20682888 0.18619448 0.23434141\n",
      " 0.20855708 0.18749157 0.21144369 0.17584652 0.19396377 0.15853862\n",
      " 0.22340178 0.2144756  0.16094439 0.17360619 0.15326141 0.17260788\n",
      " 0.17593528 0.25332606 0.19404012 0.18580836 0.2502269  0.170883\n",
      " 0.20988815 0.22524622 0.21643892 0.19854657 0.17741752 0.17559858\n",
      " 0.20356078 0.20968038 0.18304472 0.16810113 0.17811373 0.21167988\n",
      " 0.17718154 0.16954438 0.1820361  0.17057171 0.16786578 0.22405714\n",
      " 0.22175004 0.16648957 0.18481214 0.25277373 0.21612804 0.19665228\n",
      " 0.17693692 0.18498439 0.19875932 0.15548924 0.2511683  0.16238445\n",
      " 0.16411501 0.15229997 0.16019727 0.1798903  0.1724966  0.20442767\n",
      " 0.14941017 0.1736597  0.17861438 0.17080231 0.21723543 0.2456025\n",
      " 0.20177606 0.16290104 0.1735577  0.16856436 0.20175527 0.19822301\n",
      " 0.20982347 0.21993981 0.20103449 0.21704635 0.18895973 0.1676289\n",
      " 0.27681443 0.19343294 0.19908884 0.17891552] Train accuracy: 0.9887500082453092\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [2.3600459e-04 1.4281869e-03 3.2395124e-04 ... 1.0182694e-02 0.0000000e+00\n",
      " 3.5405159e-05] + 0.13813685 = [0.13837285 0.13956504 0.1384608  ... 0.14831954 0.13813685 0.13817225]\n",
      "\n",
      "\n",
      "Epoch Number: 66\n",
      "Train Loss: [0.2268983  0.18951951 0.16034529 0.22629166 0.17494415 0.2294175\n",
      " 0.18967147 0.1790672  0.21405637 0.18885022 0.20994447 0.15273768\n",
      " 0.15190393 0.17118822 0.17912918 0.1618252  0.17857027 0.19101673\n",
      " 0.18279622 0.20152633 0.23567802 0.22058174 0.18073943 0.23969105\n",
      " 0.20800237 0.19422692 0.22145961 0.18440561 0.18129058 0.15910642\n",
      " 0.1963404  0.21514782 0.15974979 0.1735064  0.1594156  0.17305425\n",
      " 0.1896272  0.24945863 0.18083663 0.2156687  0.25100943 0.17293859\n",
      " 0.20956816 0.23084408 0.22033192 0.19006132 0.18844979 0.17217682\n",
      " 0.20609267 0.2074746  0.18638438 0.18477868 0.16551603 0.22989863\n",
      " 0.20042413 0.1694254  0.1811158  0.16381846 0.16039808 0.22436585\n",
      " 0.22582597 0.16665591 0.18434851 0.2351423  0.21684049 0.19617893\n",
      " 0.17983443 0.18896495 0.19333059 0.16326116 0.24248092 0.1816501\n",
      " 0.16173322 0.1532093  0.16287802 0.17714788 0.17765304 0.18590385\n",
      " 0.15593983 0.17327565 0.17028159 0.16838336 0.22347951 0.2386616\n",
      " 0.21044835 0.1584715  0.16612254 0.20127195 0.2170846  0.19954371\n",
      " 0.21636455 0.21648936 0.20297967 0.21864206 0.18088594 0.16532642\n",
      " 0.26973325 0.17317161 0.17792195 0.17484806] Train accuracy: 0.9880555619796118\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: [2.3826957e-04 2.3168921e-03 3.8783252e-04 ... 6.8352968e-03 0.0000000e+00\n",
      " 3.2424927e-05] + 0.13892163 = [0.1391599  0.14123853 0.13930947 ... 0.14575693 0.13892163 0.13895406]\n",
      "\n",
      "\n",
      "Epoch Number: 67\n",
      "Train Loss: [0.20557816 0.17718643 0.1581647  0.23753619 0.16625676 0.23242848\n",
      " 0.18495323 0.18093795 0.20523944 0.18882357 0.20029493 0.15412387\n",
      " 0.15295751 0.17660946 0.18337126 0.16001543 0.18067276 0.17999817\n",
      " 0.17184518 0.21175879 0.2350452  0.2294063  0.1815634  0.24668404\n",
      " 0.20453489 0.1903244  0.22984627 0.17113692 0.19746368 0.15976493\n",
      " 0.20850833 0.21275257 0.16003868 0.17139073 0.15697402 0.17969617\n",
      " 0.17363127 0.25935188 0.1817337  0.19312136 0.2495734  0.17061657\n",
      " 0.21038535 0.22500587 0.21635166 0.19360855 0.1747406  0.18490553\n",
      " 0.18450232 0.2043621  0.17543806 0.16593182 0.17431973 0.21163899\n",
      " 0.1814491  0.18951628 0.17637506 0.17218488 0.16491032 0.22133581\n",
      " 0.22102326 0.16703741 0.18283845 0.22816592 0.22515768 0.22018968\n",
      " 0.1715682  0.18989916 0.20689751 0.1658147  0.25287184 0.16709822\n",
      " 0.1704497  0.1492232  0.16384892 0.18599334 0.17047606 0.19702041\n",
      " 0.15546846 0.17096893 0.17519027 0.1833694  0.21770047 0.24269968\n",
      " 0.20962729 0.15965588 0.16982913 0.17875111 0.21059433 0.20380916\n",
      " 0.23388594 0.22493757 0.20583074 0.22270125 0.17983001 0.16565913\n",
      " 0.287968   0.17054884 0.1874656  0.17211404] Train accuracy: 0.9884722315602832\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [1.11937523e-04 1.17935240e-03 3.02031636e-04 ... 3.97403538e-03\n",
      " 1.15036964e-04 4.39882278e-05] + 0.13868223 = [0.13879417 0.13986158 0.13898426 ... 0.14265627 0.13879727 0.13872622]\n",
      "\n",
      "\n",
      "Epoch Number: 68\n",
      "Train Loss: [0.22194141 0.1748458  0.15896505 0.22104235 0.17345846 0.22949502\n",
      " 0.18178757 0.17917207 0.21082386 0.18992843 0.21582808 0.15289028\n",
      " 0.1601612  0.17394908 0.18096605 0.1591875  0.17513941 0.18235587\n",
      " 0.17126101 0.20653589 0.23560855 0.19826904 0.18248576 0.23881496\n",
      " 0.21111375 0.18613355 0.22071654 0.18056875 0.18847668 0.16092914\n",
      " 0.19115396 0.2102649  0.15999202 0.16468534 0.16015558 0.17624478\n",
      " 0.17673041 0.24567792 0.19183189 0.1995602  0.25118116 0.17187919\n",
      " 0.20904025 0.23786119 0.2251183  0.19896843 0.1787077  0.17208785\n",
      " 0.21116234 0.21423861 0.18337862 0.16471887 0.18591267 0.21990627\n",
      " 0.18594939 0.17057316 0.18344124 0.1925756  0.16307168 0.22211382\n",
      " 0.22078459 0.16484489 0.18112102 0.23804396 0.2132727  0.19663252\n",
      " 0.1779226  0.18283056 0.1970954  0.15499559 0.24853842 0.17353651\n",
      " 0.16512793 0.15187663 0.16336983 0.17852198 0.17273025 0.20259598\n",
      " 0.1469233  0.17518711 0.17691526 0.17597272 0.21686584 0.23552592\n",
      " 0.20207424 0.16331267 0.17130512 0.19619073 0.21701916 0.20021479\n",
      " 0.21562846 0.25127283 0.19561471 0.22382522 0.18795824 0.16804487\n",
      " 0.27508542 0.1885385  0.18682948 0.17755821] Train accuracy: 0.9883333411481645\n",
      "Test accuracy 0.933732\n",
      "MarginLoss + RegLoss: [1.5293062e-04 4.6930909e-03 2.7389824e-04 ... 1.7135546e-02 0.0000000e+00\n",
      " 3.5643578e-05] + 0.13838662 = [0.13853955 0.14307971 0.13866052 ... 0.15552217 0.13838662 0.13842227]\n",
      "\n",
      "\n",
      "Epoch Number: 69\n",
      "Train Loss: [0.21137714 0.18602583 0.16244599 0.24010107 0.18052244 0.22629932\n",
      " 0.18955915 0.1904595  0.21752907 0.18999921 0.20923728 0.15532452\n",
      " 0.1533725  0.16227537 0.17695747 0.15982954 0.18702503 0.18812394\n",
      " 0.18517429 0.20755571 0.23759945 0.22317751 0.18886587 0.24326399\n",
      " 0.2130758  0.18497778 0.21771659 0.17928839 0.18864048 0.16304456\n",
      " 0.19624251 0.21589242 0.1549502  0.17943355 0.16224927 0.18168128\n",
      " 0.24304043 0.27164343 0.18106645 0.18017657 0.25751162 0.16989172\n",
      " 0.20618163 0.2213183  0.22278783 0.19699788 0.19114628 0.1823363\n",
      " 0.22876365 0.21672195 0.18563998 0.17922264 0.17048848 0.23528947\n",
      " 0.18975253 0.19504407 0.17398731 0.16976236 0.16876966 0.22458227\n",
      " 0.22752497 0.16969746 0.18631597 0.23547384 0.237513   0.18795474\n",
      " 0.17283152 0.18532792 0.20627691 0.1617808  0.24965239 0.1662262\n",
      " 0.16528471 0.15239562 0.17826809 0.17881437 0.2087162  0.19436392\n",
      " 0.16844772 0.23057087 0.17826855 0.1637073  0.23611529 0.24241795\n",
      " 0.22638018 0.15809403 0.1709394  0.17027277 0.21848606 0.21118563\n",
      " 0.2213133  0.22467573 0.20325527 0.20664348 0.18695623 0.16677515\n",
      " 0.2653786  0.16994175 0.18573333 0.18458615] Train accuracy: 0.9868055623438623\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [2.0037591e-04 5.6667626e-04 3.3098459e-04 ... 7.0015043e-03 4.3585896e-04\n",
      " 4.1365623e-05] + 0.1403873 = [0.14058767 0.14095397 0.14071828 ... 0.1473888  0.14082316 0.14042866]\n",
      "\n",
      "\n",
      "Epoch Number: 70\n",
      "Train Loss: [0.23709798 0.17912154 0.16113175 0.24961543 0.17391665 0.2512377\n",
      " 0.18770197 0.2144452  0.21515675 0.19053203 0.20550258 0.15872611\n",
      " 0.16422705 0.1829993  0.1832868  0.16161238 0.18813632 0.19786583\n",
      " 0.17232066 0.21921001 0.23569168 0.21073745 0.18639696 0.24988045\n",
      " 0.2086389  0.232273   0.21768822 0.17503244 0.18447778 0.16206257\n",
      " 0.2045551  0.20805472 0.16621183 0.19509111 0.15925638 0.18716556\n",
      " 0.1723586  0.2511916  0.23460147 0.23166148 0.25452048 0.1779774\n",
      " 0.21595767 0.24272503 0.23421447 0.20436499 0.18272611 0.18648463\n",
      " 0.20967539 0.21778144 0.18523633 0.1804855  0.18043925 0.21793362\n",
      " 0.19364205 0.17461704 0.19851997 0.17768186 0.1874677  0.23254585\n",
      " 0.2196367  0.16890584 0.19575147 0.22290267 0.23162399 0.21349004\n",
      " 0.18552268 0.19754444 0.20356749 0.16112031 0.23579039 0.20307738\n",
      " 0.18421757 0.15335397 0.16163082 0.18643136 0.17400509 0.2053743\n",
      " 0.14752045 0.17525738 0.17555515 0.17633845 0.23081943 0.25631842\n",
      " 0.20151263 0.15929806 0.1738623  0.19675523 0.21820062 0.20707278\n",
      " 0.21765245 0.24720573 0.20341954 0.26717228 0.1844915  0.16647713\n",
      " 0.3195973  0.19910452 0.18897381 0.17219031] Train accuracy: 0.9861111218730608\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: [1.5805662e-04 2.8445721e-03 2.0395219e-04 ... 1.5135437e-02 1.1277199e-04\n",
      " 4.4703484e-05] + 0.14008114 = [0.1402392  0.14292571 0.14028509 ... 0.15521657 0.14019391 0.14012584]\n",
      "\n",
      "\n",
      "Epoch Number: 71\n",
      "Train Loss: [0.20912364 0.17441808 0.16024965 0.22056831 0.17448956 0.22657543\n",
      " 0.18162343 0.18211728 0.21535021 0.19003627 0.20375429 0.15374333\n",
      " 0.1507991  0.16899353 0.17968182 0.16561395 0.18197088 0.18976103\n",
      " 0.17459434 0.20413038 0.22603361 0.2040122  0.18995719 0.2347347\n",
      " 0.21193826 0.18578187 0.23324895 0.1770064  0.1856778  0.157687\n",
      " 0.20402922 0.20864363 0.15871084 0.17063469 0.1559246  0.17399718\n",
      " 0.18784398 0.2551355  0.19259581 0.1971963  0.25832093 0.17173141\n",
      " 0.21106833 0.22966044 0.21216393 0.1918245  0.17607038 0.18147284\n",
      " 0.19124635 0.20619324 0.1838314  0.16758148 0.17127462 0.21164767\n",
      " 0.2095914  0.192541   0.18314447 0.16789278 0.1664142  0.21615015\n",
      " 0.22534402 0.16869973 0.18133801 0.24142987 0.21827441 0.18751821\n",
      " 0.17565864 0.18221806 0.20111977 0.15620625 0.24346921 0.16840963\n",
      " 0.17292431 0.15313415 0.1602241  0.18332428 0.16650578 0.19479755\n",
      " 0.14779362 0.1724414  0.17713329 0.17237067 0.21813147 0.23654635\n",
      " 0.2077732  0.1682428  0.17037795 0.18963115 0.19653083 0.2148636\n",
      " 0.20996976 0.22272904 0.19716984 0.22143573 0.18502341 0.17011344\n",
      " 0.2770953  0.17038259 0.17532992 0.17609115] Train accuracy: 0.9888888961739011\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [1.5245378e-04 1.5516132e-03 3.6090612e-04 ... 1.0757193e-02 0.0000000e+00\n",
      " 4.3034554e-05] + 0.13903736 = [0.13918981 0.14058897 0.13939826 ... 0.14979455 0.13903736 0.13908039]\n",
      "\n",
      "\n",
      "Epoch Number: 72\n",
      "Train Loss: [0.22219877 0.17782937 0.15962714 0.23413661 0.1774866  0.2338452\n",
      " 0.18443811 0.17529395 0.2088448  0.18923803 0.20761111 0.15264027\n",
      " 0.15219948 0.17423299 0.18356675 0.15621012 0.18111044 0.18810767\n",
      " 0.17376988 0.20861615 0.22993006 0.20838337 0.18359157 0.23996894\n",
      " 0.21301286 0.18640378 0.20256744 0.16940956 0.18194537 0.15969726\n",
      " 0.19642958 0.2131105  0.1618754  0.17252856 0.15739468 0.1795447\n",
      " 0.18302576 0.24868073 0.18413718 0.19879821 0.2505089  0.17442125\n",
      " 0.20673636 0.2263061  0.22623354 0.19985059 0.18309072 0.18772504\n",
      " 0.2082017  0.21031985 0.18212323 0.17320898 0.18448181 0.22577119\n",
      " 0.18075623 0.17973001 0.17836934 0.16913757 0.16385648 0.22482021\n",
      " 0.21996492 0.16512598 0.18909304 0.22916299 0.21537882 0.19515345\n",
      " 0.17199586 0.1877714  0.19494241 0.15613128 0.2466135  0.16872123\n",
      " 0.15905946 0.15150729 0.15931481 0.18221837 0.1675136  0.21649092\n",
      " 0.14742237 0.1738705  0.17480332 0.17285468 0.22285455 0.24007344\n",
      " 0.1972205  0.16107301 0.17145838 0.16688254 0.21025956 0.19587961\n",
      " 0.20910203 0.22511952 0.19923182 0.22438155 0.18350285 0.16518052\n",
      " 0.27538317 0.19743945 0.1912483  0.17480718] Train accuracy: 0.9881944532195727\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [1.4720857e-04 8.5769594e-04 3.1097233e-04 ... 3.1055808e-03 0.0000000e+00\n",
      " 4.2676926e-05] + 0.13865276 = [0.13879997 0.13951045 0.13896373 ... 0.14175834 0.13865276 0.13869543]\n",
      "\n",
      "\n",
      "Epoch Number: 73\n",
      "Train Loss: [0.2060315  0.17296395 0.15914874 0.21479185 0.1727758  0.22549483\n",
      " 0.18525036 0.18375967 0.211887   0.19096854 0.20056444 0.15103334\n",
      " 0.15364194 0.16491178 0.1804197  0.16070913 0.17565133 0.17533761\n",
      " 0.17905462 0.20203577 0.23879102 0.20138983 0.18120158 0.23075467\n",
      " 0.20874503 0.18268341 0.22336432 0.18387127 0.17123902 0.16010429\n",
      " 0.19696206 0.21495937 0.15825681 0.17836426 0.15910168 0.1778166\n",
      " 0.18538363 0.25038978 0.18323638 0.19202241 0.24989486 0.16783868\n",
      " 0.21096586 0.22196174 0.20752995 0.18407804 0.17327556 0.17413247\n",
      " 0.21277596 0.20626149 0.17588839 0.16337675 0.16937673 0.23753482\n",
      " 0.19355318 0.17839146 0.18002734 0.16747603 0.1707503  0.2230353\n",
      " 0.21870726 0.16785285 0.18697071 0.2284766  0.21059737 0.19859284\n",
      " 0.17741317 0.19104725 0.19905642 0.15725964 0.24949871 0.18594176\n",
      " 0.16799262 0.1500951  0.16977039 0.18030955 0.1777392  0.18958628\n",
      " 0.14981282 0.17335325 0.17723967 0.17260362 0.21599692 0.23670417\n",
      " 0.21030518 0.15804473 0.16873907 0.17969652 0.19730297 0.20606966\n",
      " 0.21236904 0.21962194 0.20145366 0.23031953 0.1777274  0.16734573\n",
      " 0.27221778 0.16670634 0.17459592 0.17715183] Train accuracy: 0.9887500082453092\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [1.4983118e-04 2.4681538e-03 3.6281347e-04 ... 6.1577111e-03 0.0000000e+00\n",
      " 5.0306320e-05] + 0.13831854 = [0.13846837 0.14078669 0.13868135 ... 0.14447625 0.13831854 0.13836884]\n",
      "\n",
      "\n",
      "Epoch Number: 74\n",
      "Train Loss: [0.22442603 0.17722301 0.15868853 0.21307893 0.16974132 0.2335599\n",
      " 0.17982163 0.180138   0.21001248 0.1871412  0.21093231 0.15164913\n",
      " 0.15229185 0.17890365 0.17945814 0.15787917 0.18133715 0.17580256\n",
      " 0.1702455  0.20557493 0.23031068 0.19966225 0.1836466  0.23499867\n",
      " 0.20936139 0.1863916  0.21991874 0.17430526 0.18933226 0.15722391\n",
      " 0.194689   0.21211341 0.16155037 0.17033295 0.1593476  0.17392756\n",
      " 0.17643537 0.24506672 0.18149179 0.19205806 0.25186843 0.17274362\n",
      " 0.20715708 0.22908664 0.22118491 0.18163158 0.1791777  0.1811191\n",
      " 0.20338155 0.20272675 0.17638154 0.16633    0.16288413 0.22101459\n",
      " 0.17502233 0.20053321 0.18386395 0.16302504 0.16095974 0.21277826\n",
      " 0.21726134 0.16640295 0.18478411 0.22651699 0.21854599 0.20303655\n",
      " 0.17082094 0.18243068 0.19412594 0.15757409 0.24603163 0.16126959\n",
      " 0.16004877 0.1497655  0.16029511 0.18167624 0.16433688 0.18636097\n",
      " 0.14550345 0.17168972 0.17543226 0.17257781 0.2241132  0.23806106\n",
      " 0.2059748  0.15768084 0.16883296 0.17053813 0.20195094 0.20399481\n",
      " 0.2025553  0.22704938 0.20143716 0.21290977 0.18099187 0.16483837\n",
      " 0.27838683 0.17034623 0.18085758 0.17434664] Train accuracy: 0.9902777853939269\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.3874471e-04 1.9838214e-03 4.2727590e-04 ... 9.5844865e-03 0.0000000e+00\n",
      " 4.7564507e-05] + 0.13795128 = [0.13809003 0.1399351  0.13837856 ... 0.14753577 0.13795128 0.13799885]\n",
      "\n",
      "\n",
      "Epoch Number: 75\n",
      "Train Loss: [0.2103199  0.17403549 0.15854865 0.21915357 0.17180105 0.22842976\n",
      " 0.17895573 0.18254514 0.20920604 0.18689765 0.20001222 0.15006794\n",
      " 0.15159899 0.17394497 0.18013385 0.15932968 0.17850439 0.18265218\n",
      " 0.17251061 0.20650424 0.24447165 0.19804375 0.17965625 0.24162346\n",
      " 0.20343426 0.19035615 0.20525901 0.17312574 0.17843774 0.15685207\n",
      " 0.19610372 0.20717722 0.16214582 0.17393257 0.1564089  0.17255892\n",
      " 0.17673275 0.25552675 0.18855226 0.2027794  0.25082338 0.17579524\n",
      " 0.20706297 0.22381292 0.22319876 0.1849372  0.1814622  0.17260116\n",
      " 0.20400809 0.20811161 0.18222767 0.1672978  0.16838476 0.21904045\n",
      " 0.17968133 0.17099947 0.17885248 0.16944061 0.16288334 0.22220688\n",
      " 0.21518323 0.16581526 0.19369137 0.22215688 0.21347162 0.20496328\n",
      " 0.17114325 0.18791124 0.1950385  0.1549461  0.25061688 0.17936176\n",
      " 0.16351666 0.14857407 0.15818356 0.18019313 0.17271739 0.19819067\n",
      " 0.14437103 0.17201698 0.17088352 0.17018624 0.20971705 0.23886397\n",
      " 0.20360494 0.16447997 0.16842288 0.17938682 0.20430952 0.19621426\n",
      " 0.20931959 0.23139757 0.2007703  0.21757928 0.18056461 0.1657443\n",
      " 0.2826406  0.17091101 0.18632719 0.17338905] Train accuracy: 0.9886111203167174\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: [1.2312829e-04 4.4684559e-03 3.5232306e-04 ... 2.4387717e-02 0.0000000e+00\n",
      " 4.5895576e-05] + 0.1376415 = [0.13776463 0.14210996 0.13799383 ... 0.16202922 0.1376415  0.1376874 ]\n",
      "\n",
      "\n",
      "Epoch Number: 76\n",
      "Train Loss: [0.21815182 0.17878494 0.1590182  0.22266014 0.1734912  0.22203839\n",
      " 0.19117858 0.18249156 0.2102188  0.18927497 0.21196459 0.15315524\n",
      " 0.15259722 0.17135683 0.18143448 0.15968034 0.17864925 0.17099077\n",
      " 0.17489141 0.2030434  0.2342609  0.21621764 0.18372656 0.24113856\n",
      " 0.20340186 0.19917883 0.22114187 0.18163131 0.18666603 0.16103996\n",
      " 0.19341713 0.21964607 0.16065097 0.17858464 0.15554924 0.17753571\n",
      " 0.18836474 0.25743568 0.1923222  0.19863166 0.25397924 0.1684656\n",
      " 0.21056998 0.22758262 0.21875766 0.18902265 0.18591124 0.17200387\n",
      " 0.21347745 0.20299765 0.18807423 0.16796383 0.17553896 0.2200039\n",
      " 0.18278214 0.19166669 0.17574428 0.16286087 0.17254381 0.22284706\n",
      " 0.21927062 0.16874221 0.18685466 0.23735431 0.2223888  0.19604948\n",
      " 0.17632127 0.19720316 0.2007158  0.15861708 0.24958093 0.17065233\n",
      " 0.18455338 0.15017933 0.180188   0.18266386 0.18374708 0.20071343\n",
      " 0.14605036 0.17109403 0.17551786 0.17037708 0.22005287 0.25049394\n",
      " 0.21000777 0.15471424 0.16941553 0.17027263 0.20252931 0.2094144\n",
      " 0.21317188 0.22749826 0.19570059 0.20954102 0.18105453 0.16530673\n",
      " 0.28676784 0.17277193 0.1763485  0.17916283] Train accuracy: 0.9870833415124152\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: [1.4220178e-04 1.7036051e-03 4.9448013e-04 ... 6.9354475e-03 0.0000000e+00\n",
      " 4.2080879e-05] + 0.13914132 = [0.13928352 0.14084493 0.1396358  ... 0.14607677 0.13914132 0.1391834 ]\n",
      "\n",
      "\n",
      "Epoch Number: 77\n",
      "Train Loss: [0.20647778 0.17697535 0.16163552 0.21558753 0.17433232 0.224781\n",
      " 0.17878526 0.1789271  0.21364945 0.18758646 0.20500802 0.15211323\n",
      " 0.15188059 0.16875862 0.18292205 0.15719034 0.18050466 0.17467414\n",
      " 0.16963364 0.21459422 0.22551645 0.20293829 0.18139182 0.23560983\n",
      " 0.20937096 0.17983346 0.21349834 0.16904902 0.19198692 0.15967847\n",
      " 0.19013041 0.21560219 0.15652794 0.16661578 0.15450418 0.17811772\n",
      " 0.1739757  0.24159262 0.17729676 0.18912771 0.2523242  0.16958241\n",
      " 0.20861667 0.22282614 0.21597302 0.19506365 0.17457615 0.18231732\n",
      " 0.19868547 0.2094361  0.1697219  0.16312554 0.18531902 0.22160691\n",
      " 0.17258032 0.17316341 0.1830867  0.17297284 0.16221863 0.21750985\n",
      " 0.22265686 0.16545628 0.18921438 0.23233697 0.21550325 0.19600041\n",
      " 0.18059492 0.19405031 0.18963277 0.15513618 0.25335634 0.17439851\n",
      " 0.16269974 0.15162227 0.16646682 0.18072636 0.16475081 0.19616757\n",
      " 0.14855617 0.17562668 0.18098009 0.16609886 0.2161898  0.2344643\n",
      " 0.2009655  0.16019294 0.17186682 0.16954733 0.20359303 0.20354956\n",
      " 0.2128805  0.23251258 0.19901328 0.21499683 0.18745032 0.16582723\n",
      " 0.27140796 0.17336406 0.18908747 0.17723085] Train accuracy: 0.9895833416117562\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [2.1015108e-04 4.3517947e-03 2.9821694e-04 ... 2.9262304e-03 0.0000000e+00\n",
      " 5.0187111e-05] + 0.13763829 = [0.13784844 0.14199008 0.1379365  ... 0.14056452 0.13763829 0.13768847]\n",
      "\n",
      "\n",
      "Epoch Number: 78\n",
      "Train Loss: [0.21032538 0.1797989  0.15703945 0.21869224 0.17331336 0.2261888\n",
      " 0.18828562 0.18474992 0.20356338 0.18775809 0.19515532 0.15113592\n",
      " 0.15006872 0.17273624 0.1789974  0.16234702 0.17576481 0.17999496\n",
      " 0.17768383 0.20327345 0.2283363  0.20536141 0.17959374 0.23700367\n",
      " 0.20609264 0.1842073  0.21659753 0.17247528 0.17998233 0.15483938\n",
      " 0.19622687 0.20772581 0.15803523 0.1706276  0.154911   0.17820391\n",
      " 0.18318585 0.25273883 0.18631588 0.19513786 0.24645652 0.16780716\n",
      " 0.21088001 0.22632125 0.22400567 0.1856059  0.17651689 0.17808801\n",
      " 0.20075049 0.20318271 0.1797518  0.17528243 0.16892812 0.21274093\n",
      " 0.18049198 0.18174388 0.1704163  0.15948929 0.16757292 0.222632\n",
      " 0.21988793 0.16737574 0.1831857  0.22860534 0.21540645 0.19202876\n",
      " 0.17588474 0.19136591 0.19811323 0.15284207 0.2457712  0.16645664\n",
      " 0.16542904 0.14938243 0.16342342 0.1842085  0.17781384 0.18760213\n",
      " 0.14393575 0.16991237 0.17443132 0.17274119 0.20760435 0.23611665\n",
      " 0.20972806 0.15203565 0.16591844 0.17203912 0.18773283 0.19821304\n",
      " 0.20598687 0.22846645 0.20059034 0.22217907 0.17857526 0.16451304\n",
      " 0.27102494 0.16877595 0.17620273 0.17153089] Train accuracy: 0.9888888953460587\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.5686452e-04 2.5081187e-03 3.1216443e-04 ... 2.9169619e-03 0.0000000e+00\n",
      " 4.1484833e-05] + 0.13795702 = [0.13811389 0.14046514 0.13826919 ... 0.14087398 0.13795702 0.1379985 ]\n",
      "\n",
      "\n",
      "Epoch Number: 79\n",
      "Train Loss: [0.21780348 0.17309433 0.15976647 0.2152234  0.1700536  0.22655419\n",
      " 0.17783007 0.18113495 0.20982441 0.1879317  0.19614819 0.1515108\n",
      " 0.15365691 0.17499925 0.18219665 0.1567023  0.17883073 0.17474906\n",
      " 0.17239109 0.20556898 0.23215958 0.19631538 0.18181363 0.23999232\n",
      " 0.2085558  0.18352067 0.20389181 0.17196426 0.18520656 0.1613321\n",
      " 0.19214298 0.21481922 0.15802906 0.16542633 0.15942907 0.17452021\n",
      " 0.17567739 0.25343654 0.18936493 0.19545375 0.24886791 0.17048179\n",
      " 0.20461978 0.22742707 0.21951553 0.18667759 0.17697668 0.17745085\n",
      " 0.1975674  0.20922492 0.1750306  0.1652145  0.16510949 0.21903242\n",
      " 0.17651683 0.17410144 0.17740451 0.16656175 0.1660007  0.21464847\n",
      " 0.217788   0.16663419 0.1843493  0.22918688 0.21654576 0.20268871\n",
      " 0.17333794 0.19569339 0.19650795 0.15589657 0.24799848 0.17005327\n",
      " 0.16293587 0.14890967 0.15947546 0.18083948 0.16821663 0.18966128\n",
      " 0.1471306  0.17090617 0.17421837 0.17083377 0.2258393  0.24070586\n",
      " 0.20880313 0.15088098 0.16872531 0.18204202 0.20077366 0.19806142\n",
      " 0.20312941 0.22982886 0.19689928 0.22045851 0.1846273  0.16607799\n",
      " 0.27167577 0.17651029 0.18156417 0.1735043 ] Train accuracy: 0.9890277865860198\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.2324750e-04 3.6839247e-03 3.9224327e-04 ... 6.9837570e-03 0.0000000e+00\n",
      " 5.1617622e-05] + 0.13741909 = [0.13754234 0.14110301 0.13781133 ... 0.14440285 0.13741909 0.1374707 ]\n",
      "\n",
      "\n",
      "Epoch Number: 80\n",
      "Train Loss: [0.21726221 0.17625894 0.1592938  0.21157524 0.17046478 0.21847133\n",
      " 0.1836163  0.17696601 0.21285759 0.1906577  0.20725639 0.1512981\n",
      " 0.15145497 0.17083733 0.18763822 0.15884598 0.17942885 0.1724662\n",
      " 0.17126267 0.20419306 0.2388477  0.20157596 0.17701899 0.23999351\n",
      " 0.20721993 0.19248655 0.21021733 0.16265121 0.18286313 0.15812474\n",
      " 0.20343636 0.21818784 0.15799473 0.18370901 0.15404066 0.17339948\n",
      " 0.18633404 0.25977314 0.18136972 0.194604   0.2561561  0.17325628\n",
      " 0.2069745  0.22474974 0.22210363 0.19653508 0.18809411 0.18132398\n",
      " 0.20200223 0.20939189 0.18657489 0.17194968 0.19585747 0.21273834\n",
      " 0.1710811  0.1712195  0.18016462 0.16979082 0.16187052 0.23150904\n",
      " 0.21535933 0.16831298 0.18683812 0.22976248 0.22185047 0.20693423\n",
      " 0.18710315 0.19058138 0.19573694 0.1527955  0.24996713 0.1786067\n",
      " 0.16551974 0.1498218  0.19039539 0.18405685 0.174361   0.20601375\n",
      " 0.1602148  0.17480722 0.1717158  0.17120367 0.22928667 0.2437082\n",
      " 0.1946446  0.1717604  0.17656577 0.1673558  0.2051494  0.1974068\n",
      " 0.22053857 0.22276466 0.19920911 0.2125277  0.17954928 0.16558325\n",
      " 0.2753801  0.17527658 0.21337172 0.18066067] Train accuracy: 0.987638897365994\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: [8.0227852e-05 9.8351240e-03 2.4411082e-04 ... 3.9694160e-03 1.4662743e-05\n",
      " 4.8160553e-05] + 0.13865131 = [0.13873154 0.14848644 0.13889542 ... 0.14262073 0.13866597 0.13869947]\n",
      "\n",
      "\n",
      "Epoch Number: 81\n",
      "Train Loss: [0.20857957 0.18362182 0.16015771 0.22626098 0.17133535 0.23692282\n",
      " 0.18379217 0.19320998 0.21948369 0.19017954 0.19405004 0.15515973\n",
      " 0.1561699  0.17544384 0.1846953  0.1596304  0.17896965 0.17589435\n",
      " 0.18114544 0.21327585 0.23283887 0.22769901 0.18057579 0.23924494\n",
      " 0.20108241 0.20561047 0.23540442 0.19781964 0.20078881 0.17841572\n",
      " 0.19756915 0.21881244 0.16403535 0.17374627 0.15877675 0.19013995\n",
      " 0.18292242 0.25092492 0.18690076 0.23226902 0.24605256 0.17301401\n",
      " 0.21041365 0.22626083 0.20822579 0.18915021 0.1841461  0.17875692\n",
      " 0.19732998 0.20624262 0.18194419 0.1637979  0.1698886  0.223621\n",
      " 0.17261861 0.17731255 0.18439938 0.17675275 0.16910774 0.22938251\n",
      " 0.22724706 0.1701577  0.190983   0.23343277 0.21785447 0.2119463\n",
      " 0.18885455 0.20452972 0.20185058 0.16062625 0.25292635 0.18242541\n",
      " 0.18586726 0.15142058 0.19054343 0.18625434 0.16549237 0.2078714\n",
      " 0.15114708 0.17615134 0.1778879  0.1864292  0.2088573  0.2731167\n",
      " 0.2217386  0.16964167 0.16895853 0.17930967 0.22083926 0.2132781\n",
      " 0.23178509 0.22423574 0.19836216 0.21246836 0.17399956 0.16624373\n",
      " 0.31775454 0.17144483 0.19046468 0.17792186] Train accuracy: 0.9861111210452186\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [2.4649501e-04 1.3044924e-03 3.7865341e-04 ... 3.7472248e-03 0.0000000e+00\n",
      " 6.1035156e-05] + 0.14016719 = [0.14041369 0.14147168 0.14054585 ... 0.14391442 0.14016719 0.14022823]\n",
      "\n",
      "\n",
      "Epoch Number: 82\n",
      "Train Loss: [0.22396752 0.17481846 0.15790693 0.216334   0.17442563 0.23383085\n",
      " 0.17729793 0.18261157 0.21940504 0.18681332 0.21135768 0.15451577\n",
      " 0.16459008 0.17440969 0.1803707  0.15830699 0.18218333 0.17876977\n",
      " 0.1746951  0.21456672 0.22333176 0.19913584 0.18780564 0.24470922\n",
      " 0.20370622 0.18198176 0.22936869 0.1681136  0.19782346 0.16181591\n",
      " 0.19411491 0.21285003 0.15908931 0.1667701  0.15981326 0.17689538\n",
      " 0.18455626 0.24460453 0.2177987  0.196724   0.25503477 0.17018226\n",
      " 0.2115347  0.23741157 0.22718495 0.19676088 0.17613587 0.18320163\n",
      " 0.20543388 0.2152211  0.17449492 0.1650572  0.18221232 0.20766772\n",
      " 0.18583903 0.17902878 0.17305098 0.17684406 0.1838011  0.22294228\n",
      " 0.23474802 0.16851936 0.18694964 0.22700967 0.23036559 0.19764954\n",
      " 0.18052308 0.19060716 0.20021097 0.15646315 0.25359574 0.1831178\n",
      " 0.17412496 0.15261967 0.16132157 0.1832347  0.18978623 0.19289052\n",
      " 0.1449964  0.17049712 0.17887436 0.16707301 0.2153454  0.25245282\n",
      " 0.21029676 0.16114336 0.17242028 0.21784747 0.208457   0.20287356\n",
      " 0.22153383 0.2537277  0.20332907 0.23566474 0.19044691 0.16987062\n",
      " 0.29944712 0.18193847 0.17980978 0.17835243] Train accuracy: 0.9879166757067045\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [2.0645559e-04 4.0187985e-03 3.8723648e-04 ... 4.4839978e-03 0.0000000e+00\n",
      " 4.6491623e-05] + 0.1387158 = [0.13892226 0.1427346  0.13910304 ... 0.1431998  0.1387158  0.1387623 ]\n",
      "\n",
      "\n",
      "Epoch Number: 83\n",
      "Train Loss: [0.21548143 0.17217608 0.1575951  0.21988745 0.17067619 0.21651517\n",
      " 0.17598909 0.18786293 0.20910001 0.18700421 0.20229922 0.15915996\n",
      " 0.15257496 0.1744074  0.17977387 0.15597862 0.18080547 0.1741001\n",
      " 0.17982194 0.21349514 0.22836457 0.20033266 0.1822869  0.24267322\n",
      " 0.20040789 0.18707295 0.20774487 0.16520308 0.18696697 0.15913159\n",
      " 0.19539388 0.2135603  0.15757124 0.18240058 0.15563214 0.17925006\n",
      " 0.17880517 0.25333813 0.1723892  0.19594894 0.25435787 0.16753879\n",
      " 0.20652932 0.22886986 0.22608215 0.1933196  0.17424147 0.18035445\n",
      " 0.20219731 0.20434019 0.18052192 0.17179641 0.17332964 0.2119893\n",
      " 0.17610937 0.19091108 0.19258069 0.16615118 0.16359207 0.21995257\n",
      " 0.2243975  0.16744614 0.18387026 0.23592809 0.20963216 0.21036693\n",
      " 0.17572261 0.18384075 0.20834357 0.15374322 0.24328242 0.16844448\n",
      " 0.16337377 0.15058306 0.16186535 0.18166523 0.18273158 0.20066068\n",
      " 0.16096656 0.17066497 0.17715994 0.16841564 0.22613284 0.24528459\n",
      " 0.20816751 0.15931976 0.17342737 0.19057259 0.19824174 0.2005956\n",
      " 0.21173315 0.23643303 0.19595271 0.22033273 0.18531087 0.16822186\n",
      " 0.26250616 0.16831559 0.1925308  0.17428733] Train accuracy: 0.9888888961739011\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.5293062e-04 1.4777035e-03 2.9571354e-04 ... 4.2335689e-03 0.0000000e+00\n",
      " 4.6849251e-05] + 0.13839953 = [0.13855246 0.13987723 0.13869524 ... 0.1426331  0.13839953 0.13844638]\n",
      "\n",
      "\n",
      "Epoch Number: 84\n",
      "Train Loss: [0.22441156 0.17025942 0.15956616 0.20897168 0.16912518 0.22170499\n",
      " 0.17529258 0.18194681 0.2096666  0.18966013 0.19578184 0.15289228\n",
      " 0.15433472 0.16948612 0.17919773 0.15839255 0.17540689 0.17714089\n",
      " 0.17282121 0.19953062 0.23383586 0.19430913 0.18233602 0.23726957\n",
      " 0.2030546  0.18536653 0.19780558 0.17321026 0.17648822 0.15898019\n",
      " 0.20303352 0.2123315  0.15925138 0.17300183 0.15631026 0.17738459\n",
      " 0.17830497 0.252594   0.1934101  0.19575536 0.24420711 0.16743821\n",
      " 0.20800632 0.22427137 0.22025803 0.18561761 0.17086394 0.17741802\n",
      " 0.1938276  0.20896472 0.17953643 0.1637764  0.1695047  0.21460433\n",
      " 0.18227562 0.17718203 0.1706608  0.16671291 0.16629978 0.21788593\n",
      " 0.22585781 0.16877292 0.18069631 0.22353321 0.20820668 0.1937747\n",
      " 0.1746996  0.18515511 0.19657461 0.15160285 0.24650335 0.18018176\n",
      " 0.16914985 0.14939396 0.16402481 0.18412846 0.1663296  0.20707598\n",
      " 0.14486346 0.1738954  0.17932996 0.17159165 0.218885   0.2291772\n",
      " 0.20338053 0.16015993 0.16704096 0.18048349 0.20540814 0.19538008\n",
      " 0.20705774 0.22026794 0.19288144 0.22229256 0.18393934 0.16712275\n",
      " 0.26490963 0.1796391  0.18455964 0.17621799] Train accuracy: 0.989722229540348\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: [1.4100969e-04 2.8731078e-03 4.2679906e-04 ... 1.2754068e-02 0.0000000e+00\n",
      " 6.0439110e-05] + 0.13773197 = [0.13787298 0.14060508 0.13815877 ... 0.15048604 0.13773197 0.13779241]\n",
      "\n",
      "\n",
      "Epoch Number: 85\n",
      "Train Loss: [0.21117827 0.17637022 0.15736668 0.21578091 0.17289332 0.23623289\n",
      " 0.17953674 0.18986067 0.2134947  0.18676347 0.20646983 0.15274632\n",
      " 0.15022942 0.16631693 0.18269812 0.15964392 0.18457103 0.18022017\n",
      " 0.18049948 0.20416227 0.24626136 0.21845126 0.18212767 0.2390634\n",
      " 0.20711313 0.18921131 0.22330356 0.16748786 0.17350727 0.15762417\n",
      " 0.19166766 0.21368232 0.1556818  0.17040825 0.16437691 0.18262161\n",
      " 0.22399163 0.27707097 0.18193167 0.18294485 0.24905038 0.17470054\n",
      " 0.20848754 0.22469738 0.2205711  0.19121286 0.18261753 0.18325996\n",
      " 0.21732678 0.215739   0.1780874  0.1697022  0.16820341 0.23756719\n",
      " 0.1758775  0.22305271 0.17098309 0.1914179  0.16210455 0.22452055\n",
      " 0.22166568 0.16889742 0.19337401 0.22429532 0.2265516  0.19539171\n",
      " 0.18650113 0.18480057 0.19675076 0.15160106 0.23861939 0.20500392\n",
      " 0.16739708 0.15052463 0.1985106  0.182791   0.18229164 0.19618447\n",
      " 0.16001655 0.17052066 0.177192   0.22749662 0.22471234 0.23347653\n",
      " 0.21155088 0.15581147 0.1685409  0.16386661 0.19548193 0.21408628\n",
      " 0.25755304 0.22667964 0.20637155 0.2125704  0.17094201 0.17033671\n",
      " 0.2777346  0.1656471  0.17901754 0.17809246] Train accuracy: 0.9876388981938362\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [1.3063848e-04 1.3862699e-03 4.0939450e-04 ... 4.7272593e-03 6.5460801e-04\n",
      " 6.9975853e-05] + 0.14033532 = [0.14046596 0.14172159 0.14074472 ... 0.14506258 0.14098993 0.1404053 ]\n",
      "\n",
      "\n",
      "Epoch Number: 86\n",
      "Train Loss: [0.22389722 0.17726225 0.16445063 0.21457444 0.17450298 0.23201057\n",
      " 0.17449065 0.18846554 0.2168893  0.1826234  0.21516006 0.1540805\n",
      " 0.17186302 0.17764202 0.18289837 0.171905   0.1813668  0.18103069\n",
      " 0.1740812  0.20214933 0.2300236  0.20613876 0.1893578  0.24307245\n",
      " 0.22299963 0.1941868  0.23102573 0.18311185 0.20952284 0.16828616\n",
      " 0.18604392 0.21388465 0.16182324 0.1722649  0.1649621  0.1843296\n",
      " 0.1731882  0.24649231 0.20059921 0.22416036 0.272277   0.1724811\n",
      " 0.21392573 0.22369403 0.23647825 0.18139061 0.1769247  0.17960069\n",
      " 0.22601825 0.22761543 0.18131003 0.16607285 0.17623566 0.249919\n",
      " 0.18343896 0.18046287 0.18348163 0.19351596 0.17320442 0.2179361\n",
      " 0.22347039 0.17148091 0.18348591 0.23132409 0.2090718  0.21598436\n",
      " 0.21359016 0.19220157 0.20442864 0.15195407 0.25160107 0.1726417\n",
      " 0.17247921 0.15236983 0.16358373 0.18712787 0.16869554 0.19947022\n",
      " 0.14586017 0.17555736 0.17932731 0.17130566 0.23584734 0.24102646\n",
      " 0.21253736 0.15802628 0.16775768 0.19549218 0.21863529 0.227207\n",
      " 0.21160004 0.25715336 0.19167547 0.23921913 0.19033748 0.1667108\n",
      " 0.29441777 0.17561978 0.17849158 0.17376181] Train accuracy: 0.986527787314521\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.4458597e-04 1.4892519e-03 6.9606304e-04 ... 6.1532110e-03 0.0000000e+00\n",
      " 8.3446503e-05] + 0.14015624 = [0.14030083 0.14164549 0.1408523  ... 0.14630945 0.14015624 0.14023969]\n",
      "\n",
      "\n",
      "Epoch Number: 87\n",
      "Train Loss: [0.21703884 0.17824468 0.16128044 0.2190426  0.17295957 0.22250454\n",
      " 0.18011887 0.18432625 0.21493326 0.19097042 0.19109215 0.15558244\n",
      " 0.14854522 0.16983318 0.18737325 0.15956059 0.18516275 0.17693692\n",
      " 0.17447232 0.21352726 0.22633725 0.2018184  0.18419293 0.24396878\n",
      " 0.22349334 0.18496017 0.21382588 0.17328082 0.19839562 0.1595738\n",
      " 0.20327364 0.21704765 0.16091068 0.17327206 0.15757418 0.18037125\n",
      " 0.18004782 0.25374198 0.19555739 0.19798392 0.25865135 0.1747085\n",
      " 0.20901065 0.21981043 0.22967829 0.19901086 0.172792   0.18473049\n",
      " 0.20440225 0.20576409 0.18650703 0.17220008 0.18424873 0.21426159\n",
      " 0.17954308 0.17917873 0.19146757 0.16977158 0.17287777 0.226314\n",
      " 0.21936917 0.1674043  0.1959659  0.22420815 0.21939737 0.19757932\n",
      " 0.1799775  0.19636531 0.200139   0.15359099 0.23806998 0.1626881\n",
      " 0.18165925 0.15321226 0.1613842  0.1827812  0.17298287 0.2174617\n",
      " 0.14618091 0.17774741 0.17457539 0.18327852 0.21765922 0.25086653\n",
      " 0.20538893 0.17900337 0.1763336  0.16141628 0.2082131  0.19722614\n",
      " 0.2171385  0.23227368 0.19991106 0.2192068  0.1854313  0.16819264\n",
      " 0.280985   0.17535222 0.21278453 0.17539592] Train accuracy: 0.9880555644631386\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: [1.3814867e-04 5.1079690e-04 2.8415024e-04 ... 1.0033414e-02 0.0000000e+00\n",
      " 7.0214272e-05] + 0.13894446 = [0.13908261 0.13945526 0.13922861 ... 0.14897788 0.13894446 0.13901468]\n",
      "\n",
      "\n",
      "Epoch Number: 88\n",
      "Train Loss: [0.21965535 0.18005563 0.15792324 0.2144318  0.18298146 0.22572361\n",
      " 0.17506534 0.18588673 0.2122315  0.18326198 0.19982173 0.15151905\n",
      " 0.15092    0.17496121 0.179401   0.16309011 0.17553712 0.17573585\n",
      " 0.17455761 0.21048318 0.21937768 0.2019602  0.18419388 0.24387084\n",
      " 0.20635246 0.18675928 0.2247852  0.17155854 0.17407525 0.1586175\n",
      " 0.19321577 0.20598064 0.15638775 0.17015626 0.15630208 0.17301762\n",
      " 0.18979791 0.25275645 0.18781215 0.20946433 0.24560335 0.16919477\n",
      " 0.21428204 0.22864692 0.23442902 0.1862222  0.17557542 0.17634635\n",
      " 0.20817026 0.20915776 0.18279123 0.18031046 0.1800629  0.22864781\n",
      " 0.19260094 0.17627054 0.18410088 0.16305117 0.1628483  0.2125533\n",
      " 0.22619237 0.16760002 0.187838   0.227024   0.20482609 0.19673496\n",
      " 0.17675741 0.20076585 0.19724272 0.15442915 0.24110071 0.19082974\n",
      " 0.16208851 0.1529895  0.16658147 0.19225903 0.16686712 0.19157857\n",
      " 0.14756249 0.17235811 0.1750326  0.16742378 0.21374129 0.23267579\n",
      " 0.21135572 0.15926948 0.16534944 0.19973022 0.20306708 0.20047933\n",
      " 0.22063959 0.22957985 0.19727764 0.2175419  0.17932183 0.16505308\n",
      " 0.26771247 0.17306322 0.17848577 0.173959  ] Train accuracy: 0.9877777844667435\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.10030174e-04 2.76896358e-03 6.12542033e-04 ... 1.17153078e-02\n",
      " 0.00000000e+00 7.25984573e-05] + 0.13798267 = [0.1380927  0.14075163 0.13859521 ... 0.14969797 0.13798267 0.13805526]\n",
      "\n",
      "\n",
      "Epoch Number: 89\n",
      "Train Loss: [0.22497238 0.20057085 0.16095567 0.23492241 0.17690621 0.21871562\n",
      " 0.18108445 0.17736368 0.20612034 0.18539304 0.20043631 0.1521661\n",
      " 0.14957337 0.18076485 0.19516867 0.16812558 0.1780486  0.17652048\n",
      " 0.17946635 0.2106075  0.23105958 0.23151666 0.18395878 0.24956706\n",
      " 0.20483994 0.17966163 0.21315767 0.20402081 0.18354885 0.15882964\n",
      " 0.20529209 0.2144907  0.16042799 0.16587159 0.15611345 0.17342345\n",
      " 0.19201757 0.25432226 0.18686007 0.18528382 0.25009292 0.16734183\n",
      " 0.20069967 0.22492579 0.22144203 0.19998851 0.18083626 0.1790883\n",
      " 0.20722313 0.20972131 0.18486144 0.18896177 0.17609555 0.22610718\n",
      " 0.1705064  0.18247415 0.17998393 0.16293466 0.16354328 0.2119913\n",
      " 0.22186123 0.16619483 0.18153632 0.24841213 0.20626287 0.19907737\n",
      " 0.17678513 0.21552074 0.19124848 0.15318148 0.24386433 0.1597949\n",
      " 0.16028935 0.15018766 0.16942886 0.19130813 0.16941251 0.19187373\n",
      " 0.14851075 0.17167145 0.17642291 0.16509722 0.21986853 0.25191814\n",
      " 0.20904769 0.15605691 0.17089587 0.15561157 0.21682796 0.20979816\n",
      " 0.19932872 0.23487227 0.20188466 0.21272604 0.17691174 0.16478015\n",
      " 0.27778572 0.17852838 0.1758942  0.17632401] Train accuracy: 0.9881944523917304\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: [1.4041364e-04 2.0701885e-03 4.4955313e-04 ... 6.9021881e-03 0.0000000e+00\n",
      " 5.0902367e-05] + 0.13837391 = [0.13851433 0.1404441  0.13882346 ... 0.1452761  0.13837391 0.13842481]\n",
      "\n",
      "\n",
      "Epoch Number: 90\n",
      "Train Loss: [0.20575827 0.17399403 0.15863098 0.22561176 0.16866219 0.22612429\n",
      " 0.17819329 0.18375427 0.21583754 0.18963042 0.18915346 0.15099409\n",
      " 0.15056404 0.17131989 0.18666916 0.15884413 0.17937607 0.17808469\n",
      " 0.17715228 0.20900387 0.22879015 0.20387584 0.1773009  0.2407563\n",
      " 0.19386098 0.1922472  0.2182554  0.16782251 0.17760234 0.16123803\n",
      " 0.19654769 0.21015047 0.15909196 0.16982709 0.15980196 0.17274582\n",
      " 0.1784358  0.255716   0.17909078 0.19851112 0.24656942 0.16778849\n",
      " 0.20976174 0.22766253 0.22269064 0.17985862 0.17291167 0.18780348\n",
      " 0.20212871 0.21229556 0.17283572 0.16607398 0.16561635 0.22892302\n",
      " 0.18928605 0.1679528  0.17789952 0.17037636 0.1673758  0.21439677\n",
      " 0.21936558 0.16643505 0.19240397 0.21204987 0.21338704 0.20838147\n",
      " 0.17781793 0.2273801  0.19569485 0.15237953 0.24914002 0.17729098\n",
      " 0.16452713 0.147962   0.16177122 0.1975666  0.17013821 0.19588849\n",
      " 0.14624132 0.16974926 0.17250386 0.17746145 0.21717018 0.24194296\n",
      " 0.20164229 0.15667742 0.16997404 0.1794805  0.19548637 0.20208842\n",
      " 0.21443406 0.2211625  0.19694777 0.21324229 0.17710926 0.16983108\n",
      " 0.29017815 0.16576315 0.18088786 0.17437212] Train accuracy: 0.9877777861224281\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: [1.7486513e-04 2.2637248e-03 3.8175285e-04 ... 9.9776983e-03 0.0000000e+00\n",
      " 5.9604645e-05] + 0.13750465 = [0.13767952 0.13976838 0.1378864  ... 0.14748235 0.13750465 0.13756426]\n",
      "\n",
      "\n",
      "Epoch Number: 91\n",
      "Train Loss: [0.21705975 0.1693685  0.15884615 0.20313245 0.16872892 0.21998258\n",
      " 0.18080735 0.1819287  0.20652469 0.18807858 0.19549891 0.15165997\n",
      " 0.15167496 0.16836938 0.18109292 0.15861619 0.1788432  0.16692695\n",
      " 0.17403716 0.20758055 0.22984801 0.18783258 0.17881747 0.24085434\n",
      " 0.20301339 0.19255888 0.21041304 0.17565052 0.18535669 0.158714\n",
      " 0.18949822 0.21553174 0.15583023 0.1744573  0.15839802 0.1759388\n",
      " 0.16983023 0.2524771  0.20230035 0.20064963 0.24504665 0.16796406\n",
      " 0.21198735 0.22952567 0.22237784 0.18636097 0.17465109 0.17711625\n",
      " 0.20217948 0.20366685 0.1780354  0.16521995 0.17224547 0.21998766\n",
      " 0.17336173 0.19084089 0.18663388 0.16769454 0.16396517 0.2150402\n",
      " 0.22169271 0.16937298 0.18556878 0.22754224 0.2111779  0.2014157\n",
      " 0.17699769 0.19039261 0.20014268 0.15258396 0.248504   0.17287263\n",
      " 0.17866543 0.1494753  0.1673271  0.18434177 0.1662522  0.19637693\n",
      " 0.1429161  0.17038321 0.18256193 0.17856054 0.2175412  0.23338121\n",
      " 0.21061552 0.15392016 0.1652115  0.16018254 0.19360012 0.21038254\n",
      " 0.21501976 0.23157947 0.19603941 0.214719   0.17641021 0.16689456\n",
      " 0.27074707 0.1666237  0.18457693 0.18090984] Train accuracy: 0.9880555636352963\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: [1.4136732e-04 8.4793568e-04 5.4654479e-04 ... 2.1713272e-02 0.0000000e+00\n",
      " 5.4597855e-05] + 0.13749284 = [0.1376342  0.13834077 0.13803938 ... 0.1592061  0.13749284 0.13754743]\n",
      "\n",
      "\n",
      "Epoch Number: 92\n",
      "Train Loss: [0.21237099 0.1734046  0.15936732 0.20232655 0.17140593 0.21942538\n",
      " 0.17398423 0.17923735 0.21201926 0.18470748 0.19215605 0.14999284\n",
      " 0.15099485 0.1685546  0.179704   0.15440768 0.17903815 0.16925226\n",
      " 0.16916691 0.20741084 0.22620156 0.18921283 0.17675684 0.24355271\n",
      " 0.19730482 0.18092598 0.20034517 0.16817081 0.18494603 0.15830943\n",
      " 0.17914642 0.21729766 0.15435229 0.16458209 0.15453185 0.1718722\n",
      " 0.17116372 0.24278529 0.18708976 0.18695192 0.24967831 0.16617471\n",
      " 0.20847923 0.22300178 0.2198295  0.1842871  0.17225835 0.17781669\n",
      " 0.20173103 0.2131505  0.17395999 0.16374533 0.1675938  0.22095203\n",
      " 0.17159578 0.16871202 0.18078762 0.17192051 0.1611967  0.21370952\n",
      " 0.22263497 0.16723458 0.18401213 0.21628463 0.20548078 0.19675082\n",
      " 0.1839706  0.18478517 0.19268459 0.1493233  0.24532515 0.16708013\n",
      " 0.16109833 0.1480809  0.16360618 0.18208256 0.16387957 0.1901709\n",
      " 0.148252   0.17221807 0.17530346 0.1729882  0.215285   0.23696473\n",
      " 0.20071714 0.15360507 0.16803664 0.17062926 0.19715573 0.19537738\n",
      " 0.20736665 0.23835778 0.19254008 0.20201993 0.18379319 0.16546607\n",
      " 0.2689765  0.16813043 0.18951339 0.1759314 ] Train accuracy: 0.9898611199524667\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [2.4005771e-04 1.1395812e-03 4.2715669e-04 ... 7.7202767e-03 0.0000000e+00\n",
      " 6.0081482e-05] + 0.13652465 = [0.1367647  0.13766423 0.1369518  ... 0.14424492 0.13652465 0.13658473]\n",
      "\n",
      "\n",
      "Epoch Number: 93\n",
      "Train Loss: [0.21002215 0.17306283 0.15768759 0.20363346 0.16766122 0.22028612\n",
      " 0.17766565 0.17731816 0.2069175  0.18450183 0.19264928 0.15021604\n",
      " 0.14726532 0.16935952 0.1789437  0.15592065 0.17797634 0.17429471\n",
      " 0.17351155 0.20382872 0.22761232 0.19238454 0.18116659 0.24214035\n",
      " 0.19654803 0.1822112  0.21876997 0.16678947 0.18414119 0.15596366\n",
      " 0.18840331 0.21524614 0.15604977 0.17199323 0.15413806 0.1733975\n",
      " 0.17349651 0.24554093 0.17851585 0.19609939 0.2427295  0.16616906\n",
      " 0.20645446 0.22604412 0.21944343 0.18221861 0.17153874 0.17883386\n",
      " 0.197866   0.20590001 0.1757595  0.16474237 0.16787417 0.21724543\n",
      " 0.17081267 0.17823538 0.1727578  0.16526316 0.1600885  0.21707502\n",
      " 0.21920459 0.16665232 0.18443616 0.22117205 0.20939775 0.19436902\n",
      " 0.1774732  0.18272369 0.19690318 0.14919677 0.24186754 0.17473179\n",
      " 0.16621378 0.14781298 0.16166084 0.18175341 0.1732909  0.19527131\n",
      " 0.14678445 0.16707936 0.17620714 0.16984837 0.21196394 0.22888014\n",
      " 0.20757256 0.15003878 0.1664216  0.16265933 0.18693836 0.19993502\n",
      " 0.20620137 0.22847922 0.19660509 0.2153414  0.17504798 0.16503283\n",
      " 0.26800567 0.1615843  0.18522707 0.17046168] Train accuracy: 0.9888888961739011\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: [2.7009845e-04 1.3938993e-03 3.6066771e-04 ... 6.0977638e-03 0.0000000e+00\n",
      " 6.2584877e-05] + 0.13659409 = [0.13686419 0.13798799 0.13695475 ... 0.14269185 0.13659409 0.13665667]\n",
      "\n",
      "\n",
      "Epoch Number: 94\n",
      "Train Loss: [0.21883608 0.17099538 0.15760554 0.20634007 0.17037687 0.21964756\n",
      " 0.17585956 0.17947443 0.20686932 0.18664831 0.19720072 0.15076111\n",
      " 0.15051496 0.17283976 0.17704438 0.15507735 0.17714673 0.17630157\n",
      " 0.1704185  0.20296933 0.22995295 0.19037944 0.17712237 0.24189189\n",
      " 0.20138651 0.18051499 0.20290388 0.1711141  0.17986338 0.15804113\n",
      " 0.18559583 0.21279582 0.15572363 0.16505921 0.15531796 0.1740574\n",
      " 0.17328453 0.24681643 0.18518579 0.18421286 0.24111488 0.16492093\n",
      " 0.2055604  0.22658457 0.2172672  0.18590893 0.16983585 0.17489007\n",
      " 0.20824997 0.21122548 0.18072222 0.16325057 0.16821174 0.22083884\n",
      " 0.17116852 0.17928274 0.16991816 0.165856   0.16366835 0.21852368\n",
      " 0.22035155 0.1667627  0.18128248 0.22250631 0.20700069 0.19783252\n",
      " 0.17963494 0.1816768  0.19653653 0.14956607 0.24241717 0.17015743\n",
      " 0.1647041  0.14773236 0.16602206 0.17988949 0.167464   0.19534168\n",
      " 0.14468223 0.16826051 0.17533718 0.1694819  0.21991271 0.22962686\n",
      " 0.20377228 0.15561974 0.16575763 0.17923665 0.19791858 0.19744486\n",
      " 0.20535511 0.23933503 0.1895197  0.23366107 0.17708118 0.16545342\n",
      " 0.26263207 0.16784233 0.18579184 0.17544696] Train accuracy: 0.9902777853939269\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [2.3278594e-04 1.6080290e-03 3.8902462e-04 ... 9.2165023e-03 0.0000000e+00\n",
      " 6.8426132e-05] + 0.13676885 = [0.13700163 0.13837688 0.13715787 ... 0.14598535 0.13676885 0.13683727]\n",
      "\n",
      "\n",
      "Epoch Number: 95\n",
      "Train Loss: [0.22110921 0.17538314 0.15856268 0.20893711 0.1680023  0.22706471\n",
      " 0.17947292 0.18618417 0.20767623 0.18487021 0.20216505 0.15239455\n",
      " 0.14740145 0.17302746 0.18095905 0.1554246  0.18030916 0.16951506\n",
      " 0.18193735 0.20875286 0.22934487 0.19331095 0.18322538 0.24106547\n",
      " 0.1994652  0.1791752  0.21474256 0.16533983 0.17994624 0.15705281\n",
      " 0.18420848 0.21206379 0.16092107 0.16803458 0.15603262 0.17763565\n",
      " 0.1842704  0.24254626 0.18949479 0.19207758 0.25249496 0.1660568\n",
      " 0.20624928 0.22447681 0.2284129  0.18101023 0.1729594  0.17958593\n",
      " 0.19870342 0.20624903 0.1740051  0.16739148 0.1687253  0.22642364\n",
      " 0.17640708 0.19659501 0.16939455 0.16252941 0.16518791 0.21780325\n",
      " 0.22292024 0.16736376 0.18314011 0.22148572 0.21532118 0.19465071\n",
      " 0.17713176 0.18994328 0.19443241 0.15144645 0.24629882 0.17827554\n",
      " 0.16768423 0.1489702  0.16161789 0.18221383 0.1921138  0.18649681\n",
      " 0.14543766 0.17164515 0.17487887 0.17013282 0.21620244 0.23518512\n",
      " 0.20948854 0.15258878 0.16842306 0.16381443 0.19283634 0.20709711\n",
      " 0.2079546  0.23804845 0.19360854 0.22022964 0.18000354 0.16486277\n",
      " 0.276764   0.16230243 0.179647   0.17316341] Train accuracy: 0.988472230732441\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: [1.8809736e-04 2.9515475e-03 4.3907762e-04 ... 9.9928081e-03 0.0000000e+00\n",
      " 6.1154366e-05] + 0.13737345 = [0.13756154 0.140325   0.13781253 ... 0.14736626 0.13737345 0.1374346 ]\n",
      "\n",
      "\n",
      "Epoch Number: 96\n",
      "Train Loss: [0.22304824 0.1721762  0.15713735 0.213017   0.17077665 0.22223197\n",
      " 0.18337245 0.1863553  0.21036923 0.18540901 0.19217376 0.15300353\n",
      " 0.14928463 0.1784075  0.18007892 0.15642458 0.17936818 0.17284152\n",
      " 0.1706811  0.21391149 0.22822367 0.18943013 0.17878865 0.24481562\n",
      " 0.20397158 0.19159944 0.20283401 0.17246547 0.18631807 0.1601887\n",
      " 0.19260642 0.21824083 0.15808982 0.1836212  0.15678476 0.17655317\n",
      " 0.17348513 0.252936   0.18525508 0.20459676 0.24438561 0.1712625\n",
      " 0.20746285 0.23168574 0.23255695 0.1898543  0.16970825 0.17703986\n",
      " 0.19116314 0.2130903  0.17311195 0.16313596 0.16686845 0.23006408\n",
      " 0.17982614 0.19325453 0.19813472 0.17213883 0.16082317 0.21724468\n",
      " 0.22145851 0.16734736 0.19060439 0.2258816  0.21296816 0.19318384\n",
      " 0.17889935 0.18293901 0.22900653 0.1549359  0.23191565 0.18050735\n",
      " 0.16628861 0.15007253 0.1639115  0.18766952 0.17386478 0.189658\n",
      " 0.14337324 0.16968355 0.17511465 0.17413694 0.24440129 0.24523722\n",
      " 0.20571703 0.14772393 0.17315048 0.17131548 0.2132455  0.2048743\n",
      " 0.20808119 0.23688684 0.19639903 0.21632093 0.18195082 0.16848347\n",
      " 0.2782853  0.20498425 0.18307635 0.1731657 ] Train accuracy: 0.9883333428038491\n",
      "Test accuracy 0.933732\n",
      "MarginLoss + RegLoss: [2.1098554e-04 2.0054728e-03 3.0978024e-04 ... 1.5529245e-03 0.0000000e+00\n",
      " 7.8558922e-05] + 0.13782711 = [0.1380381  0.13983259 0.1381369  ... 0.13938004 0.13782711 0.13790567]\n",
      "\n",
      "\n",
      "Epoch Number: 97\n",
      "Train Loss: [0.23507439 0.1897038  0.1606063  0.22317457 0.20111437 0.22265236\n",
      " 0.17840362 0.18093951 0.2055082  0.19036533 0.19745614 0.1533568\n",
      " 0.15103541 0.17482573 0.1777305  0.1649466  0.17684752 0.1799294\n",
      " 0.1715073  0.20922236 0.22929467 0.21294567 0.1820189  0.2507013\n",
      " 0.20803167 0.18245858 0.21200782 0.16854054 0.18107235 0.16026324\n",
      " 0.1841246  0.20971842 0.16109157 0.16866213 0.15537228 0.17279953\n",
      " 0.20108883 0.27817756 0.1993748  0.18868154 0.25155762 0.16967213\n",
      " 0.20684545 0.22885793 0.23076926 0.18889631 0.1772013  0.17659433\n",
      " 0.20687886 0.21510905 0.18557332 0.19866624 0.18041585 0.23183121\n",
      " 0.18309528 0.16681327 0.17828792 0.16664544 0.17812328 0.21842988\n",
      " 0.22974473 0.16909721 0.18264931 0.2280844  0.20952907 0.19923483\n",
      " 0.18260579 0.2256183  0.19318245 0.15057829 0.24938676 0.1884365\n",
      " 0.18468767 0.15100104 0.16586575 0.18180257 0.17355984 0.21243456\n",
      " 0.14738683 0.16794066 0.1759342  0.16597761 0.2172863  0.24152197\n",
      " 0.20836261 0.15991992 0.16995722 0.19561535 0.20728515 0.20562094\n",
      " 0.22339746 0.24458054 0.19075036 0.23309    0.18236132 0.16926713\n",
      " 0.2599124  0.16926247 0.18089588 0.18022706] Train accuracy: 0.9863888969024023\n",
      "Test accuracy 0.926756\n",
      "MarginLoss + RegLoss: [0.00019489 0.00525273 0.00095882 ... 0.01325203 0.         0.00014363] + 0.13810088 = [0.13829577 0.14335361 0.1390597  ... 0.15135291 0.13810088 0.13824451]\n",
      "\n",
      "\n",
      "Epoch Number: 98\n",
      "Train Loss: [0.2397351  0.1792462  0.15983707 0.23625332 0.17630367 0.22308414\n",
      " 0.18646076 0.19292127 0.22596553 0.19032249 0.21673483 0.1546958\n",
      " 0.15296647 0.17674175 0.21070619 0.1729061  0.18997276 0.1759469\n",
      " 0.18834928 0.20834686 0.32815847 0.23983829 0.19642274 0.24701265\n",
      " 0.2209695  0.19820471 0.27136275 0.20552036 0.1907457  0.16083786\n",
      " 0.1907013  0.21068023 0.15460178 0.17703125 0.1634454  0.1829085\n",
      " 0.20801026 0.29714358 0.20759374 0.18864903 0.25449893 0.18574794\n",
      " 0.2109578  0.22653124 0.22036728 0.19217242 0.22041316 0.25741112\n",
      " 0.2236259  0.23047431 0.18838385 0.18023804 0.17352073 0.34019625\n",
      " 0.17716172 0.250212   0.199341   0.21337655 0.16716997 0.21944815\n",
      " 0.22114782 0.17148344 0.19297485 0.26826686 0.22771525 0.21013959\n",
      " 0.1968048  0.2278187  0.20093529 0.15148726 0.26085067 0.17289248\n",
      " 0.16742538 0.15189844 0.19142862 0.20813316 0.18245652 0.20160058\n",
      " 0.16341023 0.17305684 0.179147   0.2652558  0.22800899 0.25087512\n",
      " 0.22181772 0.17017615 0.17135915 0.17178164 0.2038255  0.22327346\n",
      " 0.2506351  0.2348015  0.20883445 0.22388    0.19039376 0.2207759\n",
      " 0.28760773 0.17496113 0.18743747 0.18010563] Train accuracy: 0.9831944538487328\n",
      "Test accuracy 0.927255\n",
      "MarginLoss + RegLoss: [3.1633675e-04 6.3338727e-03 5.5429339e-04 ... 5.4713786e-03 2.3841858e-06\n",
      " 1.0442734e-04] + 0.14195542 = [0.14227176 0.1482893  0.14250971 ... 0.1474268  0.1419578  0.14205985]\n",
      "\n",
      "\n",
      "Epoch Number: 99\n",
      "Train Loss: [0.22358492 0.17578994 0.16409647 0.22616018 0.17788506 0.227899\n",
      " 0.1710988  0.19148964 0.23534757 0.19404195 0.20075923 0.15945822\n",
      " 0.15830629 0.16695137 0.19062167 0.16206497 0.1886463  0.18199772\n",
      " 0.17591621 0.20309693 0.22876194 0.22783028 0.18383786 0.25257003\n",
      " 0.20693094 0.20392257 0.2369176  0.16958293 0.22632106 0.18365812\n",
      " 0.20819622 0.21510139 0.16058    0.17820254 0.15478434 0.1830299\n",
      " 0.17093557 0.26929238 0.23646872 0.21261624 0.25601512 0.17470545\n",
      " 0.21295154 0.23435634 0.23651436 0.19853611 0.19913313 0.20453486\n",
      " 0.23403485 0.22266634 0.18798901 0.17480153 0.20885438 0.2334202\n",
      " 0.20993385 0.17491534 0.2092043  0.22282328 0.17566787 0.22853112\n",
      " 0.2416717  0.17199814 0.1932359  0.20908946 0.20683919 0.21180792\n",
      " 0.22108214 0.21609516 0.2031432  0.15404412 0.23480013 0.17805693\n",
      " 0.21492286 0.16084152 0.17953748 0.19034672 0.18189211 0.22105744\n",
      " 0.14811896 0.18188936 0.18190484 0.18817718 0.22005002 0.26476088\n",
      " 0.19423732 0.16587268 0.17325264 0.24753544 0.22418958 0.22585066\n",
      " 0.21854214 0.27134746 0.20182505 0.22143549 0.20318383 0.17564867\n",
      " 0.31353945 0.17291182 0.19568226 0.17875779] Train accuracy: 0.9850000118215879\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: [2.71409750e-04 1.26293302e-03 4.28229570e-04 ... 6.76152110e-03\n",
      " 8.46385956e-06 1.13129616e-04] + 0.14090762 = [0.14117903 0.14217055 0.14133584 ... 0.14766914 0.14091608 0.14102075]\n",
      "\n",
      "\n",
      "Maximum Test accuracy at compressed model size(including early stopping): 0.9352267 at Epoch: 55\n",
      "Final Test Accuracy: 0.9307424\n"
     ]
    }
   ],
   "source": [
    "totalEpochs = 100\n",
    "batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
