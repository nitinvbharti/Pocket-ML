{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProtoNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "# sys.path.insert(0, '../../')\n",
    "from protoNNTrainer import ProtoNNTrainer\n",
    "from protoNN import ProtoNN\n",
    "import utils as utils\n",
    "import helpermethods as helper\n",
    "import matplotlib.pyplot as mplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.271026Z",
     "start_time": "2018-08-15T13:06:10.225900Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_train = np.load('./usps/Xtrain.npy')\n",
    "# y_train = np.load('./usps/Ytrain.npy')\n",
    "# x_test = np.load('./usps/Xtest.npy')\n",
    "# y_test = np.load('./usps/Ytest.npy')\n",
    "# N, dataDimension = x_train.shape\n",
    "# # nClasses = len(np.unique(Y_train))\n",
    "# numClasses = y_train.shape[1]\n",
    "# # print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# # x_train, y_train = out[2], out[3]\n",
    "# # x_test, y_test = out[4], out[5]\n",
    "# print(\"Feature Dimension: \", dataDimension)\n",
    "# print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.279204Z",
     "start_time": "2018-08-15T13:06:10.272880Z"
    }
   },
   "outputs": [],
   "source": [
    "# PROJECTION_DIM = 60\n",
    "# NUM_PROTOTYPES = 60\n",
    "# REG_W = 0.000005\n",
    "# REG_B = 0.0\n",
    "# REG_Z = 0.00005\n",
    "# SPAR_W = 0.8\n",
    "# SPAR_B = 1.0\n",
    "# SPAR_Z = 1.0\n",
    "# LEARNING_RATE = 0.05\n",
    "# NUM_EPOCHS = 100\n",
    "# BATCH_SIZE = 32\n",
    "# GAMMA = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.307632Z",
     "start_time": "2018-08-15T13:06:10.280955Z"
    }
   },
   "outputs": [],
   "source": [
    "#     W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "#                        NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.641991Z",
     "start_time": "2018-08-15T13:06:10.309353Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup input and train protoNN\n",
    "# X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "# Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "# protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "#                   NUM_PROTOTYPES, numClasses,\n",
    "#                   gamma, W=W, B=B)\n",
    "# trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "#                          SPAR_W, SPAR_B, SPAR_Z,\n",
    "#                          LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "# sess = tf.Session()\n",
    "# start = time.time()\n",
    "# trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "#               printStep=600, valStep=10)\n",
    "\n",
    "# end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.671507Z",
     "start_time": "2018-08-15T13:07:22.645050Z"
    }
   },
   "outputs": [],
   "source": [
    "# acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "# # W, B, Z are tensorflow graph nodes\n",
    "# W, B, Z, _ = protoNN.getModelMatrices()\n",
    "# matrixList = sess.run([W, B, Z])\n",
    "# sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "# nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "# print(\"Final test accuracy\", acc)\n",
    "# print(\"Model size constraint (Bytes): \", size)\n",
    "# print(\"Number of non-zeros: \", nnz)\n",
    "# nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "# print(\"Actual model size: \", size)\n",
    "# print(\"Actual non-zeros: \", nnz)\n",
    "# print(\"Time for computation : \", np.round(end-start,4),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For notmnist : \n",
      "(100000, 784)\n",
      "Feature Dimension:  784\n",
      "Num classes:  10\n"
     ]
    }
   ],
   "source": [
    "# folders = ['madelon','usps'] #cifar,mnistsmall.abalone,iris\n",
    "fol = 'notmnist'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "x_train = x_train.reshape(x_train.shape[0],28*28)\n",
    "x_test = x_test.reshape(x_test.shape[0],28*28)\n",
    "print(x_train.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "y_train = mo2.fit_transform(mo1.fit_transform((y_train.ravel())).reshape(-1,1)).todense()\n",
    "y_test = mo2.transform(mo1.transform((y_test.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "\n",
    "numClasses = y_train.shape[1]\n",
    "# print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# x_train, y_train = out[2], out[3]\n",
    "# x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "proto = [25]#,50,100,200]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = 16\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n \\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For usps : \n",
      "(7291, 257)\n",
      "Feature Dimension:  257\n",
      "Num classes:  10\n",
      "Final test accuracy 0.90632784\n",
      "Model size constraint (Bytes):  57400\n",
      "Actual model size:  14350\n",
      "Time for computation :  0.048  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.91380167\n",
      "Model size constraint (Bytes):  63400\n",
      "Actual model size:  15850\n",
      "Time for computation :  0.0781  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.9192825\n",
      "Model size constraint (Bytes):  75400\n",
      "Actual model size:  18850\n",
      "Time for computation :  0.1094  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.9192825\n",
      "Model size constraint (Bytes):  99400\n",
      "Actual model size:  24850\n",
      "Time for computation :  0.1718  seconds\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# folders = ['madelon','usps'] #cifar,mnistsmall.abalone,iris\n",
    "fol = 'usps'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "print(x_train.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "\n",
    "y_train[y_train[:] == -1]=0\n",
    "y_test[y_test[:] == -1]=0\n",
    "numClasses = y_train.shape[1]\n",
    "# print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# x_train, y_train = out[2], out[3]\n",
    "# x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "proj = [50]\n",
    "proto = [25,50,100,200]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = 50\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n \\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cifar : \n",
      "(50000, 3072) (50000, 10)\n",
      "Feature Dimension:  3072\n",
      "Num classes:  10\n",
      "Final test accuracy 0.0984\n",
      "Model size constraint (Bytes):  1239800\n",
      "Actual model size:  309950\n",
      "Time for computation :  0.5155  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.0984\n",
      "Model size constraint (Bytes):  1250800\n",
      "Actual model size:  312700\n",
      "Time for computation :  0.6151  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.0984\n",
      "Model size constraint (Bytes):  1338800\n",
      "Actual model size:  334700\n",
      "Time for computation :  1.8176  seconds\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#cifar,mnistsmall.abalone,iris\n",
    "fol= 'cifar'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "x_train = x_train.reshape(x_train.shape[0],1024*3)\n",
    "x_test = x_test.reshape(x_test.shape[0],1024*3)\n",
    "print(x_train.shape,y_train.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "numClasses = y_train.shape[1]\n",
    "# print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# x_train, y_train = out[2], out[3]\n",
    "# x_test, y_test = out[4], out[5]\n",
    "\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "proj = [100]\n",
    "proto = [25,50,250]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = 100\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n \\n ')\n",
    "    \n",
    "# 100\n",
    "# Final test accuracy 0.0986\n",
    "# Model size constraint (Bytes):  1272800\n",
    "# Actual model size:  318200\n",
    "# Time for computation :  0.8694  seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For mnist : \n",
      "(7500, 784) (2500, 784)\n",
      "Feature Dimension:  784\n",
      "Num classes:  10\n",
      "Final test accuracy 0.734\n",
      "Model size constraint (Bytes):  24424.0\n",
      "Actual model size:  6106\n",
      "Time for computation :  0.0605  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.8056\n",
      "Model size constraint (Bytes):  27184.0\n",
      "Actual model size:  6796\n",
      "Time for computation :  0.0838  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.8048\n",
      "Model size constraint (Bytes):  31784.0\n",
      "Actual model size:  7946\n",
      "Time for computation :  0.1456  seconds\n",
      "\n",
      " \n",
      " \n",
      "Final test accuracy 0.8244\n",
      "Model size constraint (Bytes):  40984.0\n",
      "Actual model size:  10246\n",
      "Time for computation :  0.1506  seconds\n",
      "\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#cifar,mnistsmall.abalone,iris\n",
    "fol= 'mnist'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "# x_train = x_train.reshape(x_train.shape[0],1024*3)\n",
    "# x_test = x_test.reshape(x_test.shape[0],1024*3)\n",
    "print(x_train.shape,x_test.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "temp = y_test.copy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "y_train = mo2.fit_transform(mo1.fit_transform((y_train.ravel())).reshape(-1,1)).todense()\n",
    "y_test = mo2.transform(mo1.transform((y_test.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "\n",
    "numClasses = y_train.shape[1]\n",
    "\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "# t = 1500\n",
    "# mplot.imshow(x_test[t].reshape(28,28),cmap=\"gray\")\n",
    "# print(temp[t])\n",
    "\n",
    "proto = [10,25,50,100]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = 36\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.0005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.0005\n",
    "    SPAR_W = 0.1\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.005\n",
    "    NUM_EPOCHS = 500\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n \\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iris : \n",
      "(112, 4) (112,)\n",
      "Feature Dimension:  4\n",
      "Num classes:  3\n",
      "PROJECTION_DIM :  4 NUM_PROTOTYPES :  5\n",
      "Final test accuracy 0.39473686\n",
      "Model size constraint (Bytes):  204\n",
      "Actual model size:  51\n",
      "Time for computation :  0.1758  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  4 NUM_PROTOTYPES :  10\n",
      "Final test accuracy 0.8684211\n",
      "Model size constraint (Bytes):  344\n",
      "Actual model size:  86\n",
      "Time for computation :  0.1805  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  4 NUM_PROTOTYPES :  25\n",
      "Final test accuracy 0.8947368\n",
      "Model size constraint (Bytes):  764\n",
      "Actual model size:  191\n",
      "Time for computation :  0.1865  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  4 NUM_PROTOTYPES :  50\n",
      "Final test accuracy 0.92105263\n",
      "Model size constraint (Bytes):  1464\n",
      "Actual model size:  366\n",
      "Time for computation :  0.1905  seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cifar,mnistsmall.abalone,iris\n",
    "fol= 'iris'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "# x_train = x_train.reshape(x_train.shape[0],1024*3)\n",
    "# x_test = x_test.reshape(x_test.shape[0],1024*3)\n",
    "print(x_train.shape,y_train.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "flowers = np.unique(y_train)\n",
    "cl=1\n",
    "for flower in flowers: \n",
    "    y_train[y_train == flower]=cl\n",
    "    y_test[y_test == flower]=cl\n",
    "    cl=cl+1\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "y_train = mo2.fit_transform(mo1.fit_transform((y_train.ravel())).reshape(-1,1)).todense()\n",
    "y_test = mo2.transform(mo1.transform((y_test.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "numClasses = y_train.shape[1]\n",
    "# print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# x_train, y_train = out[2], out[3]\n",
    "# x_test, y_test = out[4], out[5]\n",
    "\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "proto = [5,10,25,50]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = dataDimension\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    \n",
    "    print('PROJECTION_DIM : ',PROJECTION_DIM,'NUM_PROTOTYPES : ',NUM_PROTOTYPES )\n",
    "    \n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "   \n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "#     acc1 = sess.run(protoNN.accuracy, feed_dict={X: x_train, Y: y_train})\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For madelon : \n",
      "(2000, 500) (2000, 1)\n",
      "Feature Dimension:  500\n",
      "Num classes:  2\n",
      "PROJECTION_DIM :  50 NUM_PROTOTYPES :  5\n",
      "Final test accuracy 0.5\n",
      "Model size constraint (Bytes):  101040\n",
      "Actual model size:  25260\n",
      "Time for computation :  0.1178  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  50 NUM_PROTOTYPES :  10\n",
      "Final test accuracy 0.5\n",
      "Model size constraint (Bytes):  102080\n",
      "Actual model size:  25520\n",
      "Time for computation :  0.1253  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  50 NUM_PROTOTYPES :  25\n",
      "Final test accuracy 0.5\n",
      "Model size constraint (Bytes):  105200\n",
      "Actual model size:  26300\n",
      "Time for computation :  0.125  seconds\n",
      "\n",
      "\n",
      "PROJECTION_DIM :  50 NUM_PROTOTYPES :  50\n",
      "Final test accuracy 0.5\n",
      "Model size constraint (Bytes):  110400\n",
      "Actual model size:  27600\n",
      "Time for computation :  0.1406  seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cifar,mnistsmall.abalone,iris\n",
    "fol= 'madelon'\n",
    "print(\"For \" + fol + \" : \")\n",
    "x_train = np.load(str('./'+ fol + '/Xtrain.npy'))\n",
    "y_train = np.load(str('./'+ fol + '/Ytrain.npy'))\n",
    "x_test = np.load(str('./'+ fol + '/Xtest.npy'))\n",
    "y_test = np.load(str('./'+ fol + '/Ytest.npy'))\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "N, dataDimension = x_train.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "# y_train = y_train.reshape(y_train.shape[0],1)\n",
    "# y_test = y_test.reshape(y_test.shape[0],1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "y_train = mo2.fit_transform(mo1.fit_transform((y_train.ravel())).reshape(-1,1)).todense()\n",
    "y_test = mo2.transform(mo1.transform((y_test.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "numClasses = y_train.shape[1]\n",
    "# print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)\n",
    "# x_train, y_train = out[2], out[3]\n",
    "# x_test, y_test = out[4], out[5]\n",
    "\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)\n",
    "\n",
    "proto = [5,10,25,50]\n",
    "for i in range(len(proto)):\n",
    "    PROJECTION_DIM = 50\n",
    "    NUM_PROTOTYPES = proto[i]\n",
    "    REG_W = 0.000005\n",
    "    REG_B = 0.0\n",
    "    REG_Z = 0.00005\n",
    "    SPAR_W = 0.8\n",
    "    SPAR_B = 1.0\n",
    "    SPAR_Z = 1.0\n",
    "    LEARNING_RATE = 0.05\n",
    "    NUM_EPOCHS = 100\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.0015\n",
    "    \n",
    "    print('PROJECTION_DIM : ',PROJECTION_DIM,'NUM_PROTOTYPES : ',NUM_PROTOTYPES )\n",
    "    \n",
    "    W, B, gamma = helper.getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                           NUM_PROTOTYPES, x_train)\n",
    "    # Setup input and train protoNN\n",
    "    X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "    Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                      NUM_PROTOTYPES, numClasses,\n",
    "                      gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                             SPAR_W, SPAR_B, SPAR_Z,\n",
    "                             LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    sess = tf.Session()\n",
    "   \n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "                  printStep=600, valStep=10)\n",
    "\n",
    "\n",
    "#     acc1 = sess.run(protoNN.accuracy, feed_dict={X: x_train, Y: y_train})\n",
    "    start = time.time()\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    end = time.time()\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n",
    "\n",
    "    print(\"Final test accuracy\", acc)\n",
    "    print(\"Model size constraint (Bytes): \", size)\n",
    "    # print(\"Number of non-zeros: \", nnz)\n",
    "    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList, expected=False)\n",
    "    print(\"Actual model size: \", size)\n",
    "    # print(\"Actual non-zeros: \", nnz)\n",
    "    print(\"Time for computation : \", np.round(end-start,4),\" seconds\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
